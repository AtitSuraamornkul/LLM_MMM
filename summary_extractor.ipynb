{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/summary_output.html', 'r', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Contribution Analysis:\n",
      "\n",
      "Business Context: Your channel contributions help you\n",
      "understand what drove your revenue. Dv_360_X1 and Google drove the most overall\n",
      "revenue.\n",
      "\n",
      "Revenue Attribution:\n",
      "- Baseline revenue accounts for 90.9% of total revenue\n",
      "- Marketing channels drive 9.1% of total revenue\n",
      "- Total revenue split: 90.9% organic/baseline vs 9.1% paid marketing\n",
      "\n",
      "Marketing Channel Performance:\n",
      "- DV_360_X1 contributes 6.0% of total revenue ($7,227,618)\n",
      "- GOOGLE contributes 2.0% of total revenue ($2,444,298)\n",
      "- META contributes 1.0% of total revenue ($1,245,430)\n",
      "- TIKTOK contributes 0.1% of total revenue ($61,527)\n",
      "\n",
      "Key Insights:\n",
      "- DV_360_X1 is the top performing marketing channel at 6.0%\n",
      "- Baseline/organic traffic dominates revenue generation at 90.9%\n",
      "- Marketing channels collectively contribute 9.1% to total revenue\n",
      "\n",
      "Methodology: Note: This graphic encompasses all of\n",
      "your revenue drivers, but breaks down your marketing revenue by the baseline\n",
      "and all channels.\n"
     ]
    }
   ],
   "source": [
    "def get_channel_contribution(soup):\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    chart_card = soup.find(\"card\", {\"id\": \"channel-contrib\"})\n",
    "    \n",
    "    if not chart_card:\n",
    "        return \"No channel contribution data found.\"\n",
    "    \n",
    "    # Extract data (same extraction logic as before)\n",
    "    insight_text = chart_card.find(\"p\", {\"class\": \"insights-text\"})\n",
    "    insight_text_content = insight_text.get_text(strip=True) if insight_text else None\n",
    "    \n",
    "    card_title = chart_card.find(\"card-title\")\n",
    "    card_title_content = card_title.get_text(strip=True) if card_title else None\n",
    "    \n",
    "    chart_description = chart_card.find(\"chart-description\")\n",
    "    chart_description_content = chart_description.get_text(strip=True) if chart_description else None\n",
    "    \n",
    "    # [Same JSON extraction logic as before...]\n",
    "    script_tag = chart_card.find(\"script\", {\"type\": \"text/javascript\"})\n",
    "    channel_data = []\n",
    "    \n",
    "    if script_tag:\n",
    "        script_content = script_tag.get_text()\n",
    "        json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                escaped_json = json_match.group(1)\n",
    "                unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "                chart_spec = json.loads(unescaped_json)\n",
    "                datasets = chart_spec.get('datasets', {})\n",
    "                for dataset_key, dataset_value in datasets.items():\n",
    "                    if isinstance(dataset_value, list):\n",
    "                        channel_data = dataset_value\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Process data\n",
    "    baseline_data = None\n",
    "    marketing_channels = []\n",
    "    \n",
    "    for channel in channel_data:\n",
    "        if channel.get('channel') == 'BASELINE':\n",
    "            baseline_data = channel\n",
    "        else:\n",
    "            marketing_channels.append(channel)\n",
    "    \n",
    "    marketing_channels.sort(key=lambda x: x.get('incremental_outcome', 0), reverse=True)\n",
    "    \n",
    "    # Format for RAG/Vector DB\n",
    "    baseline_pct = round(baseline_data.get('pct_of_contribution', 0) * 100, 1) if baseline_data else 0\n",
    "    total_marketing_pct = round(sum(ch.get('pct_of_contribution', 0) for ch in marketing_channels) * 100, 1)\n",
    "    \n",
    "    # Create structured, searchable content\n",
    "    rag_content = f\"\"\"\n",
    "Channel Contribution Analysis:\n",
    "\n",
    "Business Context: {insight_text_content}\n",
    "\n",
    "Revenue Attribution:\n",
    "- Baseline revenue accounts for {baseline_pct}% of total revenue\n",
    "- Marketing channels drive {total_marketing_pct}% of total revenue\n",
    "- Total revenue split: {baseline_pct}% organic/baseline vs {total_marketing_pct}% paid marketing\n",
    "\n",
    "Marketing Channel Performance:\n",
    "\"\"\".strip()\n",
    "        \n",
    "    # Add individual channel performance\n",
    "    for i, channel in enumerate(marketing_channels, 1):\n",
    "        ch_pct = round(channel.get('pct_of_contribution', 0) * 100, 1)\n",
    "        revenue = channel.get('incremental_outcome', 0)\n",
    "        rag_content += f\"\\n- {channel.get('channel')} contributes {ch_pct}% of total revenue (${revenue:,.0f})\"\n",
    "    \n",
    "    # Add key insights for better retrieval\n",
    "    if marketing_channels:\n",
    "        top_channel = marketing_channels[0]\n",
    "        top_ch_pct = round(top_channel.get('pct_of_contribution', 0) * 100, 1)\n",
    "        rag_content += f\"\\n\\nKey Insights:\\n- {top_channel.get('channel')} is the top performing marketing channel at {top_ch_pct}%\"\n",
    "        rag_content += f\"\\n- Baseline/organic traffic dominates revenue generation at {baseline_pct}%\"\n",
    "        rag_content += f\"\\n- Marketing channels collectively contribute {total_marketing_pct}% to total revenue\"\n",
    "    \n",
    "    if chart_description_content:\n",
    "        rag_content += f\"\\n\\nMethodology: {chart_description_content}\"\n",
    "    \n",
    "    return rag_content\n",
    "\n",
    "\n",
    "print(get_channel_contribution(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marketing Channel Spend and ROI Analysis:\n",
      "\n",
      "Performance Overview:\n",
      "- Marketing channels account for 100.0% of attributed revenue\n",
      "- Total marketing spend allocation: 100.0%\n",
      "- Average ROI across all channels: 4.8x\n",
      "\n",
      "Channel Performance by ROI:\n",
      "- META: 5.1x ROI, 11.3% revenue share, 10.4% spend share\n",
      "- TIKTOK: 5.0x ROI, 0.6% revenue share, 0.5% spend share\n",
      "- GOOGLE: 4.6x ROI, 22.3% revenue share, 22.5% spend share\n",
      "- DV_360_X1: 4.6x ROI, 65.8% revenue share, 66.6% spend share\n",
      "\n",
      "Channel Efficiency Analysis:\n",
      "- Most efficient spend allocation: META (revenue/spend ratio: 1.09)\n",
      "- Least efficient spend allocation: DV_360_X1 (revenue/spend ratio: 0.99)\n",
      "\n",
      "ROI Performance:\n",
      "- Highest ROI: META at 5.1x return\n",
      "- Lowest ROI: DV_360_X1 at 4.6x return\n",
      "- ROI range: 4.6x to 5.1x across all channels\n",
      "\n",
      "Budget Allocation Insights:\n",
      "- META: Over-performing (generates 11.3% revenue with 10.4% spend)\n",
      "- TIKTOK: Over-performing (generates 0.6% revenue with 0.5% spend)\n",
      "- GOOGLE: Under-performing (generates 22.3% revenue with 22.5% spend)\n",
      "- DV_360_X1: Under-performing (generates 65.8% revenue with 66.6% spend)\n",
      "\n",
      "Methodology: Note: Return on investment is calculated by\n",
      "dividing the revenue attributed to a channel by marketing costs.\n"
     ]
    }
   ],
   "source": [
    "def get_spend_outcome_insights(soup):\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    # Find the spend-outcome chart\n",
    "    chart_embed = soup.find(\"chart-embed\", {\"id\": \"spend-outcome-chart\"})\n",
    "    if not chart_embed:\n",
    "        return \"No spend-outcome data found.\"\n",
    "    \n",
    "    # Find the parent chart element to get description and script\n",
    "    chart_element = chart_embed.find_parent(\"chart\")\n",
    "    if not chart_element:\n",
    "        return \"No spend-outcome chart element found.\"\n",
    "    \n",
    "    # Extract chart description\n",
    "    chart_description = chart_element.find(\"chart-description\")\n",
    "    chart_description_content = chart_description.get_text(strip=True) if chart_description else None\n",
    "    \n",
    "    # Extract chart data from script\n",
    "    script_tag = chart_element.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "    channel_data = []\n",
    "    \n",
    "    if script_tag:\n",
    "        script_content = script_tag.get_text()\n",
    "        \n",
    "        # Find the JSON.parse() content\n",
    "        json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                # Get the escaped JSON string and unescape it\n",
    "                escaped_json = json_match.group(1)\n",
    "                unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "                \n",
    "                # Parse the JSON\n",
    "                chart_spec = json.loads(unescaped_json)\n",
    "                \n",
    "                # Extract the dataset\n",
    "                datasets = chart_spec.get('datasets', {})\n",
    "                for dataset_key, dataset_value in datasets.items():\n",
    "                    if isinstance(dataset_value, list):\n",
    "                        channel_data = dataset_value\n",
    "                        break\n",
    "                        \n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                pass\n",
    "    \n",
    "    if not channel_data:\n",
    "        return \"No spend-outcome data could be extracted.\"\n",
    "    \n",
    "    # Process the data - separate revenue and spend data\n",
    "    revenue_data = [ch for ch in channel_data if ch.get('label') == '% Revenue']\n",
    "    spend_data = [ch for ch in channel_data if ch.get('label') == '% Spend']\n",
    "    \n",
    "    # Create channel analysis by combining revenue and spend data\n",
    "    channel_analysis = {}\n",
    "    \n",
    "    for rev_ch in revenue_data:\n",
    "        channel = rev_ch['channel']\n",
    "        spend_ch = next((s for s in spend_data if s['channel'] == channel), None)\n",
    "        \n",
    "        if spend_ch:\n",
    "            channel_analysis[channel] = {\n",
    "                'revenue_pct': rev_ch['pct'] * 100,\n",
    "                'spend_pct': spend_ch['pct'] * 100,\n",
    "                'roi': rev_ch['roi'],\n",
    "                'efficiency': rev_ch['pct'] / spend_ch['pct'] if spend_ch['pct'] > 0 else 0\n",
    "            }\n",
    "    \n",
    "    # Sort channels by ROI\n",
    "    sorted_channels = sorted(channel_analysis.items(), key=lambda x: x[1]['roi'], reverse=True)\n",
    "    \n",
    "    # Calculate totals\n",
    "    total_revenue_pct = sum(ch['revenue_pct'] for ch in channel_analysis.values())\n",
    "    total_spend_pct = sum(ch['spend_pct'] for ch in channel_analysis.values())\n",
    "    avg_roi = sum(ch['roi'] for ch in channel_analysis.values()) / len(channel_analysis)\n",
    "    \n",
    "    # Format results for RAG\n",
    "    rag_content = f\"\"\"\n",
    "Marketing Channel Spend and ROI Analysis:\n",
    "\n",
    "Performance Overview:\n",
    "- Marketing channels account for {total_revenue_pct:.1f}% of attributed revenue\n",
    "- Total marketing spend allocation: {total_spend_pct:.1f}%\n",
    "- Average ROI across all channels: {avg_roi:.1f}x\n",
    "\n",
    "Channel Performance by ROI:\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # Add individual channel performance\n",
    "    for channel, data in sorted_channels:\n",
    "        rag_content += f\"\\n- {channel.upper()}: {data['roi']:.1f}x ROI, {data['revenue_pct']:.1f}% revenue share, {data['spend_pct']:.1f}% spend share\"\n",
    "    \n",
    "    # Add efficiency insights\n",
    "    rag_content += f\"\\n\\nChannel Efficiency Analysis:\"\n",
    "    most_efficient = max(sorted_channels, key=lambda x: x[1]['efficiency'])\n",
    "    least_efficient = min(sorted_channels, key=lambda x: x[1]['efficiency'])\n",
    "    \n",
    "    rag_content += f\"\\n- Most efficient spend allocation: {most_efficient[0].upper()} (revenue/spend ratio: {most_efficient[1]['efficiency']:.2f})\"\n",
    "    rag_content += f\"\\n- Least efficient spend allocation: {least_efficient[0].upper()} (revenue/spend ratio: {least_efficient[1]['efficiency']:.2f})\"\n",
    "    \n",
    "    # ROI insights\n",
    "    best_roi_channel = sorted_channels[0]\n",
    "    worst_roi_channel = sorted_channels[-1]\n",
    "    \n",
    "    rag_content += f\"\\n\\nROI Performance:\"\n",
    "    rag_content += f\"\\n- Highest ROI: {best_roi_channel[0].upper()} at {best_roi_channel[1]['roi']:.1f}x return\"\n",
    "    rag_content += f\"\\n- Lowest ROI: {worst_roi_channel[0].upper()} at {worst_roi_channel[1]['roi']:.1f}x return\"\n",
    "    rag_content += f\"\\n- ROI range: {worst_roi_channel[1]['roi']:.1f}x to {best_roi_channel[1]['roi']:.1f}x across all channels\"\n",
    "    \n",
    "    # Budget allocation insights\n",
    "    rag_content += f\"\\n\\nBudget Allocation Insights:\"\n",
    "    for channel, data in sorted_channels:\n",
    "        if data['revenue_pct'] > data['spend_pct']:\n",
    "            rag_content += f\"\\n- {channel.upper()}: Over-performing (generates {data['revenue_pct']:.1f}% revenue with {data['spend_pct']:.1f}% spend)\"\n",
    "        elif data['revenue_pct'] < data['spend_pct']:\n",
    "            rag_content += f\"\\n- {channel.upper()}: Under-performing (generates {data['revenue_pct']:.1f}% revenue with {data['spend_pct']:.1f}% spend)\"\n",
    "        else:\n",
    "            rag_content += f\"\\n- {channel.upper()}: Balanced performance (revenue and spend percentages aligned)\"\n",
    "    \n",
    "    if chart_description_content:\n",
    "        rag_content += f\"\\n\\nMethodology: {chart_description_content}\"\n",
    "\n",
    "    return rag_content\n",
    "\n",
    "print(get_spend_outcome_insights(soup))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "import calendar\n",
    "\n",
    "def get_channel_time_insights_with_anomalies(soup):\n",
    "    \n",
    "    # [Same extraction logic as before...]\n",
    "    chart_embed = soup.find(\"chart-embed\", {\"id\": \"channel-contrib-by-time-chart\"})\n",
    "    if not chart_embed:\n",
    "        return \"No time-series channel contribution data found.\"\n",
    "    \n",
    "    chart_element = chart_embed.find_parent(\"chart\")\n",
    "    if not chart_element:\n",
    "        return \"No time-series chart element found.\"\n",
    "    \n",
    "    chart_description = chart_element.find(\"chart-description\")\n",
    "    chart_description_content = chart_description.get_text(strip=True) if chart_description else None\n",
    "    \n",
    "    script_tag = chart_element.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "    time_data = []\n",
    "    \n",
    "    if script_tag:\n",
    "        script_content = script_tag.get_text()\n",
    "        json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                escaped_json = json_match.group(1)\n",
    "                unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "                chart_spec = json.loads(unescaped_json)\n",
    "                datasets = chart_spec.get('datasets', {})\n",
    "                for dataset_key, dataset_value in datasets.items():\n",
    "                    if isinstance(dataset_value, list):\n",
    "                        time_data = dataset_value\n",
    "                        break\n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                pass\n",
    "    \n",
    "    if not time_data:\n",
    "        return \"No time-series data could be extracted.\"\n",
    "    \n",
    "    # Organize data by channel and time\n",
    "    channel_monthly_data = defaultdict(lambda: defaultdict(dict))\n",
    "    channel_quarterly_data = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    for record in time_data:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(record['time'], '%Y-%m-%d')\n",
    "            revenue = record.get('incremental_outcome', 0) or 0\n",
    "            contribution_pct = record.get('pct_of_contribution', 0) * 100\n",
    "            channel = record['channel']\n",
    "            \n",
    "            # Monthly data by channel\n",
    "            month_key = date_obj.strftime('%Y-%m')\n",
    "            channel_monthly_data[channel][month_key] = {\n",
    "                'revenue': revenue,\n",
    "                'contribution_pct': contribution_pct,\n",
    "                'date': date_obj\n",
    "            }\n",
    "            \n",
    "            # Quarterly data by channel\n",
    "            quarter = f\"Q{(date_obj.month - 1) // 3 + 1}\"\n",
    "            quarter_key = f\"{date_obj.year}-{quarter}\"\n",
    "            \n",
    "            # Keep the latest data point for each quarter\n",
    "            if (quarter_key not in channel_quarterly_data[channel] or \n",
    "                date_obj > channel_quarterly_data[channel][quarter_key].get('date', datetime.min)):\n",
    "                channel_quarterly_data[channel][quarter_key] = {\n",
    "                    'revenue': revenue,\n",
    "                    'contribution_pct': contribution_pct,\n",
    "                    'date': date_obj\n",
    "                }\n",
    "                \n",
    "        except (ValueError, KeyError):\n",
    "            continue\n",
    "    \n",
    "    # Generate comprehensive summaries\n",
    "    chunks = []\n",
    "    \n",
    "    # Chunk 1: Monthly Performance by Channel with Anomalies\n",
    "    chunk1 = create_monthly_channel_summaries_with_anomalies(channel_monthly_data)\n",
    "    chunks.append(chunk1)\n",
    "    \n",
    "    # Chunk 2: Quarterly Performance by Channel with Trends\n",
    "    chunk2 = create_quarterly_channel_summaries_with_trends(channel_quarterly_data)\n",
    "    chunks.append(chunk2)\n",
    "    \n",
    "    # Chunk 3: Spike and Dip Analysis Across All Channels\n",
    "    chunk3 = create_anomaly_analysis(channel_monthly_data)\n",
    "    chunks.append(chunk3)\n",
    "    \n",
    "    # Chunk 4: Channel Performance Comparison and Rankings\n",
    "    chunk4 = create_channel_comparison_analysis(channel_monthly_data, channel_quarterly_data)\n",
    "    chunks.append(chunk4)\n",
    "    \n",
    "    # Chunk 5: Growth Momentum and Trend Analysis\n",
    "    chunk5 = create_momentum_analysis(channel_monthly_data, channel_quarterly_data)\n",
    "    chunks.append(chunk5)\n",
    "    \n",
    "    # Create formatted output\n",
    "    formatted_output = []\n",
    "    \n",
    "    formatted_output.append(\"=== MONTHLY CHANNEL PERFORMANCE WITH ANOMALIES ===\")\n",
    "    formatted_output.append(chunks[0])\n",
    "    formatted_output.append(\"\\n=== QUARTERLY CHANNEL TRENDS ===\")\n",
    "    formatted_output.append(chunks[1])\n",
    "    formatted_output.append(\"\\n=== SPIKE AND DIP ANALYSIS ===\")\n",
    "    formatted_output.append(chunks[2])\n",
    "    formatted_output.append(\"\\n=== CHANNEL COMPARISON ===\")\n",
    "    formatted_output.append(chunks[3])\n",
    "    formatted_output.append(\"\\n=== MOMENTUM ANALYSIS ===\")\n",
    "    formatted_output.append(chunks[4])\n",
    "    \n",
    "    # Return both individual chunks and formatted output\n",
    "    return {\n",
    "        'chunks': chunks,\n",
    "        'formatted_output': '\\n'.join(formatted_output),\n",
    "        'summary_sections': {\n",
    "            'monthly_anomalies': chunks[0],\n",
    "            'quarterly_trends': chunks[1],\n",
    "            'spike_dip_analysis': chunks[2],\n",
    "            'channel_comparison': chunks[3],\n",
    "            'momentum_analysis': chunks[4]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def detect_spikes_and_dips(revenue_series, threshold_multiplier=1.5):\n",
    "    \"\"\"Detect spikes and dips in revenue series\"\"\"\n",
    "    if len(revenue_series) < 3:\n",
    "        return [], []\n",
    "    \n",
    "    # Calculate moving average and standard deviation\n",
    "    values = [v for v in revenue_series.values() if v > 0]\n",
    "    if len(values) < 2:\n",
    "        return [], []\n",
    "    \n",
    "    mean_revenue = statistics.mean(values)\n",
    "    std_revenue = statistics.stdev(values) if len(values) > 1 else 0\n",
    "    \n",
    "    spikes = []\n",
    "    dips = []\n",
    "    \n",
    "    for period, revenue in revenue_series.items():\n",
    "        if revenue > 0:\n",
    "            # Spike detection: revenue > mean + (threshold * std)\n",
    "            if revenue > mean_revenue + (threshold_multiplier * std_revenue):\n",
    "                spike_magnitude = ((revenue - mean_revenue) / mean_revenue) * 100\n",
    "                spikes.append({\n",
    "                    'period': period,\n",
    "                    'revenue': revenue,\n",
    "                    'magnitude': spike_magnitude\n",
    "                })\n",
    "            \n",
    "            # Dip detection: revenue < mean - (threshold * std) and significantly below mean\n",
    "            elif revenue < mean_revenue - (threshold_multiplier * std_revenue) and revenue < mean_revenue * 0.5:\n",
    "                dip_magnitude = ((mean_revenue - revenue) / mean_revenue) * 100\n",
    "                dips.append({\n",
    "                    'period': period,\n",
    "                    'revenue': revenue,\n",
    "                    'magnitude': dip_magnitude\n",
    "                })\n",
    "    \n",
    "    return spikes, dips\n",
    "\n",
    "def create_monthly_channel_summaries_with_anomalies(channel_monthly_data):\n",
    "    \"\"\"Create monthly summaries by channel with spike/dip detection\"\"\"\n",
    "    \n",
    "    summary = \"Monthly Channel Performance Analysis with Anomaly Detection:\\n\"\n",
    "    \n",
    "    for channel in sorted(channel_monthly_data.keys()):\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        monthly_data = channel_monthly_data[channel]\n",
    "        if not monthly_data:\n",
    "            continue\n",
    "        \n",
    "        # Get revenue series for spike/dip detection\n",
    "        revenue_series = {month: data['revenue'] for month, data in monthly_data.items()}\n",
    "        spikes, dips = detect_spikes_and_dips(revenue_series)\n",
    "        \n",
    "        # Calculate basic stats\n",
    "        revenues = [data['revenue'] for data in monthly_data.values() if data['revenue'] > 0]\n",
    "        if not revenues:\n",
    "            continue\n",
    "            \n",
    "        avg_revenue = statistics.mean(revenues)\n",
    "        max_revenue = max(revenues)\n",
    "        min_revenue = min(revenues)\n",
    "        \n",
    "        # Find peak and trough months\n",
    "        peak_month = max(monthly_data.items(), key=lambda x: x[1]['revenue'])\n",
    "        trough_month = min(monthly_data.items(), key=lambda x: x[1]['revenue'])\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "{channel.upper()} - Monthly Performance:\n",
    "- Average Monthly Revenue: ${avg_revenue:,.0f}\n",
    "- Peak Month: {peak_month[0]} (${peak_month[1]['revenue']:,.0f})\n",
    "- Lowest Month: {trough_month[0]} (${trough_month[1]['revenue']:,.0f})\n",
    "- Active Months: {len([r for r in revenues if r > 0])} out of {len(monthly_data)}\n",
    "\"\"\"\n",
    "        \n",
    "        # Add spike analysis\n",
    "        if spikes:\n",
    "            summary += f\"- Revenue Spikes Detected: {len(spikes)}\\n\"\n",
    "            for spike in sorted(spikes, key=lambda x: x['magnitude'], reverse=True)[:3]:\n",
    "                summary += f\"  • {spike['period']}: ${spike['revenue']:,.0f} (+{spike['magnitude']:.0f}% above average)\\n\"\n",
    "        \n",
    "        # Add dip analysis\n",
    "        if dips:\n",
    "            summary += f\"- Revenue Dips Detected: {len(dips)}\\n\"\n",
    "            for dip in sorted(dips, key=lambda x: x['magnitude'], reverse=True)[:3]:\n",
    "                summary += f\"  • {dip['period']}: ${dip['revenue']:,.0f} (-{dip['magnitude']:.0f}% below average)\\n\"\n",
    "        \n",
    "        # Month-over-month growth analysis\n",
    "        sorted_months = sorted(monthly_data.keys())\n",
    "        growth_periods = []\n",
    "        decline_periods = []\n",
    "        \n",
    "        for i in range(1, len(sorted_months)):\n",
    "            current_month = sorted_months[i]\n",
    "            prev_month = sorted_months[i-1]\n",
    "            \n",
    "            current_rev = monthly_data[current_month]['revenue']\n",
    "            prev_rev = monthly_data[prev_month]['revenue']\n",
    "            \n",
    "            if prev_rev > 0 and current_rev > 0:\n",
    "                growth_rate = ((current_rev - prev_rev) / prev_rev) * 100\n",
    "                if growth_rate > 50:  # Significant growth\n",
    "                    growth_periods.append(f\"{current_month} (+{growth_rate:.0f}%)\")\n",
    "                elif growth_rate < -50:  # Significant decline\n",
    "                    decline_periods.append(f\"{current_month} ({growth_rate:.0f}%)\")\n",
    "        \n",
    "        if growth_periods:\n",
    "            summary += f\"- High Growth Periods: {', '.join(growth_periods[:3])}\\n\"\n",
    "        if decline_periods:\n",
    "            summary += f\"- Decline Periods: {', '.join(decline_periods[:3])}\\n\"\n",
    "        \n",
    "        summary += \"\\n\"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "def create_quarterly_channel_summaries_with_trends(channel_quarterly_data):\n",
    "    \"\"\"Create quarterly summaries by channel with trend analysis\"\"\"\n",
    "    \n",
    "    summary = \"Quarterly Channel Performance Analysis with Trend Detection:\\n\"\n",
    "    \n",
    "    for channel in sorted(channel_quarterly_data.keys()):\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        quarterly_data = channel_quarterly_data[channel]\n",
    "        if not quarterly_data:\n",
    "            continue\n",
    "        \n",
    "        sorted_quarters = sorted(quarterly_data.keys())\n",
    "        if len(sorted_quarters) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Calculate quarterly trends\n",
    "        revenues = [quarterly_data[q]['revenue'] for q in sorted_quarters]\n",
    "        \n",
    "        # Trend analysis\n",
    "        trend_direction = \"stable\"\n",
    "        if len(revenues) >= 3:\n",
    "            recent_trend = revenues[-3:]\n",
    "            if all(recent_trend[i] < recent_trend[i+1] for i in range(len(recent_trend)-1)):\n",
    "                trend_direction = \"consistently growing\"\n",
    "            elif all(recent_trend[i] > recent_trend[i+1] for i in range(len(recent_trend)-1)):\n",
    "                trend_direction = \"consistently declining\"\n",
    "            elif revenues[-1] > revenues[0]:\n",
    "                trend_direction = \"overall growth\"\n",
    "            elif revenues[-1] < revenues[0]:\n",
    "                trend_direction = \"overall decline\"\n",
    "        \n",
    "        # Quarter-over-quarter growth rates\n",
    "        qoq_growth = []\n",
    "        for i in range(1, len(sorted_quarters)):\n",
    "            current_rev = quarterly_data[sorted_quarters[i]]['revenue']\n",
    "            prev_rev = quarterly_data[sorted_quarters[i-1]]['revenue']\n",
    "            \n",
    "            if prev_rev > 0:\n",
    "                growth_rate = ((current_rev - prev_rev) / prev_rev) * 100\n",
    "                qoq_growth.append((sorted_quarters[i], growth_rate))\n",
    "        \n",
    "        # Find best and worst quarters\n",
    "        best_quarter = max(quarterly_data.items(), key=lambda x: x[1]['revenue'])\n",
    "        worst_quarter = min(quarterly_data.items(), key=lambda x: x[1]['revenue'])\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "{channel.upper()} - Quarterly Analysis:\n",
    "- Trend Direction: {trend_direction.title()}\n",
    "- Best Quarter: {best_quarter[0]} (${best_quarter[1]['revenue']:,.0f})\n",
    "- Worst Quarter: {worst_quarter[0]} (${worst_quarter[1]['revenue']:,.0f})\n",
    "- Total Quarters Active: {len([r for r in revenues if r > 0])}\n",
    "\"\"\"\n",
    "        \n",
    "        # Add significant QoQ changes\n",
    "        significant_growth = [q for q, g in qoq_growth if g > 100]\n",
    "        significant_decline = [q for q, g in qoq_growth if g < -50]\n",
    "        \n",
    "        if significant_growth:\n",
    "            summary += f\"- Major Growth Quarters: {', '.join(significant_growth)}\\n\"\n",
    "        if significant_decline:\n",
    "            summary += f\"- Major Decline Quarters: {', '.join(significant_decline)}\\n\"\n",
    "        \n",
    "        # Recent performance (last 2 quarters)\n",
    "        if len(sorted_quarters) >= 2:\n",
    "            recent_quarters = sorted_quarters[-2:]\n",
    "            recent_avg = statistics.mean([quarterly_data[q]['revenue'] for q in recent_quarters])\n",
    "            summary += f\"- Recent Performance (Last 2Q): ${recent_avg:,.0f} average\\n\"\n",
    "        \n",
    "        summary += \"\\n\"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "def create_anomaly_analysis(channel_monthly_data):\n",
    "    \"\"\"Create comprehensive spike and dip analysis across all channels\"\"\"\n",
    "    \n",
    "    summary = \"Revenue Anomaly Analysis - Spikes and Dips Across All Channels:\\n\"\n",
    "    \n",
    "    all_spikes = []\n",
    "    all_dips = []\n",
    "    \n",
    "    # Collect all spikes and dips across channels\n",
    "    for channel, monthly_data in channel_monthly_data.items():\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        revenue_series = {month: data['revenue'] for month, data in monthly_data.items()}\n",
    "        spikes, dips = detect_spikes_and_dips(revenue_series)\n",
    "        \n",
    "        for spike in spikes:\n",
    "            spike['channel'] = channel\n",
    "            all_spikes.append(spike)\n",
    "            \n",
    "        for dip in dips:\n",
    "            dip['channel'] = channel\n",
    "            all_dips.append(dip)\n",
    "    \n",
    "    # Analyze biggest spikes\n",
    "    if all_spikes:\n",
    "        summary += f\"\\nTop Revenue Spikes (Highest Magnitude):\\n\"\n",
    "        top_spikes = sorted(all_spikes, key=lambda x: x['magnitude'], reverse=True)[:5]\n",
    "        for i, spike in enumerate(top_spikes, 1):\n",
    "            summary += f\"{i}. {spike['channel'].upper()} in {spike['period']}: ${spike['revenue']:,.0f} (+{spike['magnitude']:.0f}% above average)\\n\"\n",
    "    \n",
    "    # Analyze biggest dips\n",
    "    if all_dips:\n",
    "        summary += f\"\\nSignificant Revenue Dips:\\n\"\n",
    "        top_dips = sorted(all_dips, key=lambda x: x['magnitude'], reverse=True)[:5]\n",
    "        for i, dip in enumerate(top_dips, 1):\n",
    "            summary += f\"{i}. {dip['channel'].upper()} in {dip['period']}: ${dip['revenue']:,.0f} (-{dip['magnitude']:.0f}% below average)\\n\"\n",
    "    \n",
    "    # Seasonal spike analysis\n",
    "    spike_months = defaultdict(int)\n",
    "    dip_months = defaultdict(int)\n",
    "    \n",
    "    for spike in all_spikes:\n",
    "        try:\n",
    "            month_num = int(spike['period'].split('-')[1])\n",
    "            spike_months[month_num] += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for dip in all_dips:\n",
    "        try:\n",
    "            month_num = int(dip['period'].split('-')[1])\n",
    "            dip_months[month_num] += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if spike_months:\n",
    "        peak_spike_month = max(spike_months.items(), key=lambda x: x[1])\n",
    "        month_name = calendar.month_name[peak_spike_month[0]]\n",
    "        summary += f\"\\nSeasonal Patterns:\\n\"\n",
    "        summary += f\"- Most Common Spike Month: {month_name} ({peak_spike_month[1]} occurrences)\\n\"\n",
    "    \n",
    "    if dip_months:\n",
    "        peak_dip_month = max(dip_months.items(), key=lambda x: x[1])\n",
    "        month_name = calendar.month_name[peak_dip_month[0]]\n",
    "        summary += f\"- Most Common Dip Month: {month_name} ({peak_dip_month[1]} occurrences)\\n\"\n",
    "    \n",
    "    # Channel volatility analysis\n",
    "    channel_volatility = {}\n",
    "    for channel, monthly_data in channel_monthly_data.items():\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        revenues = [data['revenue'] for data in monthly_data.values() if data['revenue'] > 0]\n",
    "        if len(revenues) > 1:\n",
    "            avg_rev = statistics.mean(revenues)\n",
    "            std_rev = statistics.stdev(revenues)\n",
    "            volatility = (std_rev / avg_rev) * 100 if avg_rev > 0 else 0\n",
    "            channel_volatility[channel] = volatility\n",
    "    \n",
    "    if channel_volatility:\n",
    "        summary += f\"\\nChannel Volatility Analysis (Revenue Consistency):\\n\"\n",
    "        sorted_volatility = sorted(channel_volatility.items(), key=lambda x: x[1])\n",
    "        \n",
    "        most_stable = sorted_volatility[0]\n",
    "        most_volatile = sorted_volatility[-1]\n",
    "        \n",
    "        summary += f\"- Most Stable Channel: {most_stable[0].upper()} ({most_stable[1]:.0f}% volatility)\\n\"\n",
    "        summary += f\"- Most Volatile Channel: {most_volatile[0].upper()} ({most_volatile[1]:.0f}% volatility)\\n\"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "def create_channel_comparison_analysis(channel_monthly_data, channel_quarterly_data):\n",
    "    \"\"\"Create channel performance comparison and rankings\"\"\"\n",
    "    \n",
    "    summary = \"Channel Performance Comparison and Rankings:\\n\"\n",
    "    \n",
    "    # Calculate total revenue by channel\n",
    "    channel_totals = {}\n",
    "    channel_consistency = {}\n",
    "    channel_peak_performance = {}\n",
    "    \n",
    "    for channel, monthly_data in channel_monthly_data.items():\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        revenues = [data['revenue'] for data in monthly_data.values()]\n",
    "        active_revenues = [r for r in revenues if r > 0]\n",
    "        \n",
    "        if active_revenues:\n",
    "            channel_totals[channel] = sum(active_revenues)\n",
    "            channel_consistency[channel] = len(active_revenues) / len(monthly_data) * 100\n",
    "            channel_peak_performance[channel] = max(active_revenues)\n",
    "    \n",
    "    # Rankings\n",
    "    if channel_totals:\n",
    "        summary += f\"\\nTotal Revenue Rankings:\\n\"\n",
    "        revenue_ranking = sorted(channel_totals.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (channel, total_rev) in enumerate(revenue_ranking, 1):\n",
    "            summary += f\"{i}. {channel.upper()}: ${total_rev:,.0f} cumulative revenue\\n\"\n",
    "    \n",
    "    if channel_consistency:\n",
    "        summary += f\"\\nConsistency Rankings (% of months active):\\n\"\n",
    "        consistency_ranking = sorted(channel_consistency.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (channel, consistency) in enumerate(consistency_ranking, 1):\n",
    "            summary += f\"{i}. {channel.upper()}: {consistency:.0f}% of months with revenue\\n\"\n",
    "    \n",
    "    if channel_peak_performance:\n",
    "        summary += f\"\\nPeak Performance Rankings (Highest single month):\\n\"\n",
    "        peak_ranking = sorted(channel_peak_performance.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (channel, peak_rev) in enumerate(peak_ranking, 1):\n",
    "            summary += f\"{i}. {channel.upper()}: ${peak_rev:,.0f} peak monthly revenue\\n\"\n",
    "    \n",
    "    # Channel lifecycle analysis\n",
    "    summary += f\"\\nChannel Lifecycle Analysis:\\n\"\n",
    "    \n",
    "    for channel, monthly_data in channel_monthly_data.items():\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        sorted_months = sorted(monthly_data.keys())\n",
    "        active_months = [month for month in sorted_months if monthly_data[month]['revenue'] > 0]\n",
    "        \n",
    "        if active_months:\n",
    "            launch_month = active_months[0]\n",
    "            latest_month = active_months[-1]\n",
    "            \n",
    "            # Determine lifecycle stage\n",
    "            recent_months = sorted_months[-3:] if len(sorted_months) >= 3 else sorted_months\n",
    "            recent_activity = sum(1 for month in recent_months if monthly_data[month]['revenue'] > 0)\n",
    "            \n",
    "            if recent_activity == 0:\n",
    "                stage = \"Dormant\"\n",
    "            elif len(active_months) <= 3:\n",
    "                stage = \"Launch Phase\"\n",
    "            elif recent_activity == len(recent_months):\n",
    "                stage = \"Active/Mature\"\n",
    "            else:\n",
    "                stage = \"Intermittent\"\n",
    "            \n",
    "            summary += f\"- {channel.upper()}: {stage} (Active: {launch_month} to {latest_month})\\n\"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "def create_momentum_analysis(channel_monthly_data, channel_quarterly_data):\n",
    "    \"\"\"Create growth momentum and trend analysis\"\"\"\n",
    "    \n",
    "    summary = \"Channel Growth Momentum and Trend Analysis:\\n\"\n",
    "    \n",
    "    for channel, monthly_data in channel_monthly_data.items():\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        sorted_months = sorted(monthly_data.keys())\n",
    "        if len(sorted_months) < 6:  # Need at least 6 months for momentum analysis\n",
    "            continue\n",
    "        \n",
    "        # Get last 6 months of data\n",
    "        recent_months = sorted_months[-6:]\n",
    "        recent_revenues = [monthly_data[month]['revenue'] for month in recent_months]\n",
    "        \n",
    "        # Calculate momentum indicators\n",
    "        first_half_avg = statistics.mean(recent_revenues[:3])\n",
    "        second_half_avg = statistics.mean(recent_revenues[3:])\n",
    "        \n",
    "        momentum = \"neutral\"\n",
    "        momentum_pct = 0\n",
    "        \n",
    "        if first_half_avg > 0:\n",
    "            momentum_pct = ((second_half_avg - first_half_avg) / first_half_avg) * 100\n",
    "            \n",
    "            if momentum_pct > 20:\n",
    "                momentum = \"strong positive\"\n",
    "            elif momentum_pct > 5:\n",
    "                momentum = \"positive\"\n",
    "            elif momentum_pct < -20:\n",
    "                momentum = \"strong negative\"\n",
    "            elif momentum_pct < -5:\n",
    "                momentum = \"negative\"\n",
    "        \n",
    "        # Trend consistency\n",
    "        positive_months = sum(1 for i in range(1, len(recent_revenues)) \n",
    "                             if recent_revenues[i] > recent_revenues[i-1])\n",
    "        trend_consistency = (positive_months / (len(recent_revenues) - 1)) * 100\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "{channel.upper()} - 6-Month Momentum Analysis:\n",
    "- Growth Momentum: {momentum.title()} ({momentum_pct:+.0f}%)\n",
    "- Trend Consistency: {trend_consistency:.0f}% of months showed growth\n",
    "- Recent Average (Last 3 months): ${second_half_avg:,.0f}\n",
    "- Previous Average (3 months prior): ${first_half_avg:,.0f}\n",
    "\"\"\"\n",
    "        \n",
    "    \n",
    "        \n",
    "        summary += \"\\n\"\n",
    "    \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MONTHLY CHANNEL PERFORMANCE WITH ANOMALIES ===\n",
      "Monthly Channel Performance Analysis with Anomaly Detection:\n",
      "\n",
      "DV_360_X1 - Monthly Performance:\n",
      "- Average Monthly Revenue: $161,021\n",
      "- Peak Month: 2024-08 ($286,080)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 10 out of 30\n",
      "- Decline Periods: 2025-01 (-55%), 2025-02 (-80%)\n",
      "\n",
      "\n",
      "GOOGLE - Monthly Performance:\n",
      "- Average Monthly Revenue: $35,335\n",
      "- Peak Month: 2023-10 ($71,109)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 16 out of 30\n",
      "- High Growth Periods: 2024-05 (+1598%), 2024-10 (+220%)\n",
      "- Decline Periods: 2024-01 (-74%), 2024-02 (-82%), 2024-09 (-77%)\n",
      "\n",
      "\n",
      "META - Monthly Performance:\n",
      "- Average Monthly Revenue: $20,639\n",
      "- Peak Month: 2024-05 ($35,488)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 14 out of 30\n",
      "- High Growth Periods: 2024-05 (+79%)\n",
      "- Decline Periods: 2025-01 (-58%), 2025-02 (-63%)\n",
      "\n",
      "\n",
      "TIKTOK - Monthly Performance:\n",
      "- Average Monthly Revenue: $3,254\n",
      "- Peak Month: 2024-12 ($7,722)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 4 out of 30\n",
      "- High Growth Periods: 2024-12 (+208%)\n",
      "- Decline Periods: 2025-01 (-70%), 2025-02 (-80%)\n",
      "\n",
      "=== QUARTERLY CHANNEL TRENDS ===\n",
      "Quarterly Channel Performance Analysis with Trend Detection:\n",
      "\n",
      "DV_360_X1 - Quarterly Analysis:\n",
      "- Trend Direction: Stable\n",
      "- Best Quarter: 2024-Q3 ($276,999)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 2\n",
      "- Major Decline Quarters: 2025-Q1\n",
      "- Recent Performance (Last 2Q): $0 average\n",
      "\n",
      "\n",
      "GOOGLE - Quarterly Analysis:\n",
      "- Trend Direction: Stable\n",
      "- Best Quarter: 2023-Q4 ($66,506)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 4\n",
      "- Major Growth Quarters: 2024-Q4\n",
      "- Major Decline Quarters: 2024-Q1, 2024-Q3, 2025-Q1\n",
      "- Recent Performance (Last 2Q): $0 average\n",
      "\n",
      "\n",
      "META - Quarterly Analysis:\n",
      "- Trend Direction: Consistently Declining\n",
      "- Best Quarter: 2024-Q2 ($34,550)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 5\n",
      "- Major Decline Quarters: 2025-Q1\n",
      "- Recent Performance (Last 2Q): $5,039 average\n",
      "\n",
      "\n",
      "TIKTOK - Quarterly Analysis:\n",
      "- Trend Direction: Stable\n",
      "- Best Quarter: 2024-Q4 ($7,722)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 1\n",
      "- Major Decline Quarters: 2025-Q1\n",
      "- Recent Performance (Last 2Q): $0 average\n",
      "\n",
      "=== SPIKE AND DIP ANALYSIS ===\n",
      "Revenue Anomaly Analysis - Spikes and Dips Across All Channels:\n",
      "\n",
      "Channel Volatility Analysis (Revenue Consistency):\n",
      "- Most Stable Channel: META (58% volatility)\n",
      "- Most Volatile Channel: TIKTOK (96% volatility)\n",
      "\n",
      "=== CHANNEL COMPARISON ===\n",
      "Channel Performance Comparison and Rankings:\n",
      "\n",
      "Total Revenue Rankings:\n",
      "1. DV_360_X1: $1,610,209 cumulative revenue\n",
      "2. GOOGLE: $565,360 cumulative revenue\n",
      "3. META: $288,948 cumulative revenue\n",
      "4. TIKTOK: $13,017 cumulative revenue\n",
      "\n",
      "Consistency Rankings (% of months active):\n",
      "1. GOOGLE: 53% of months with revenue\n",
      "2. META: 47% of months with revenue\n",
      "3. DV_360_X1: 33% of months with revenue\n",
      "4. TIKTOK: 13% of months with revenue\n",
      "\n",
      "Peak Performance Rankings (Highest single month):\n",
      "1. DV_360_X1: $286,080 peak monthly revenue\n",
      "2. GOOGLE: $71,109 peak monthly revenue\n",
      "3. META: $35,488 peak monthly revenue\n",
      "4. TIKTOK: $7,722 peak monthly revenue\n",
      "\n",
      "Channel Lifecycle Analysis:\n",
      "- DV_360_X1: Dormant (Active: 2024-01 to 2025-02)\n",
      "- GOOGLE: Dormant (Active: 2023-10 to 2025-02)\n",
      "- META: Active/Mature (Active: 2024-04 to 2025-05)\n",
      "- TIKTOK: Dormant (Active: 2024-11 to 2025-02)\n",
      "\n",
      "=== MOMENTUM ANALYSIS ===\n",
      "Channel Growth Momentum and Trend Analysis:\n",
      "\n",
      "DV_360_X1 - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-100%)\n",
      "- Trend Consistency: 0% of months showed growth\n",
      "- Recent Average (Last 3 months): $0\n",
      "- Previous Average (3 months prior): $108,797\n",
      "\n",
      "\n",
      "GOOGLE - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-100%)\n",
      "- Trend Consistency: 0% of months showed growth\n",
      "- Recent Average (Last 3 months): $0\n",
      "- Previous Average (3 months prior): $18,947\n",
      "\n",
      "\n",
      "META - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-67%)\n",
      "- Trend Consistency: 20% of months showed growth\n",
      "- Recent Average (Last 3 months): $4,841\n",
      "- Previous Average (3 months prior): $14,516\n",
      "\n",
      "\n",
      "TIKTOK - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-100%)\n",
      "- Trend Consistency: 0% of months showed growth\n",
      "- Recent Average (Last 3 months): $0\n",
      "- Previous Average (3 months prior): $3,504\n"
     ]
    }
   ],
   "source": [
    "result = get_channel_time_insights_with_anomalies(soup)\n",
    "\n",
    "print(result['formatted_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Return on investment\n",
      "\n",
      "Insights: Your return on investment (ROI) helps\n",
      "you understand how your marketing activities impacted your business objectives.\n",
      "Meta drove the highest ROI at 5.1. For every $1\n",
      "you spent on Meta, you saw $5.11 in revenue.\n",
      "Dv_360_X1 had the highest effectiveness, which is your\n",
      "incremental outcome per media unit. Meta had the highest marginal\n",
      "ROI at 2.08. Meta drove the lowest CPIK\n",
      "at $0.20. For every KPI unit, you spent $0.20.\n"
     ]
    }
   ],
   "source": [
    "def get_roi_insights(soup):\n",
    "    \"\"\"Extract ROI title and insights text\"\"\"\n",
    "    \n",
    "    # Find the ROI card\n",
    "    roi_card = soup.find(\"card\", {\"id\": \"performance-breakdown\"})\n",
    "    if not roi_card:\n",
    "        return \"No ROI performance data found.\"\n",
    "    \n",
    "    # Extract card title\n",
    "    card_title = roi_card.find(\"card-title\")\n",
    "    card_title_content = card_title.get_text(strip=True) if card_title else None\n",
    "    \n",
    "    # Extract insights text\n",
    "    insights_text = roi_card.find(\"p\", {\"class\": \"insights-text\"})\n",
    "    insights_text_content = insights_text.get_text(strip=True) if insights_text else None\n",
    "    \n",
    "    if not card_title_content and not insights_text_content:\n",
    "        return \"No ROI data found.\"\n",
    "    \n",
    "    # Format output\n",
    "    output = \"\"\n",
    "    if card_title_content:\n",
    "        output += f\"Title: {card_title_content}\\n\\n\"\n",
    "    \n",
    "    if insights_text_content:\n",
    "        output += f\"Insights: {insights_text_content}\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(get_roi_insights(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI vs Effectiveness Analysis:\n",
      "\n",
      "Performance Overview:\n",
      "- Average ROI across channels: 4.8x\n",
      "- Average effectiveness: 0.0169 incremental outcome per impression\n",
      "- Total media spend analyzed: $2,340,830\n",
      "\n",
      "ROI Rankings:\n",
      "1. META: 5.1x ROI\n",
      "2. TIKTOK: 5.0x ROI\n",
      "3. GOOGLE: 4.6x ROI\n",
      "4. DV_360_X1: 4.6x ROI\n",
      "\n",
      "Effectiveness Rankings:\n",
      "1. DV_360_X1: 0.0491 outcome per impression\n",
      "2. GOOGLE: 0.0065 outcome per impression\n",
      "3. META: 0.0062 outcome per impression\n",
      "4. TIKTOK: 0.0057 outcome per impression\n",
      "\n",
      "Spend Allocation:\n",
      "1. DV_360_X1: $1,558,208 (66.6% of total spend)\n",
      "2. GOOGLE: $526,693 (22.5% of total spend)\n",
      "3. META: $243,623 (10.4% of total spend)\n",
      "4. TIKTOK: $12,306 (0.5% of total spend)\n",
      "\n",
      "Channel Performance Categories:\n",
      "\n",
      "High Potential:\n",
      "- DV_360_X1 (4.6x ROI, 0.0491 effectiveness)\n",
      "\n",
      "Optimization Needed:\n",
      "- GOOGLE (4.6x ROI, 0.0065 effectiveness)\n",
      "\n",
      "Cost Efficient:\n",
      "- META (5.1x ROI, 0.0062 effectiveness)\n",
      "- TIKTOK (5.0x ROI, 0.0057 effectiveness)\n",
      "\n",
      "Strategic Insights:\n",
      "- META delivers highest ROI (5.1x) - prioritize for budget allocation\n",
      "- DV_360_X1 shows highest effectiveness (0.0491) - strong media performance per impression\n",
      "- DV_360_X1 receives largest budget ($1,558,208) - monitor efficiency closely\n",
      "- DV_360_X1: High effectiveness but expensive - optimize costs to improve ROI\n",
      "- GOOGLE: Both ROI and effectiveness below average - requires optimization or budget reallocation\n",
      "- META: Cost efficient but low reach - consider scaling if effectiveness can be maintained\n",
      "- TIKTOK: Cost efficient but low reach - consider scaling if effectiveness can be maintained\n",
      "\n",
      "Methodology: Note: Effectiveness measures the\n",
      "incremental outcome generated per impression. A low ROI does not necessarily\n",
      "imply low media effectiveness; it may result from high media cost, as positioned\n",
      "in the upper-left corner of the chart. Conversely, a high ROI can coexist with\n",
      "low media effectiveness and low media costs, as indicated in the bottom-right\n",
      "corner of the chart. The diagonal section of the chart suggests that the ROI is\n",
      "primarily influenced by media effectiveness. The size of the bubbles represents\n",
      "the scale of the media spend.\n"
     ]
    }
   ],
   "source": [
    "def get_roi_effectiveness_insights(soup):\n",
    "    \"\"\"Extract ROI vs Effectiveness chart insights\"\"\"\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    # Find the ROI effectiveness chart\n",
    "    chart_embed = soup.find(\"chart-embed\", {\"id\": \"roi-effectiveness-chart\"})\n",
    "    if not chart_embed:\n",
    "        return \"No ROI effectiveness chart found.\"\n",
    "    \n",
    "    # Find the parent chart element\n",
    "    chart_element = chart_embed.find_parent(\"chart\")\n",
    "    if not chart_element:\n",
    "        return \"No ROI effectiveness chart element found.\"\n",
    "    \n",
    "    # Extract chart description\n",
    "    chart_description = chart_element.find(\"chart-description\")\n",
    "    chart_description_content = chart_description.get_text(strip=True) if chart_description else None\n",
    "    \n",
    "    # Extract chart data from script\n",
    "    script_tag = chart_element.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "    chart_data = []\n",
    "    \n",
    "    if script_tag:\n",
    "        script_content = script_tag.get_text()\n",
    "        json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                escaped_json = json_match.group(1)\n",
    "                unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "                chart_spec = json.loads(unescaped_json)\n",
    "                \n",
    "                datasets = chart_spec.get('datasets', {})\n",
    "                for dataset_key, dataset_value in datasets.items():\n",
    "                    if isinstance(dataset_value, list):\n",
    "                        chart_data = dataset_value\n",
    "                        break\n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                pass\n",
    "    \n",
    "    if not chart_data:\n",
    "        return \"No ROI effectiveness data could be extracted.\"\n",
    "    \n",
    "    # Analyze the data\n",
    "    analysis = analyze_roi_effectiveness(chart_data, chart_description_content)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def analyze_roi_effectiveness(chart_data, chart_description):\n",
    "    \"\"\"Analyze ROI vs Effectiveness data\"\"\"\n",
    "    \n",
    "    # Sort channels by different metrics\n",
    "    by_roi = sorted(chart_data, key=lambda x: x['roi'], reverse=True)\n",
    "    by_effectiveness = sorted(chart_data, key=lambda x: x['effectiveness'], reverse=True)\n",
    "    by_spend = sorted(chart_data, key=lambda x: x['spend'], reverse=True)\n",
    "    \n",
    "    # Calculate totals and averages\n",
    "    total_spend = sum(ch['spend'] for ch in chart_data)\n",
    "    avg_roi = sum(ch['roi'] for ch in chart_data) / len(chart_data)\n",
    "    avg_effectiveness = sum(ch['effectiveness'] for ch in chart_data) / len(chart_data)\n",
    "    \n",
    "    # Categorize channels based on ROI and effectiveness\n",
    "    high_roi_threshold = avg_roi\n",
    "    high_effectiveness_threshold = avg_effectiveness\n",
    "    \n",
    "    channel_categories = {}\n",
    "    for channel in chart_data:\n",
    "        roi_level = \"High\" if channel['roi'] >= high_roi_threshold else \"Low\"\n",
    "        eff_level = \"High\" if channel['effectiveness'] >= high_effectiveness_threshold else \"Low\"\n",
    "        \n",
    "        if roi_level == \"High\" and eff_level == \"High\":\n",
    "            category = \"Star Performers\"\n",
    "        elif roi_level == \"High\" and eff_level == \"Low\":\n",
    "            category = \"Cost Efficient\"\n",
    "        elif roi_level == \"Low\" and eff_level == \"High\":\n",
    "            category = \"High Potential\"\n",
    "        else:\n",
    "            category = \"Optimization Needed\"\n",
    "        \n",
    "        channel_categories[channel['channel']] = {\n",
    "            'category': category,\n",
    "            'roi': channel['roi'],\n",
    "            'effectiveness': channel['effectiveness'],\n",
    "            'spend': channel['spend'],\n",
    "            'spend_share': (channel['spend'] / total_spend) * 100\n",
    "        }\n",
    "    \n",
    "    # Build analysis\n",
    "    analysis = f\"\"\"\n",
    "ROI vs Effectiveness Analysis:\n",
    "\n",
    "Performance Overview:\n",
    "- Average ROI across channels: {avg_roi:.1f}x\n",
    "- Average effectiveness: {avg_effectiveness:.4f} incremental outcome per impression\n",
    "- Total media spend analyzed: ${total_spend:,.0f}\n",
    "\n",
    "ROI Rankings:\n",
    "\"\"\".strip()\n",
    "    \n",
    "    for i, channel in enumerate(by_roi, 1):\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: {channel['roi']:.1f}x ROI\"\n",
    "    \n",
    "    analysis += f\"\\n\\nEffectiveness Rankings:\"\n",
    "    for i, channel in enumerate(by_effectiveness, 1):\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: {channel['effectiveness']:.4f} outcome per impression\"\n",
    "    \n",
    "    analysis += f\"\\n\\nSpend Allocation:\"\n",
    "    for i, channel in enumerate(by_spend, 1):\n",
    "        spend_pct = (channel['spend'] / total_spend) * 100\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: ${channel['spend']:,.0f} ({spend_pct:.1f}% of total spend)\"\n",
    "    \n",
    "    # Channel categorization\n",
    "    analysis += f\"\\n\\nChannel Performance Categories:\"\n",
    "    \n",
    "    categories = {}\n",
    "    for channel, data in channel_categories.items():\n",
    "        category = data['category']\n",
    "        if category not in categories:\n",
    "            categories[category] = []\n",
    "        categories[category].append(f\"{channel.upper()} ({data['roi']:.1f}x ROI, {data['effectiveness']:.4f} effectiveness)\")\n",
    "    \n",
    "    for category, channels in categories.items():\n",
    "        analysis += f\"\\n\\n{category}:\"\n",
    "        for channel_info in channels:\n",
    "            analysis += f\"\\n- {channel_info}\"\n",
    "    \n",
    "    # Strategic insights\n",
    "    analysis += f\"\\n\\nStrategic Insights:\"\n",
    "    \n",
    "    # Find best performers\n",
    "    highest_roi = by_roi[0]\n",
    "    highest_effectiveness = by_effectiveness[0]\n",
    "    largest_spend = by_spend[0]\n",
    "    \n",
    "    analysis += f\"\\n- {highest_roi['channel'].upper()} delivers highest ROI ({highest_roi['roi']:.1f}x) - prioritize for budget allocation\"\n",
    "    analysis += f\"\\n- {highest_effectiveness['channel'].upper()} shows highest effectiveness ({highest_effectiveness['effectiveness']:.4f}) - strong media performance per impression\"\n",
    "    analysis += f\"\\n- {largest_spend['channel'].upper()} receives largest budget (${largest_spend['spend']:,.0f}) - monitor efficiency closely\"\n",
    "    \n",
    "    # Efficiency vs spend analysis\n",
    "    for channel, data in channel_categories.items():\n",
    "        if data['category'] == \"Star Performers\":\n",
    "            analysis += f\"\\n- {channel.upper()}: Ideal performance - high ROI and effectiveness, maintain investment\"\n",
    "        elif data['category'] == \"Cost Efficient\":\n",
    "            analysis += f\"\\n- {channel.upper()}: Cost efficient but low reach - consider scaling if effectiveness can be maintained\"\n",
    "        elif data['category'] == \"High Potential\":\n",
    "            analysis += f\"\\n- {channel.upper()}: High effectiveness but expensive - optimize costs to improve ROI\"\n",
    "        elif data['category'] == \"Optimization Needed\":\n",
    "            analysis += f\"\\n- {channel.upper()}: Both ROI and effectiveness below average - requires optimization or budget reallocation\"\n",
    "    \n",
    "    if chart_description:\n",
    "        analysis += f\"\\n\\nMethodology: {chart_description}\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "print(get_roi_effectiveness_insights(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI vs Marginal ROI Performance Analysis:\n",
      "\n",
      "Performance Metrics:\n",
      "- Average ROI across channels: 4.8x\n",
      "- Average Marginal ROI: 2.0x (additional return per additional dollar)\n",
      "- Total media spend analyzed: $2,340,830\n",
      "\n",
      "ROI Rankings:\n",
      "1. META: 5.1x ROI\n",
      "2. TIKTOK: 5.0x ROI\n",
      "3. GOOGLE: 4.6x ROI\n",
      "4. DV_360_X1: 4.6x ROI\n",
      "\n",
      "Marginal ROI Rankings (Incremental Efficiency):\n",
      "1. META: 2.1x marginal ROI\n",
      "2. TIKTOK: 2.0x marginal ROI\n",
      "3. GOOGLE: 1.9x marginal ROI\n",
      "4. DV_360_X1: 1.8x marginal ROI\n",
      "\n",
      "Spend Distribution:\n",
      "1. DV_360_X1: $1,558,208 (66.6% of total spend)\n",
      "2. GOOGLE: $526,693 (22.5% of total spend)\n",
      "3. META: $243,623 (10.4% of total spend)\n",
      "4. TIKTOK: $12,306 (0.5% of total spend)\n",
      "\n",
      "Saturation Indicators:\n",
      "- DV_360_X1: High saturation signal (ROI/Marginal ROI ratio: 2.6x)\n",
      "- GOOGLE: Moderate saturation signal (ROI/Marginal ROI ratio: 2.4x)\n",
      "- META: Moderate saturation signal (ROI/Marginal ROI ratio: 2.5x)\n",
      "- TIKTOK: Moderate saturation signal (ROI/Marginal ROI ratio: 2.5x)\n",
      "\n",
      "Efficiency Gap Analysis:\n",
      "- DV_360_X1: 2.9x gap between current and marginal returns\n",
      "- GOOGLE: 2.7x gap between current and marginal returns\n",
      "- META: 3.0x gap between current and marginal returns\n",
      "- TIKTOK: 3.0x gap between current and marginal returns\n",
      "\n",
      "Performance Consistency:\n",
      "- ROI variation across channels: 0.21 standard deviation\n",
      "- Marginal ROI variation: 0.12 standard deviation\n",
      "- ROI and marginal ROI show similar variation patterns\n",
      "\n",
      "Channel Performance Patterns:\n",
      "- META leads in both ROI and marginal ROI - consistent high performer\n",
      "- DV_360_X1: High spend concentration (66.6%) with below-average marginal efficiency\n",
      "- META: Low spend allocation (10.4%) but above-average marginal efficiency\n",
      "- TIKTOK: Low spend allocation (0.5%) but above-average marginal efficiency\n",
      "\n",
      "Diminishing Returns Assessment:\n",
      "- DV_360_X1: Moderate diminishing returns (current ROI 2.6x higher than marginal)\n",
      "- GOOGLE: Moderate diminishing returns (current ROI 2.4x higher than marginal)\n",
      "- META: Moderate diminishing returns (current ROI 2.5x higher than marginal)\n",
      "- TIKTOK: Moderate diminishing returns (current ROI 2.5x higher than marginal)\n",
      "\n",
      "Methodology: Note: Marginal ROI measures the additional\n",
      "return generated for every additional dollar spent. It's an indicator of\n",
      "efficiency of additional spend. Channels with a high ROI but a low marginal ROI\n",
      "are likely in the saturation phase, where the initial investments have paid off,\n",
      "but additional investment does not bring in as much return. Conversely, channels\n",
      "that have a high ROI and a high marginal ROI perform well and continue to yield\n",
      "high returns with additional spending. The size of the bubbles represents the\n",
      "scale of the media spend.\n"
     ]
    }
   ],
   "source": [
    "def get_roi_marginal_insights(soup):\n",
    "    \"\"\"Extract ROI vs Marginal ROI chart insights - focused on performance analysis\"\"\"\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    # Find the ROI marginal chart\n",
    "    chart_embed = soup.find(\"chart-embed\", {\"id\": \"roi-marginal-chart\"})\n",
    "    if not chart_embed:\n",
    "        return \"No ROI marginal chart found.\"\n",
    "    \n",
    "    # Find the parent chart element\n",
    "    chart_element = chart_embed.find_parent(\"chart\")\n",
    "    if not chart_element:\n",
    "        return \"No ROI marginal chart element found.\"\n",
    "    \n",
    "    # Extract chart description\n",
    "    chart_description = chart_element.find(\"chart-description\")\n",
    "    chart_description_content = chart_description.get_text(strip=True) if chart_description else None\n",
    "    \n",
    "    # Extract chart data from script\n",
    "    script_tag = chart_element.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "    chart_data = []\n",
    "    \n",
    "    if script_tag:\n",
    "        script_content = script_tag.get_text()\n",
    "        json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                escaped_json = json_match.group(1)\n",
    "                unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "                chart_spec = json.loads(unescaped_json)\n",
    "                \n",
    "                datasets = chart_spec.get('datasets', {})\n",
    "                for dataset_key, dataset_value in datasets.items():\n",
    "                    if isinstance(dataset_value, list):\n",
    "                        chart_data = dataset_value\n",
    "                        break\n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                pass\n",
    "    \n",
    "    if not chart_data:\n",
    "        return \"No ROI marginal data could be extracted.\"\n",
    "    \n",
    "    # Analyze the data\n",
    "    analysis = analyze_roi_marginal_performance(chart_data, chart_description_content)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def analyze_roi_marginal_performance(chart_data, chart_description):\n",
    "    \"\"\"Analyze ROI vs Marginal ROI performance insights only\"\"\"\n",
    "    \n",
    "    # Sort channels by different metrics\n",
    "    by_roi = sorted(chart_data, key=lambda x: x['roi'], reverse=True)\n",
    "    by_marginal_roi = sorted(chart_data, key=lambda x: x['mroi'], reverse=True)\n",
    "    by_spend = sorted(chart_data, key=lambda x: x['spend'], reverse=True)\n",
    "    \n",
    "    # Calculate totals and averages\n",
    "    total_spend = sum(ch['spend'] for ch in chart_data)\n",
    "    avg_roi = sum(ch['roi'] for ch in chart_data) / len(chart_data)\n",
    "    avg_marginal_roi = sum(ch['mroi'] for ch in chart_data) / len(chart_data)\n",
    "    \n",
    "    # Build analysis\n",
    "    analysis = f\"\"\"\n",
    "ROI vs Marginal ROI Performance Analysis:\n",
    "\n",
    "Performance Metrics:\n",
    "- Average ROI across channels: {avg_roi:.1f}x\n",
    "- Average Marginal ROI: {avg_marginal_roi:.1f}x (additional return per additional dollar)\n",
    "- Total media spend analyzed: ${total_spend:,.0f}\n",
    "\n",
    "ROI Rankings:\n",
    "\"\"\".strip()\n",
    "    \n",
    "    for i, channel in enumerate(by_roi, 1):\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: {channel['roi']:.1f}x ROI\"\n",
    "    \n",
    "    analysis += f\"\\n\\nMarginal ROI Rankings (Incremental Efficiency):\"\n",
    "    for i, channel in enumerate(by_marginal_roi, 1):\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: {channel['mroi']:.1f}x marginal ROI\"\n",
    "    \n",
    "    analysis += f\"\\n\\nSpend Distribution:\"\n",
    "    for i, channel in enumerate(by_spend, 1):\n",
    "        spend_pct = (channel['spend'] / total_spend) * 100\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: ${channel['spend']:,.0f} ({spend_pct:.1f}% of total spend)\"\n",
    "    \n",
    "    # Saturation analysis (key insight for MMM)\n",
    "    analysis += f\"\\n\\nSaturation Indicators:\"\n",
    "    for channel in chart_data:\n",
    "        roi_marginal_ratio = channel['roi'] / channel['mroi']\n",
    "        saturation_level = \"High\" if roi_marginal_ratio > 2.5 else \"Moderate\" if roi_marginal_ratio > 2.0 else \"Low\"\n",
    "        analysis += f\"\\n- {channel['channel'].upper()}: {saturation_level} saturation signal (ROI/Marginal ROI ratio: {roi_marginal_ratio:.1f}x)\"\n",
    "    \n",
    "    # Efficiency gap analysis\n",
    "    analysis += f\"\\n\\nEfficiency Gap Analysis:\"\n",
    "    for channel in chart_data:\n",
    "        efficiency_gap = channel['roi'] - channel['mroi']\n",
    "        analysis += f\"\\n- {channel['channel'].upper()}: {efficiency_gap:.1f}x gap between current and marginal returns\"\n",
    "    \n",
    "    # Performance consistency analysis\n",
    "    analysis += f\"\\n\\nPerformance Consistency:\"\n",
    "    roi_std = (sum((ch['roi'] - avg_roi) ** 2 for ch in chart_data) / len(chart_data)) ** 0.5\n",
    "    marginal_roi_std = (sum((ch['mroi'] - avg_marginal_roi) ** 2 for ch in chart_data) / len(chart_data)) ** 0.5\n",
    "    \n",
    "    analysis += f\"\\n- ROI variation across channels: {roi_std:.2f} standard deviation\"\n",
    "    analysis += f\"\\n- Marginal ROI variation: {marginal_roi_std:.2f} standard deviation\"\n",
    "    \n",
    "    if marginal_roi_std > roi_std:\n",
    "        analysis += f\"\\n- Marginal ROI shows higher variation than base ROI, indicating different scaling potentials\"\n",
    "    else:\n",
    "        analysis += f\"\\n- ROI and marginal ROI show similar variation patterns\"\n",
    "    \n",
    "    # Channel performance patterns\n",
    "    analysis += f\"\\n\\nChannel Performance Patterns:\"\n",
    "    \n",
    "    best_roi = by_roi[0]\n",
    "    best_marginal = by_marginal_roi[0]\n",
    "    largest_spend = by_spend[0]\n",
    "    \n",
    "    if best_roi['channel'] == best_marginal['channel']:\n",
    "        analysis += f\"\\n- {best_roi['channel'].upper()} leads in both ROI and marginal ROI - consistent high performer\"\n",
    "    else:\n",
    "        analysis += f\"\\n- {best_roi['channel'].upper()} has highest ROI while {best_marginal['channel'].upper()} has highest marginal ROI - different optimization opportunities\"\n",
    "    \n",
    "    # Spend efficiency vs performance\n",
    "    for channel in chart_data:\n",
    "        spend_share = (channel['spend'] / total_spend) * 100\n",
    "        if spend_share > 50 and channel['mroi'] < avg_marginal_roi:\n",
    "            analysis += f\"\\n- {channel['channel'].upper()}: High spend concentration ({spend_share:.1f}%) with below-average marginal efficiency\"\n",
    "        elif spend_share < 15 and channel['mroi'] > avg_marginal_roi:\n",
    "            analysis += f\"\\n- {channel['channel'].upper()}: Low spend allocation ({spend_share:.1f}%) but above-average marginal efficiency\"\n",
    "    \n",
    "    # Diminishing returns analysis\n",
    "    analysis += f\"\\n\\nDiminishing Returns Assessment:\"\n",
    "    for channel in chart_data:\n",
    "        returns_ratio = channel['roi'] / channel['mroi']\n",
    "        if returns_ratio > 3.0:\n",
    "            analysis += f\"\\n- {channel['channel'].upper()}: Strong diminishing returns pattern (current ROI {returns_ratio:.1f}x higher than marginal)\"\n",
    "        elif returns_ratio > 2.0:\n",
    "            analysis += f\"\\n- {channel['channel'].upper()}: Moderate diminishing returns (current ROI {returns_ratio:.1f}x higher than marginal)\"\n",
    "        else:\n",
    "            analysis += f\"\\n- {channel['channel'].upper()}: Minimal diminishing returns (current ROI {returns_ratio:.1f}x higher than marginal)\"\n",
    "    \n",
    "    if chart_description:\n",
    "        analysis += f\"\\n\\nMethodology: {chart_description}\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "print(get_roi_marginal_insights(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI and CPIK Performance Analysis with Confidence Intervals:\n",
      "\n",
      "ROI Performance with 90% Credible Intervals:\n",
      "1. META:\n",
      "   - Point Estimate: 5.11x ROI\n",
      "   - Confidence Range: 3.54x to 6.98x\n",
      "   - Uncertainty Level: ±67% around point estimate\n",
      "2. TIKTOK:\n",
      "   - Point Estimate: 5.00x ROI\n",
      "   - Confidence Range: 4.92x to 5.08x\n",
      "   - Uncertainty Level: ±3% around point estimate\n",
      "3. GOOGLE:\n",
      "   - Point Estimate: 4.64x ROI\n",
      "   - Confidence Range: 2.18x to 8.31x\n",
      "   - Uncertainty Level: ±132% around point estimate\n",
      "4. DV_360_X1:\n",
      "   - Point Estimate: 4.64x ROI\n",
      "   - Confidence Range: 1.40x to 8.48x\n",
      "   - Uncertainty Level: ±153% around point estimate\n",
      "\n",
      "ROI Confidence Analysis:\n",
      "- Most Reliable ROI Estimate: TIKTOK (±3% uncertainty)\n",
      "- Least Reliable ROI Estimate: DV_360_X1 (±153% uncertainty)\n",
      "\n",
      "ROI Statistical Significance:\n",
      "- DV_360_X1 and GOOGLE: Overlapping confidence intervals (no statistically significant difference)\n",
      "- DV_360_X1 and META: Overlapping confidence intervals (no statistically significant difference)\n",
      "- DV_360_X1 and TIKTOK: Overlapping confidence intervals (no statistically significant difference)\n",
      "- GOOGLE and META: Overlapping confidence intervals (no statistically significant difference)\n",
      "- GOOGLE and TIKTOK: Overlapping confidence intervals (no statistically significant difference)\n",
      "- META and TIKTOK: Overlapping confidence intervals (no statistically significant difference)\n",
      "\n",
      "CPIK Performance with 90% Credible Intervals:\n",
      "1. META (Best to Worst CPIK):\n",
      "   - Point Estimate: $0.200 per KPI unit\n",
      "   - Confidence Range: $0.143 to $0.282\n",
      "   - Uncertainty Level: ±70% around point estimate\n",
      "2. TIKTOK (Best to Worst CPIK):\n",
      "   - Point Estimate: $0.200 per KPI unit\n",
      "   - Confidence Range: $0.197 to $0.203\n",
      "   - Uncertainty Level: ±3% around point estimate\n",
      "3. DV_360_X1 (Best to Worst CPIK):\n",
      "   - Point Estimate: $0.224 per KPI unit\n",
      "   - Confidence Range: $0.118 to $0.716\n",
      "   - Uncertainty Level: ±268% around point estimate\n",
      "4. GOOGLE (Best to Worst CPIK):\n",
      "   - Point Estimate: $0.234 per KPI unit\n",
      "   - Confidence Range: $0.120 to $0.460\n",
      "   - Uncertainty Level: ±145% around point estimate\n",
      "\n",
      "CPIK Confidence Analysis:\n",
      "- Most Reliable CPIK Estimate: TIKTOK (±3% uncertainty)\n",
      "- Least Reliable CPIK Estimate: DV_360_X1 (±268% uncertainty)\n",
      "\n",
      "CPIK Statistical Significance:\n",
      "- DV_360_X1 and GOOGLE: Overlapping CPIK intervals (no statistically significant difference)\n",
      "- DV_360_X1 and META: Overlapping CPIK intervals (no statistically significant difference)\n",
      "- DV_360_X1 and TIKTOK: Overlapping CPIK intervals (no statistically significant difference)\n",
      "- GOOGLE and META: Overlapping CPIK intervals (no statistically significant difference)\n",
      "- GOOGLE and TIKTOK: Overlapping CPIK intervals (no statistically significant difference)\n",
      "- META and TIKTOK: Overlapping CPIK intervals (no statistically significant difference)\n",
      "\n",
      "Combined ROI and CPIK Insights:\n",
      "\n",
      "High Confidence Performers (Low uncertainty in both ROI and CPIK):\n",
      "- TIKTOK: 5.00x ROI (±3%), $0.200 CPIK (±3%)\n",
      "\n",
      "Uncertainty Patterns:\n",
      "- Average ROI uncertainty: ±89%\n",
      "- Average CPIK uncertainty: ±121%\n",
      "- CPIK estimates show higher uncertainty than ROI estimates\n",
      "\n",
      "Model Reliability Assessment:\n",
      "- High ROI Confidence: TIKTOK (narrow intervals)\n",
      "- Low ROI Confidence: DV_360_X1, GOOGLE (wide intervals)\n",
      "- High CPIK Confidence: TIKTOK (narrow intervals)\n",
      "- Low CPIK Confidence: DV_360_X1, GOOGLE (wide intervals)\n",
      "\n",
      "Methodology Note: Note: CPIK (cost per incremental KPI) point\n",
      "estimate is determined by the posterior median, whereas ROI point estimate is\n",
      "determined by the posterior mean.\n"
     ]
    }
   ],
   "source": [
    "def get_roi_cpik_confidence_insights(soup):\n",
    "    \"\"\"Extract ROI and CPIK insights with confidence intervals\"\"\"\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    # Find both charts\n",
    "    roi_chart = soup.find(\"chart-embed\", {\"id\": \"roi-channel-chart\"})\n",
    "    cpik_chart = soup.find(\"chart-embed\", {\"id\": \"cpik-channel-chart\"})\n",
    "    \n",
    "    if not roi_chart and not cpik_chart:\n",
    "        return \"No ROI or CPIK charts found.\"\n",
    "    \n",
    "    roi_data = []\n",
    "    cpik_data = []\n",
    "    cpik_description = None\n",
    "    \n",
    "    # Extract ROI data\n",
    "    if roi_chart:\n",
    "        roi_script = roi_chart.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "        if roi_script:\n",
    "            roi_data = extract_chart_data(roi_script.get_text())\n",
    "    \n",
    "    # Extract CPIK data and description\n",
    "    if cpik_chart:\n",
    "        cpik_element = cpik_chart.find_parent(\"chart\")\n",
    "        if cpik_element:\n",
    "            cpik_desc = cpik_element.find(\"chart-description\")\n",
    "            cpik_description = cpik_desc.get_text(strip=True) if cpik_desc else None\n",
    "        \n",
    "        cpik_script = cpik_chart.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "        if cpik_script:\n",
    "            cpik_data = extract_chart_data(cpik_script.get_text())\n",
    "    \n",
    "    # Analyze the data\n",
    "    analysis = analyze_roi_cpik_confidence(roi_data, cpik_data, cpik_description)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def extract_chart_data(script_content):\n",
    "    \"\"\"Extract data from chart script\"\"\"\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "    if json_match:\n",
    "        try:\n",
    "            escaped_json = json_match.group(1)\n",
    "            unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "            chart_spec = json.loads(unescaped_json)\n",
    "            \n",
    "            datasets = chart_spec.get('datasets', {})\n",
    "            for dataset_key, dataset_value in datasets.items():\n",
    "                if isinstance(dataset_value, list):\n",
    "                    return dataset_value\n",
    "        except (json.JSONDecodeError, AttributeError):\n",
    "            pass\n",
    "    return []\n",
    "\n",
    "def analyze_roi_cpik_confidence(roi_data, cpik_data, cpik_description):\n",
    "    \"\"\"Analyze ROI and CPIK data with confidence intervals\"\"\"\n",
    "    \n",
    "    analysis = \"ROI and CPIK Performance Analysis with Confidence Intervals:\\n\"\n",
    "    \n",
    "    # ROI Analysis with Confidence Intervals\n",
    "    if roi_data:\n",
    "        analysis += f\"\\nROI Performance with 90% Credible Intervals:\"\n",
    "        \n",
    "        roi_sorted = sorted(roi_data, key=lambda x: x['roi'], reverse=True)\n",
    "        \n",
    "        for i, channel in enumerate(roi_sorted, 1):\n",
    "            roi_range = channel['ci_hi'] - channel['ci_lo']\n",
    "            uncertainty = (roi_range / channel['roi']) * 100\n",
    "            \n",
    "            analysis += f\"\\n{i}. {channel['channel'].upper()}:\"\n",
    "            analysis += f\"\\n   - Point Estimate: {channel['roi']:.2f}x ROI\"\n",
    "            analysis += f\"\\n   - Confidence Range: {channel['ci_lo']:.2f}x to {channel['ci_hi']:.2f}x\"\n",
    "            analysis += f\"\\n   - Uncertainty Level: ±{uncertainty:.0f}% around point estimate\"\n",
    "        \n",
    "        # ROI Confidence Analysis\n",
    "        analysis += f\"\\n\\nROI Confidence Analysis:\"\n",
    "        \n",
    "        # Find most/least certain estimates\n",
    "        roi_uncertainties = []\n",
    "        for channel in roi_data:\n",
    "            uncertainty = ((channel['ci_hi'] - channel['ci_lo']) / channel['roi']) * 100\n",
    "            roi_uncertainties.append((channel['channel'], uncertainty, channel['ci_hi'] - channel['ci_lo']))\n",
    "        \n",
    "        most_certain = min(roi_uncertainties, key=lambda x: x[1])\n",
    "        least_certain = max(roi_uncertainties, key=lambda x: x[1])\n",
    "        \n",
    "        analysis += f\"\\n- Most Reliable ROI Estimate: {most_certain[0].upper()} (±{most_certain[1]:.0f}% uncertainty)\"\n",
    "        analysis += f\"\\n- Least Reliable ROI Estimate: {least_certain[0].upper()} (±{least_certain[1]:.0f}% uncertainty)\"\n",
    "        \n",
    "        # Overlapping confidence intervals analysis\n",
    "        analysis += f\"\\n\\nROI Statistical Significance:\"\n",
    "        for i, ch1 in enumerate(roi_data):\n",
    "            for ch2 in roi_data[i+1:]:\n",
    "                # Check if confidence intervals overlap\n",
    "                if ch1['ci_lo'] <= ch2['ci_hi'] and ch2['ci_lo'] <= ch1['ci_hi']:\n",
    "                    analysis += f\"\\n- {ch1['channel'].upper()} and {ch2['channel'].upper()}: Overlapping confidence intervals (no statistically significant difference)\"\n",
    "                else:\n",
    "                    higher = ch1 if ch1['roi'] > ch2['roi'] else ch2\n",
    "                    lower = ch2 if ch1['roi'] > ch2['roi'] else ch1\n",
    "                    analysis += f\"\\n- {higher['channel'].upper()} significantly outperforms {lower['channel'].upper()} (non-overlapping intervals)\"\n",
    "    \n",
    "    # CPIK Analysis with Confidence Intervals\n",
    "    if cpik_data:\n",
    "        analysis += f\"\\n\\nCPIK Performance with 90% Credible Intervals:\"\n",
    "        \n",
    "        cpik_sorted = sorted(cpik_data, key=lambda x: x['cpik'])  # Lower CPIK is better\n",
    "        \n",
    "        for i, channel in enumerate(cpik_sorted, 1):\n",
    "            cpik_range = channel['ci_hi'] - channel['ci_lo']\n",
    "            uncertainty = (cpik_range / channel['cpik']) * 100\n",
    "            \n",
    "            analysis += f\"\\n{i}. {channel['channel'].upper()} (Best to Worst CPIK):\"\n",
    "            analysis += f\"\\n   - Point Estimate: ${channel['cpik']:.3f} per KPI unit\"\n",
    "            analysis += f\"\\n   - Confidence Range: ${channel['ci_lo']:.3f} to ${channel['ci_hi']:.3f}\"\n",
    "            analysis += f\"\\n   - Uncertainty Level: ±{uncertainty:.0f}% around point estimate\"\n",
    "        \n",
    "        # CPIK Confidence Analysis\n",
    "        analysis += f\"\\n\\nCPIK Confidence Analysis:\"\n",
    "        \n",
    "        cpik_uncertainties = []\n",
    "        for channel in cpik_data:\n",
    "            uncertainty = ((channel['ci_hi'] - channel['ci_lo']) / channel['cpik']) * 100\n",
    "            cpik_uncertainties.append((channel['channel'], uncertainty))\n",
    "        \n",
    "        most_certain_cpik = min(cpik_uncertainties, key=lambda x: x[1])\n",
    "        least_certain_cpik = max(cpik_uncertainties, key=lambda x: x[1])\n",
    "        \n",
    "        analysis += f\"\\n- Most Reliable CPIK Estimate: {most_certain_cpik[0].upper()} (±{most_certain_cpik[1]:.0f}% uncertainty)\"\n",
    "        analysis += f\"\\n- Least Reliable CPIK Estimate: {least_certain_cpik[0].upper()} (±{least_certain_cpik[1]:.0f}% uncertainty)\"\n",
    "        \n",
    "        # CPIK Statistical Significance\n",
    "        analysis += f\"\\n\\nCPIK Statistical Significance:\"\n",
    "        for i, ch1 in enumerate(cpik_data):\n",
    "            for ch2 in cpik_data[i+1:]:\n",
    "                if ch1['ci_lo'] <= ch2['ci_hi'] and ch2['ci_lo'] <= ch1['ci_hi']:\n",
    "                    analysis += f\"\\n- {ch1['channel'].upper()} and {ch2['channel'].upper()}: Overlapping CPIK intervals (no statistically significant difference)\"\n",
    "                else:\n",
    "                    better = ch1 if ch1['cpik'] < ch2['cpik'] else ch2  # Lower CPIK is better\n",
    "                    worse = ch2 if ch1['cpik'] < ch2['cpik'] else ch1\n",
    "                    analysis += f\"\\n- {better['channel'].upper()} significantly more cost-efficient than {worse['channel'].upper()}\"\n",
    "    \n",
    "    # Combined ROI and CPIK Analysis\n",
    "    if roi_data and cpik_data:\n",
    "        analysis += f\"\\n\\nCombined ROI and CPIK Insights:\"\n",
    "        \n",
    "        # Match channels between datasets\n",
    "        combined_data = []\n",
    "        for roi_ch in roi_data:\n",
    "            cpik_ch = next((c for c in cpik_data if c['channel'] == roi_ch['channel']), None)\n",
    "            if cpik_ch:\n",
    "                combined_data.append({\n",
    "                    'channel': roi_ch['channel'],\n",
    "                    'roi': roi_ch['roi'],\n",
    "                    'roi_uncertainty': ((roi_ch['ci_hi'] - roi_ch['ci_lo']) / roi_ch['roi']) * 100,\n",
    "                    'cpik': cpik_ch['cpik'],\n",
    "                    'cpik_uncertainty': ((cpik_ch['ci_hi'] - cpik_ch['ci_lo']) / cpik_ch['cpik']) * 100\n",
    "                })\n",
    "        \n",
    "        # Find channels with high confidence in both metrics\n",
    "        high_confidence_channels = [\n",
    "            ch for ch in combined_data \n",
    "            if ch['roi_uncertainty'] < 50 and ch['cpik_uncertainty'] < 50  # Less than 50% uncertainty\n",
    "        ]\n",
    "        \n",
    "        if high_confidence_channels:\n",
    "            analysis += f\"\\n\\nHigh Confidence Performers (Low uncertainty in both ROI and CPIK):\"\n",
    "            for ch in sorted(high_confidence_channels, key=lambda x: x['roi'], reverse=True):\n",
    "                analysis += f\"\\n- {ch['channel'].upper()}: {ch['roi']:.2f}x ROI (±{ch['roi_uncertainty']:.0f}%), ${ch['cpik']:.3f} CPIK (±{ch['cpik_uncertainty']:.0f}%)\"\n",
    "        \n",
    "        # Uncertainty patterns\n",
    "        analysis += f\"\\n\\nUncertainty Patterns:\"\n",
    "        avg_roi_uncertainty = sum(ch['roi_uncertainty'] for ch in combined_data) / len(combined_data)\n",
    "        avg_cpik_uncertainty = sum(ch['cpik_uncertainty'] for ch in combined_data) / len(combined_data)\n",
    "        \n",
    "        analysis += f\"\\n- Average ROI uncertainty: ±{avg_roi_uncertainty:.0f}%\"\n",
    "        analysis += f\"\\n- Average CPIK uncertainty: ±{avg_cpik_uncertainty:.0f}%\"\n",
    "        \n",
    "        if avg_roi_uncertainty > avg_cpik_uncertainty:\n",
    "            analysis += f\"\\n- ROI estimates show higher uncertainty than CPIK estimates\"\n",
    "        else:\n",
    "            analysis += f\"\\n- CPIK estimates show higher uncertainty than ROI estimates\"\n",
    "    \n",
    "    # Model Reliability Assessment\n",
    "    analysis += f\"\\n\\nModel Reliability Assessment:\"\n",
    "    \n",
    "    if roi_data:\n",
    "        narrow_roi_intervals = [ch for ch in roi_data if ((ch['ci_hi'] - ch['ci_lo']) / ch['roi']) < 0.3]\n",
    "        wide_roi_intervals = [ch for ch in roi_data if ((ch['ci_hi'] - ch['ci_lo']) / ch['roi']) > 0.7]\n",
    "        \n",
    "        if narrow_roi_intervals:\n",
    "            analysis += f\"\\n- High ROI Confidence: {', '.join([ch['channel'].upper() for ch in narrow_roi_intervals])} (narrow intervals)\"\n",
    "        if wide_roi_intervals:\n",
    "            analysis += f\"\\n- Low ROI Confidence: {', '.join([ch['channel'].upper() for ch in wide_roi_intervals])} (wide intervals)\"\n",
    "    \n",
    "    if cpik_data:\n",
    "        narrow_cpik_intervals = [ch for ch in cpik_data if ((ch['ci_hi'] - ch['ci_lo']) / ch['cpik']) < 0.3]\n",
    "        wide_cpik_intervals = [ch for ch in cpik_data if ((ch['ci_hi'] - ch['ci_lo']) / ch['cpik']) > 0.7]\n",
    "        \n",
    "        if narrow_cpik_intervals:\n",
    "            analysis += f\"\\n- High CPIK Confidence: {', '.join([ch['channel'].upper() for ch in narrow_cpik_intervals])} (narrow intervals)\"\n",
    "        if wide_cpik_intervals:\n",
    "            analysis += f\"\\n- Low CPIK Confidence: {', '.join([ch['channel'].upper() for ch in wide_cpik_intervals])} (wide intervals)\"\n",
    "    \n",
    "    if cpik_description:\n",
    "        analysis += f\"\\n\\nMethodology Note: {cpik_description}\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "print(get_roi_cpik_confidence_insights(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marketing Response Curves Performance Analysis:\n",
      "\n",
      "Current Channel Performance:\n",
      "1. META:\n",
      "   - Current Spend: $243,623 (10.4% of total)\n",
      "   - Current Revenue: $1,245,430 (11.3% of total)\n",
      "   - Current ROI: 5.11x\n",
      "2. TIKTOK:\n",
      "   - Current Spend: $12,306 (0.5% of total)\n",
      "   - Current Revenue: $61,527 (0.6% of total)\n",
      "   - Current ROI: 5.00x\n",
      "3. GOOGLE:\n",
      "   - Current Spend: $526,693 (22.5% of total)\n",
      "   - Current Revenue: $2,444,298 (22.3% of total)\n",
      "   - Current ROI: 4.64x\n",
      "4. DV_360_X1:\n",
      "   - Current Spend: $1,558,208 (66.6% of total)\n",
      "   - Current Revenue: $7,227,618 (65.8% of total)\n",
      "   - Current ROI: 4.64x\n",
      "\n",
      "Portfolio Summary:\n",
      "- Total Spend: $2,340,830\n",
      "- Total Revenue: $10,978,873\n",
      "- Overall ROI: 4.69x\n",
      "- Active Channels: 4\n",
      "\n",
      "Spend Increase Scenario Analysis:\n",
      "\n",
      "25% Spend Increase Scenarios:\n",
      "- META: +$60,906 → +$113,594 (Marginal ROI: 1.87x)\n",
      "- TIKTOK: +$3,077 → +$5,627 (Marginal ROI: 1.83x)\n",
      "- GOOGLE: +$131,673 → +$228,438 (Marginal ROI: 1.73x)\n",
      "- DV_360_X1: +$389,552 → +$614,472 (Marginal ROI: 1.58x)\n",
      "Portfolio 25% Increase: +$585,208 → +$962,132 (Portfolio Marginal ROI: 1.64x)\n",
      "\n",
      "50% Spend Increase Scenarios:\n",
      "- META: +$121,811 → +$206,189 (Marginal ROI: 1.69x)\n",
      "- TIKTOK: +$6,153 → +$10,243 (Marginal ROI: 1.66x)\n",
      "- GOOGLE: +$263,346 → +$414,808 (Marginal ROI: 1.58x)\n",
      "- DV_360_X1: +$779,104 → +$1,105,102 (Marginal ROI: 1.42x)\n",
      "Portfolio 50% Increase: +$1,170,415 → +$1,736,342 (Portfolio Marginal ROI: 1.48x)\n",
      "\n",
      "100% Spend Increase Scenarios:\n",
      "- META: +$243,623 → +$350,315 (Marginal ROI: 1.44x)\n",
      "- TIKTOK: +$12,306 → +$17,486 (Marginal ROI: 1.42x)\n",
      "- GOOGLE: +$526,693 → +$704,568 (Marginal ROI: 1.34x)\n",
      "- DV_360_X1: +$1,558,208 → +$1,847,447 (Marginal ROI: 1.19x)\n",
      "Portfolio 100% Increase: +$2,340,830 → +$2,919,815 (Portfolio Marginal ROI: 1.25x)\n",
      "\n",
      "Marginal Returns Analysis:\n",
      "\n",
      "Efficiency Threshold Analysis:\n",
      "- META: Efficiency maintained until 1.2x current spend\n",
      "- TIKTOK: Efficiency maintained until 1.2x current spend\n",
      "- GOOGLE: Efficiency maintained until 1.2x current spend\n",
      "- DV_360_X1: Efficiency maintained until 1.2x current spend\n",
      "\n",
      "Scale and Growth Opportunity Analysis:\n",
      "- Most Efficient Channel: META (5.11x ROI)\n",
      "- Largest Spend Channel: DV_360_X1 ($1,558,208)\n",
      "- Highest Revenue Channel: DV_360_X1 ($7,227,618)\n",
      "\n",
      "Spend Scale Differences:\n",
      "- DV_360_X1 spends 126.6x more than TIKTOK\n",
      "- DV_360_X1 spends 6.4x more than META\n",
      "- DV_360_X1 spends 3.0x more than GOOGLE\n",
      "\n",
      "Portfolio Concentration:\n",
      "- Spend Concentration: Top channel (DV_360_X1) represents 66.6% of total spend\n",
      "- Revenue Concentration: Top channel (DV_360_X1) represents 65.8% of total revenue\n",
      "- Channels with >10% spend share: 3 of 4\n",
      "\n",
      "Growth Potential Summary:\n",
      "- META: Up to 32% revenue growth potential (max 2.2x current spend)\n",
      "- TIKTOK: Up to 32% revenue growth potential (max 2.2x current spend)\n",
      "- GOOGLE: Up to 32% revenue growth potential (max 2.2x current spend)\n",
      "- DV_360_X1: Up to 29% revenue growth potential (max 2.2x current spend)\n",
      "\n",
      "Methodology Note: Response curves show cumulative incremental revenue from total media spend over the selected time period, constructed based on historical flighting patterns.\n"
     ]
    }
   ],
   "source": [
    "def extract_response_curves_data_for_rag(soup):\n",
    "    \"\"\"Extract response curves data and format for RAG input with structured analysis\"\"\"\n",
    "    import re\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Find the response curves chart\n",
    "    chart = soup.find(\"chart-embed\", {\"id\": \"response-curves-chart\"})\n",
    "    \n",
    "    if not chart:\n",
    "        return []\n",
    "    \n",
    "    # Extract script content\n",
    "    script = chart.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "    if not script:\n",
    "        return []\n",
    "    \n",
    "    # Extract JSON data from script\n",
    "    json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script.get_text(), re.DOTALL)\n",
    "    if not json_match:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Clean and parse JSON\n",
    "        escaped_json = json_match.group(1)\n",
    "        unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "        chart_spec = json.loads(unescaped_json)\n",
    "        \n",
    "        # Get datasets\n",
    "        datasets = chart_spec.get('datasets', {})\n",
    "        raw_data = []\n",
    "        for dataset_key, dataset_value in datasets.items():\n",
    "            if isinstance(dataset_value, list):\n",
    "                raw_data = dataset_value\n",
    "                break\n",
    "        \n",
    "        if not raw_data:\n",
    "            return []\n",
    "        \n",
    "        # Organize data by channel\n",
    "        channels = {}\n",
    "        for point in raw_data:\n",
    "            channel = point['channel']\n",
    "            if channel not in channels:\n",
    "                channels[channel] = {\n",
    "                    'name': channel,\n",
    "                    'data_points': [],\n",
    "                    'current_spend': None,\n",
    "                    'current_revenue': None\n",
    "                }\n",
    "            \n",
    "            channels[channel]['data_points'].append(point)\n",
    "            \n",
    "            if point.get('current_spend') == \"Current spend\":\n",
    "                channels[channel]['current_spend'] = point['spend']\n",
    "                channels[channel]['current_revenue'] = point['mean']\n",
    "        \n",
    "        # Calculate metrics for each channel\n",
    "        channel_metrics = {}\n",
    "        for channel_name, channel_data in channels.items():\n",
    "            if not channel_data['current_spend']:\n",
    "                continue\n",
    "                \n",
    "            sorted_points = sorted(channel_data['data_points'], key=lambda x: x['spend_multiplier'])\n",
    "            current_roi = channel_data['current_revenue'] / channel_data['current_spend']\n",
    "            \n",
    "            # Calculate spend increase scenarios\n",
    "            scenarios = {}\n",
    "            target_multipliers = [1.25, 1.5, 2.0]\n",
    "            \n",
    "            for multiplier in target_multipliers:\n",
    "                closest_point = min(sorted_points, key=lambda x: abs(x['spend_multiplier'] - multiplier))\n",
    "                if closest_point['spend_multiplier'] >= multiplier:\n",
    "                    additional_spend = closest_point['spend'] - channel_data['current_spend']\n",
    "                    additional_revenue = closest_point['mean'] - channel_data['current_revenue']\n",
    "                    marginal_roi = additional_revenue / additional_spend if additional_spend > 0 else 0\n",
    "                    \n",
    "                    scenarios[f\"{int((multiplier-1)*100)}%\"] = {\n",
    "                        'additional_spend': additional_spend,\n",
    "                        'additional_revenue': additional_revenue,\n",
    "                        'marginal_roi': marginal_roi,\n",
    "                        'new_total_spend': closest_point['spend'],\n",
    "                        'new_total_revenue': closest_point['mean']\n",
    "                    }\n",
    "            \n",
    "            # Calculate marginal returns at different levels\n",
    "            marginal_returns = []\n",
    "            for i in range(1, min(6, len(sorted_points))):\n",
    "                if sorted_points[i]['spend_multiplier'] > 1.0:\n",
    "                    prev_point = sorted_points[i-1]\n",
    "                    curr_point = sorted_points[i]\n",
    "                    spend_diff = curr_point['spend'] - prev_point['spend']\n",
    "                    revenue_diff = curr_point['mean'] - prev_point['mean']\n",
    "                    if spend_diff > 0:\n",
    "                        marginal_roi = revenue_diff / spend_diff\n",
    "                        marginal_returns.append({\n",
    "                            'spend_multiplier': curr_point['spend_multiplier'],\n",
    "                            'marginal_roi': marginal_roi\n",
    "                        })\n",
    "            \n",
    "            # Find efficiency threshold\n",
    "            efficiency_threshold = None\n",
    "            for point in sorted_points:\n",
    "                if point['spend_multiplier'] > 1.0:\n",
    "                    point_roi = point['mean'] / point['spend'] if point['spend'] > 0 else 0\n",
    "                    if point_roi < current_roi * 0.9:  # 10% drop threshold\n",
    "                        efficiency_threshold = point['spend_multiplier']\n",
    "                        break\n",
    "            \n",
    "            channel_metrics[channel_name] = {\n",
    "                'current_spend': channel_data['current_spend'],\n",
    "                'current_revenue': channel_data['current_revenue'],\n",
    "                'current_roi': current_roi,\n",
    "                'scenarios': scenarios,\n",
    "                'marginal_returns': marginal_returns,\n",
    "                'efficiency_threshold': efficiency_threshold,\n",
    "                'max_modeled_spend': max(point['spend'] for point in sorted_points),\n",
    "                'max_modeled_revenue': max(point['mean'] for point in sorted_points)\n",
    "            }\n",
    "        \n",
    "        # Create structured analysis document\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Calculate totals\n",
    "        total_spend = sum(m['current_spend'] for m in channel_metrics.values())\n",
    "        total_revenue = sum(m['current_revenue'] for m in channel_metrics.values())\n",
    "        overall_roi = total_revenue / total_spend if total_spend > 0 else 0\n",
    "        \n",
    "        # Build analysis content\n",
    "        content = \"Marketing Response Curves Performance Analysis:\\n\\n\"\n",
    "        \n",
    "        # Current Performance Summary\n",
    "        content += \"Current Channel Performance:\\n\"\n",
    "        sorted_channels = sorted(channel_metrics.items(), key=lambda x: x[1]['current_roi'], reverse=True)\n",
    "        \n",
    "        for i, (channel, metrics) in enumerate(sorted_channels, 1):\n",
    "            spend_pct = (metrics['current_spend'] / total_spend) * 100\n",
    "            revenue_pct = (metrics['current_revenue'] / total_revenue) * 100\n",
    "            content += f\"{i}. {channel.upper()}:\\n\"\n",
    "            content += f\"   - Current Spend: ${metrics['current_spend']:,.0f} ({spend_pct:.1f}% of total)\\n\"\n",
    "            content += f\"   - Current Revenue: ${metrics['current_revenue']:,.0f} ({revenue_pct:.1f}% of total)\\n\"\n",
    "            content += f\"   - Current ROI: {metrics['current_roi']:.2f}x\\n\"\n",
    "        \n",
    "        content += f\"\\nPortfolio Summary:\\n\"\n",
    "        content += f\"- Total Spend: ${total_spend:,.0f}\\n\"\n",
    "        content += f\"- Total Revenue: ${total_revenue:,.0f}\\n\"\n",
    "        content += f\"- Overall ROI: {overall_roi:.2f}x\\n\"\n",
    "        content += f\"- Active Channels: {len(channel_metrics)}\\n\\n\"\n",
    "        \n",
    "        # Spend Increase Scenarios\n",
    "        content += \"Spend Increase Scenario Analysis:\\n\\n\"\n",
    "        \n",
    "        for scenario_pct in [\"25%\", \"50%\", \"100%\"]:\n",
    "            content += f\"{scenario_pct} Spend Increase Scenarios:\\n\"\n",
    "            \n",
    "            total_additional_spend = 0\n",
    "            total_additional_revenue = 0\n",
    "            scenario_count = 0\n",
    "            \n",
    "            for channel, metrics in sorted_channels:\n",
    "                if scenario_pct in metrics['scenarios']:\n",
    "                    scenario = metrics['scenarios'][scenario_pct]\n",
    "                    content += f\"- {channel.upper()}: +${scenario['additional_spend']:,.0f} → +${scenario['additional_revenue']:,.0f} (Marginal ROI: {scenario['marginal_roi']:.2f}x)\\n\"\n",
    "                    total_additional_spend += scenario['additional_spend']\n",
    "                    total_additional_revenue += scenario['additional_revenue']\n",
    "                    scenario_count += 1\n",
    "            \n",
    "            if total_additional_spend > 0:\n",
    "                portfolio_marginal_roi = total_additional_revenue / total_additional_spend\n",
    "                content += f\"Portfolio {scenario_pct} Increase: +${total_additional_spend:,.0f} → +${total_additional_revenue:,.0f} (Portfolio Marginal ROI: {portfolio_marginal_roi:.2f}x)\\n\\n\"\n",
    "        \n",
    "        # Marginal Returns Analysis\n",
    "        content += \"Marginal Returns Analysis:\\n\"\n",
    "        for channel, metrics in sorted_channels:\n",
    "            if metrics['marginal_returns']:\n",
    "                content += f\"{channel.upper()} Marginal Returns:\\n\"\n",
    "                for i, mr in enumerate(metrics['marginal_returns'][:3]):\n",
    "                    content += f\"   - At {mr['spend_multiplier']:.1f}x current spend: {mr['marginal_roi']:.2f}x marginal ROI\\n\"\n",
    "        content += \"\\n\"\n",
    "        \n",
    "        # Efficiency Threshold Analysis\n",
    "        content += \"Efficiency Threshold Analysis:\\n\"\n",
    "        for channel, metrics in sorted_channels:\n",
    "            if metrics['efficiency_threshold']:\n",
    "                content += f\"- {channel.upper()}: Efficiency maintained until {metrics['efficiency_threshold']:.1f}x current spend\\n\"\n",
    "            else:\n",
    "                content += f\"- {channel.upper()}: Efficiency maintained beyond modeled range\\n\"\n",
    "        content += \"\\n\"\n",
    "        \n",
    "        # Scale and Opportunity Analysis\n",
    "        content += \"Scale and Growth Opportunity Analysis:\\n\"\n",
    "        \n",
    "        # Most efficient vs largest channels\n",
    "        most_efficient = sorted_channels[0]\n",
    "        largest_spend = max(channel_metrics.items(), key=lambda x: x[1]['current_spend'])\n",
    "        largest_revenue = max(channel_metrics.items(), key=lambda x: x[1]['current_revenue'])\n",
    "        \n",
    "        content += f\"- Most Efficient Channel: {most_efficient[0].upper()} ({most_efficient[1]['current_roi']:.2f}x ROI)\\n\"\n",
    "        content += f\"- Largest Spend Channel: {largest_spend[0].upper()} (${largest_spend[1]['current_spend']:,.0f})\\n\"\n",
    "        content += f\"- Highest Revenue Channel: {largest_revenue[0].upper()} (${largest_revenue[1]['current_revenue']:,.0f})\\n\"\n",
    "        \n",
    "        # Scale ratios\n",
    "        spend_ratios = []\n",
    "        for channel, metrics in channel_metrics.items():\n",
    "            ratio = largest_spend[1]['current_spend'] / metrics['current_spend']\n",
    "            spend_ratios.append((channel, ratio))\n",
    "        \n",
    "        spend_ratios.sort(key=lambda x: x[1], reverse=True)\n",
    "        content += f\"\\nSpend Scale Differences:\\n\"\n",
    "        for channel, ratio in spend_ratios:\n",
    "            if ratio > 1:\n",
    "                content += f\"- {largest_spend[0].upper()} spends {ratio:.1f}x more than {channel.upper()}\\n\"\n",
    "        \n",
    "        content += \"\\n\"\n",
    "        \n",
    "        # Portfolio Concentration Analysis\n",
    "        spend_concentration = (largest_spend[1]['current_spend'] / total_spend) * 100\n",
    "        revenue_concentration = (largest_revenue[1]['current_revenue'] / total_revenue) * 100\n",
    "        \n",
    "        content += \"Portfolio Concentration:\\n\"\n",
    "        content += f\"- Spend Concentration: Top channel ({largest_spend[0].upper()}) represents {spend_concentration:.1f}% of total spend\\n\"\n",
    "        content += f\"- Revenue Concentration: Top channel ({largest_revenue[0].upper()}) represents {revenue_concentration:.1f}% of total revenue\\n\"\n",
    "        \n",
    "        high_share_channels = len([ch for ch, m in channel_metrics.items() if (m['current_spend']/total_spend) > 0.1])\n",
    "        content += f\"- Channels with >10% spend share: {high_share_channels} of {len(channel_metrics)}\\n\\n\"\n",
    "        \n",
    "        # Growth Potential Summary\n",
    "        content += \"Growth Potential Summary:\\n\"\n",
    "        for channel, metrics in sorted_channels:\n",
    "            max_growth = (metrics['max_modeled_revenue'] / metrics['current_revenue']) - 1\n",
    "            max_spend_multiplier = metrics['max_modeled_spend'] / metrics['current_spend']\n",
    "            content += f\"- {channel.upper()}: Up to {max_growth*100:.0f}% revenue growth potential (max {max_spend_multiplier:.1f}x current spend)\\n\"\n",
    "        \n",
    "        content += f\"\\nMethodology Note: Response curves show cumulative incremental revenue from total media spend over the selected time period, constructed based on historical flighting patterns.\"\n",
    "        \n",
    "        \n",
    "        return content\n",
    "        \n",
    "    except (json.JSONDecodeError, KeyError, AttributeError) as e:\n",
    "        return []\n",
    "    \n",
    "print(extract_response_curves_data_for_rag(soup))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Contribution Analysis:\n",
      "\n",
      "Business Context: Your channel contributions help you\n",
      "understand what drove your revenue. Dv_360_X1 and Google drove the most overall\n",
      "revenue.\n",
      "\n",
      "Revenue Attribution:\n",
      "- Baseline revenue accounts for 90.9% of total revenue\n",
      "- Marketing channels drive 9.1% of total revenue\n",
      "- Total revenue split: 90.9% organic/baseline vs 9.1% paid marketing\n",
      "\n",
      "Marketing Channel Performance:\n",
      "- DV_360_X1 contributes 6.0% of total revenue ($7,227,618)\n",
      "- GOOGLE contributes 2.0% of total revenue ($2,444,298)\n",
      "- META contributes 1.0% of total revenue ($1,245,430)\n",
      "- TIKTOK contributes 0.1% of total revenue ($61,527)\n",
      "\n",
      "Key Insights:\n",
      "- DV_360_X1 is the top performing marketing channel at 6.0%\n",
      "- Baseline/organic traffic dominates revenue generation at 90.9%\n",
      "- Marketing channels collectively contribute 9.1% to total revenue\n",
      "\n",
      "Methodology: Note: This graphic encompasses all of\n",
      "your revenue drivers, but breaks down your marketing revenue by the baseline\n",
      "and all channels.\n",
      "Marketing Channel Spend and ROI Analysis:\n",
      "\n",
      "Performance Overview:\n",
      "- Marketing channels account for 100.0% of attributed revenue\n",
      "- Total marketing spend allocation: 100.0%\n",
      "- Average ROI across all channels: 4.8x\n",
      "\n",
      "Channel Performance by ROI:\n",
      "- META: 5.1x ROI, 11.3% revenue share, 10.4% spend share\n",
      "- TIKTOK: 5.0x ROI, 0.6% revenue share, 0.5% spend share\n",
      "- GOOGLE: 4.6x ROI, 22.3% revenue share, 22.5% spend share\n",
      "- DV_360_X1: 4.6x ROI, 65.8% revenue share, 66.6% spend share\n",
      "\n",
      "Channel Efficiency Analysis:\n",
      "- Most efficient spend allocation: META (revenue/spend ratio: 1.09)\n",
      "- Least efficient spend allocation: DV_360_X1 (revenue/spend ratio: 0.99)\n",
      "\n",
      "ROI Performance:\n",
      "- Highest ROI: META at 5.1x return\n",
      "- Lowest ROI: DV_360_X1 at 4.6x return\n",
      "- ROI range: 4.6x to 5.1x across all channels\n",
      "\n",
      "Budget Allocation Insights:\n",
      "- META: Over-performing (generates 11.3% revenue with 10.4% spend)\n",
      "- TIKTOK: Over-performing (generates 0.6% revenue with 0.5% spend)\n",
      "- GOOGLE: Under-performing (generates 22.3% revenue with 22.5% spend)\n",
      "- DV_360_X1: Under-performing (generates 65.8% revenue with 66.6% spend)\n",
      "\n",
      "Methodology: Note: Return on investment is calculated by\n",
      "dividing the revenue attributed to a channel by marketing costs.\n",
      "=== MONTHLY CHANNEL PERFORMANCE WITH ANOMALIES ===\n",
      "Monthly Channel Performance Analysis with Anomaly Detection:\n",
      "\n",
      "DV_360_X1 - Monthly Performance:\n",
      "- Average Monthly Revenue: $161,021\n",
      "- Peak Month: 2024-08 ($286,080)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 10 out of 30\n",
      "- Decline Periods: 2025-01 (-55%), 2025-02 (-80%)\n",
      "\n",
      "\n",
      "GOOGLE - Monthly Performance:\n",
      "- Average Monthly Revenue: $35,335\n",
      "- Peak Month: 2023-10 ($71,109)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 16 out of 30\n",
      "- High Growth Periods: 2024-05 (+1598%), 2024-10 (+220%)\n",
      "- Decline Periods: 2024-01 (-74%), 2024-02 (-82%), 2024-09 (-77%)\n",
      "\n",
      "\n",
      "META - Monthly Performance:\n",
      "- Average Monthly Revenue: $20,639\n",
      "- Peak Month: 2024-05 ($35,488)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 14 out of 30\n",
      "- High Growth Periods: 2024-05 (+79%)\n",
      "- Decline Periods: 2025-01 (-58%), 2025-02 (-63%)\n",
      "\n",
      "\n",
      "TIKTOK - Monthly Performance:\n",
      "- Average Monthly Revenue: $3,254\n",
      "- Peak Month: 2024-12 ($7,722)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 4 out of 30\n",
      "- High Growth Periods: 2024-12 (+208%)\n",
      "- Decline Periods: 2025-01 (-70%), 2025-02 (-80%)\n",
      "\n",
      "=== QUARTERLY CHANNEL TRENDS ===\n",
      "Quarterly Channel Performance Analysis with Trend Detection:\n",
      "\n",
      "DV_360_X1 - Quarterly Analysis:\n",
      "- Trend Direction: Stable\n",
      "- Best Quarter: 2024-Q3 ($276,999)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 2\n",
      "- Major Decline Quarters: 2025-Q1\n",
      "- Recent Performance (Last 2Q): $0 average\n",
      "\n",
      "\n",
      "GOOGLE - Quarterly Analysis:\n",
      "- Trend Direction: Stable\n",
      "- Best Quarter: 2023-Q4 ($66,506)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 4\n",
      "- Major Growth Quarters: 2024-Q4\n",
      "- Major Decline Quarters: 2024-Q1, 2024-Q3, 2025-Q1\n",
      "- Recent Performance (Last 2Q): $0 average\n",
      "\n",
      "\n",
      "META - Quarterly Analysis:\n",
      "- Trend Direction: Consistently Declining\n",
      "- Best Quarter: 2024-Q2 ($34,550)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 5\n",
      "- Major Decline Quarters: 2025-Q1\n",
      "- Recent Performance (Last 2Q): $5,039 average\n",
      "\n",
      "\n",
      "TIKTOK - Quarterly Analysis:\n",
      "- Trend Direction: Stable\n",
      "- Best Quarter: 2024-Q4 ($7,722)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 1\n",
      "- Major Decline Quarters: 2025-Q1\n",
      "- Recent Performance (Last 2Q): $0 average\n",
      "\n",
      "=== SPIKE AND DIP ANALYSIS ===\n",
      "Revenue Anomaly Analysis - Spikes and Dips Across All Channels:\n",
      "\n",
      "Channel Volatility Analysis (Revenue Consistency):\n",
      "- Most Stable Channel: META (58% volatility)\n",
      "- Most Volatile Channel: TIKTOK (96% volatility)\n",
      "\n",
      "=== CHANNEL COMPARISON ===\n",
      "Channel Performance Comparison and Rankings:\n",
      "\n",
      "Total Revenue Rankings:\n",
      "1. DV_360_X1: $1,610,209 cumulative revenue\n",
      "2. GOOGLE: $565,360 cumulative revenue\n",
      "3. META: $288,948 cumulative revenue\n",
      "4. TIKTOK: $13,017 cumulative revenue\n",
      "\n",
      "Consistency Rankings (% of months active):\n",
      "1. GOOGLE: 53% of months with revenue\n",
      "2. META: 47% of months with revenue\n",
      "3. DV_360_X1: 33% of months with revenue\n",
      "4. TIKTOK: 13% of months with revenue\n",
      "\n",
      "Peak Performance Rankings (Highest single month):\n",
      "1. DV_360_X1: $286,080 peak monthly revenue\n",
      "2. GOOGLE: $71,109 peak monthly revenue\n",
      "3. META: $35,488 peak monthly revenue\n",
      "4. TIKTOK: $7,722 peak monthly revenue\n",
      "\n",
      "Channel Lifecycle Analysis:\n",
      "- DV_360_X1: Dormant (Active: 2024-01 to 2025-02)\n",
      "- GOOGLE: Dormant (Active: 2023-10 to 2025-02)\n",
      "- META: Active/Mature (Active: 2024-04 to 2025-05)\n",
      "- TIKTOK: Dormant (Active: 2024-11 to 2025-02)\n",
      "\n",
      "=== MOMENTUM ANALYSIS ===\n",
      "Channel Growth Momentum and Trend Analysis:\n",
      "\n",
      "DV_360_X1 - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-100%)\n",
      "- Trend Consistency: 0% of months showed growth\n",
      "- Recent Average (Last 3 months): $0\n",
      "- Previous Average (3 months prior): $108,797\n",
      "\n",
      "\n",
      "GOOGLE - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-100%)\n",
      "- Trend Consistency: 0% of months showed growth\n",
      "- Recent Average (Last 3 months): $0\n",
      "- Previous Average (3 months prior): $18,947\n",
      "\n",
      "\n",
      "META - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-67%)\n",
      "- Trend Consistency: 20% of months showed growth\n",
      "- Recent Average (Last 3 months): $4,841\n",
      "- Previous Average (3 months prior): $14,516\n",
      "\n",
      "\n",
      "TIKTOK - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-100%)\n",
      "- Trend Consistency: 0% of months showed growth\n",
      "- Recent Average (Last 3 months): $0\n",
      "- Previous Average (3 months prior): $3,504\n",
      "Title: Return on investment\n",
      "\n",
      "Insights: Your return on investment (ROI) helps\n",
      "you understand how your marketing activities impacted your business objectives.\n",
      "Meta drove the highest ROI at 5.1. For every $1\n",
      "you spent on Meta, you saw $5.11 in revenue.\n",
      "Dv_360_X1 had the highest effectiveness, which is your\n",
      "incremental outcome per media unit. Meta had the highest marginal\n",
      "ROI at 2.08. Meta drove the lowest CPIK\n",
      "at $0.20. For every KPI unit, you spent $0.20.\n",
      "ROI vs Effectiveness Analysis:\n",
      "\n",
      "Performance Overview:\n",
      "- Average ROI across channels: 4.8x\n",
      "- Average effectiveness: 0.0169 incremental outcome per impression\n",
      "- Total media spend analyzed: $2,340,830\n",
      "\n",
      "ROI Rankings:\n",
      "1. META: 5.1x ROI\n",
      "2. TIKTOK: 5.0x ROI\n",
      "3. GOOGLE: 4.6x ROI\n",
      "4. DV_360_X1: 4.6x ROI\n",
      "\n",
      "Effectiveness Rankings:\n",
      "1. DV_360_X1: 0.0491 outcome per impression\n",
      "2. GOOGLE: 0.0065 outcome per impression\n",
      "3. META: 0.0062 outcome per impression\n",
      "4. TIKTOK: 0.0057 outcome per impression\n",
      "\n",
      "Spend Allocation:\n",
      "1. DV_360_X1: $1,558,208 (66.6% of total spend)\n",
      "2. GOOGLE: $526,693 (22.5% of total spend)\n",
      "3. META: $243,623 (10.4% of total spend)\n",
      "4. TIKTOK: $12,306 (0.5% of total spend)\n",
      "\n",
      "Channel Performance Categories:\n",
      "\n",
      "High Potential:\n",
      "- DV_360_X1 (4.6x ROI, 0.0491 effectiveness)\n",
      "\n",
      "Optimization Needed:\n",
      "- GOOGLE (4.6x ROI, 0.0065 effectiveness)\n",
      "\n",
      "Cost Efficient:\n",
      "- META (5.1x ROI, 0.0062 effectiveness)\n",
      "- TIKTOK (5.0x ROI, 0.0057 effectiveness)\n",
      "\n",
      "Strategic Insights:\n",
      "- META delivers highest ROI (5.1x) - prioritize for budget allocation\n",
      "- DV_360_X1 shows highest effectiveness (0.0491) - strong media performance per impression\n",
      "- DV_360_X1 receives largest budget ($1,558,208) - monitor efficiency closely\n",
      "- DV_360_X1: High effectiveness but expensive - optimize costs to improve ROI\n",
      "- GOOGLE: Both ROI and effectiveness below average - requires optimization or budget reallocation\n",
      "- META: Cost efficient but low reach - consider scaling if effectiveness can be maintained\n",
      "- TIKTOK: Cost efficient but low reach - consider scaling if effectiveness can be maintained\n",
      "\n",
      "Methodology: Note: Effectiveness measures the\n",
      "incremental outcome generated per impression. A low ROI does not necessarily\n",
      "imply low media effectiveness; it may result from high media cost, as positioned\n",
      "in the upper-left corner of the chart. Conversely, a high ROI can coexist with\n",
      "low media effectiveness and low media costs, as indicated in the bottom-right\n",
      "corner of the chart. The diagonal section of the chart suggests that the ROI is\n",
      "primarily influenced by media effectiveness. The size of the bubbles represents\n",
      "the scale of the media spend.\n",
      "ROI vs Marginal ROI Performance Analysis:\n",
      "\n",
      "Performance Metrics:\n",
      "- Average ROI across channels: 4.8x\n",
      "- Average Marginal ROI: 2.0x (additional return per additional dollar)\n",
      "- Total media spend analyzed: $2,340,830\n",
      "\n",
      "ROI Rankings:\n",
      "1. META: 5.1x ROI\n",
      "2. TIKTOK: 5.0x ROI\n",
      "3. GOOGLE: 4.6x ROI\n",
      "4. DV_360_X1: 4.6x ROI\n",
      "\n",
      "Marginal ROI Rankings (Incremental Efficiency):\n",
      "1. META: 2.1x marginal ROI\n",
      "2. TIKTOK: 2.0x marginal ROI\n",
      "3. GOOGLE: 1.9x marginal ROI\n",
      "4. DV_360_X1: 1.8x marginal ROI\n",
      "\n",
      "Spend Distribution:\n",
      "1. DV_360_X1: $1,558,208 (66.6% of total spend)\n",
      "2. GOOGLE: $526,693 (22.5% of total spend)\n",
      "3. META: $243,623 (10.4% of total spend)\n",
      "4. TIKTOK: $12,306 (0.5% of total spend)\n",
      "\n",
      "Saturation Indicators:\n",
      "- DV_360_X1: High saturation signal (ROI/Marginal ROI ratio: 2.6x)\n",
      "- GOOGLE: Moderate saturation signal (ROI/Marginal ROI ratio: 2.4x)\n",
      "- META: Moderate saturation signal (ROI/Marginal ROI ratio: 2.5x)\n",
      "- TIKTOK: Moderate saturation signal (ROI/Marginal ROI ratio: 2.5x)\n",
      "\n",
      "Efficiency Gap Analysis:\n",
      "- DV_360_X1: 2.9x gap between current and marginal returns\n",
      "- GOOGLE: 2.7x gap between current and marginal returns\n",
      "- META: 3.0x gap between current and marginal returns\n",
      "- TIKTOK: 3.0x gap between current and marginal returns\n",
      "\n",
      "Performance Consistency:\n",
      "- ROI variation across channels: 0.21 standard deviation\n",
      "- Marginal ROI variation: 0.12 standard deviation\n",
      "- ROI and marginal ROI show similar variation patterns\n",
      "\n",
      "Channel Performance Patterns:\n",
      "- META leads in both ROI and marginal ROI - consistent high performer\n",
      "- DV_360_X1: High spend concentration (66.6%) with below-average marginal efficiency\n",
      "- META: Low spend allocation (10.4%) but above-average marginal efficiency\n",
      "- TIKTOK: Low spend allocation (0.5%) but above-average marginal efficiency\n",
      "\n",
      "Diminishing Returns Assessment:\n",
      "- DV_360_X1: Moderate diminishing returns (current ROI 2.6x higher than marginal)\n",
      "- GOOGLE: Moderate diminishing returns (current ROI 2.4x higher than marginal)\n",
      "- META: Moderate diminishing returns (current ROI 2.5x higher than marginal)\n",
      "- TIKTOK: Moderate diminishing returns (current ROI 2.5x higher than marginal)\n",
      "\n",
      "Methodology: Note: Marginal ROI measures the additional\n",
      "return generated for every additional dollar spent. It's an indicator of\n",
      "efficiency of additional spend. Channels with a high ROI but a low marginal ROI\n",
      "are likely in the saturation phase, where the initial investments have paid off,\n",
      "but additional investment does not bring in as much return. Conversely, channels\n",
      "that have a high ROI and a high marginal ROI perform well and continue to yield\n",
      "high returns with additional spending. The size of the bubbles represents the\n",
      "scale of the media spend.\n",
      "ROI and CPIK Performance Analysis with Confidence Intervals:\n",
      "\n",
      "ROI Performance with 90% Credible Intervals:\n",
      "1. META:\n",
      "   - Point Estimate: 5.11x ROI\n",
      "   - Confidence Range: 3.54x to 6.98x\n",
      "   - Uncertainty Level: ±67% around point estimate\n",
      "2. TIKTOK:\n",
      "   - Point Estimate: 5.00x ROI\n",
      "   - Confidence Range: 4.92x to 5.08x\n",
      "   - Uncertainty Level: ±3% around point estimate\n",
      "3. GOOGLE:\n",
      "   - Point Estimate: 4.64x ROI\n",
      "   - Confidence Range: 2.18x to 8.31x\n",
      "   - Uncertainty Level: ±132% around point estimate\n",
      "4. DV_360_X1:\n",
      "   - Point Estimate: 4.64x ROI\n",
      "   - Confidence Range: 1.40x to 8.48x\n",
      "   - Uncertainty Level: ±153% around point estimate\n",
      "\n",
      "ROI Confidence Analysis:\n",
      "- Most Reliable ROI Estimate: TIKTOK (±3% uncertainty)\n",
      "- Least Reliable ROI Estimate: DV_360_X1 (±153% uncertainty)\n",
      "\n",
      "ROI Statistical Significance:\n",
      "- DV_360_X1 and GOOGLE: Overlapping confidence intervals (no statistically significant difference)\n",
      "- DV_360_X1 and META: Overlapping confidence intervals (no statistically significant difference)\n",
      "- DV_360_X1 and TIKTOK: Overlapping confidence intervals (no statistically significant difference)\n",
      "- GOOGLE and META: Overlapping confidence intervals (no statistically significant difference)\n",
      "- GOOGLE and TIKTOK: Overlapping confidence intervals (no statistically significant difference)\n",
      "- META and TIKTOK: Overlapping confidence intervals (no statistically significant difference)\n",
      "\n",
      "CPIK Performance with 90% Credible Intervals:\n",
      "1. META (Best to Worst CPIK):\n",
      "   - Point Estimate: $0.200 per KPI unit\n",
      "   - Confidence Range: $0.143 to $0.282\n",
      "   - Uncertainty Level: ±70% around point estimate\n",
      "2. TIKTOK (Best to Worst CPIK):\n",
      "   - Point Estimate: $0.200 per KPI unit\n",
      "   - Confidence Range: $0.197 to $0.203\n",
      "   - Uncertainty Level: ±3% around point estimate\n",
      "3. DV_360_X1 (Best to Worst CPIK):\n",
      "   - Point Estimate: $0.224 per KPI unit\n",
      "   - Confidence Range: $0.118 to $0.716\n",
      "   - Uncertainty Level: ±268% around point estimate\n",
      "4. GOOGLE (Best to Worst CPIK):\n",
      "   - Point Estimate: $0.234 per KPI unit\n",
      "   - Confidence Range: $0.120 to $0.460\n",
      "   - Uncertainty Level: ±145% around point estimate\n",
      "\n",
      "CPIK Confidence Analysis:\n",
      "- Most Reliable CPIK Estimate: TIKTOK (±3% uncertainty)\n",
      "- Least Reliable CPIK Estimate: DV_360_X1 (±268% uncertainty)\n",
      "\n",
      "CPIK Statistical Significance:\n",
      "- DV_360_X1 and GOOGLE: Overlapping CPIK intervals (no statistically significant difference)\n",
      "- DV_360_X1 and META: Overlapping CPIK intervals (no statistically significant difference)\n",
      "- DV_360_X1 and TIKTOK: Overlapping CPIK intervals (no statistically significant difference)\n",
      "- GOOGLE and META: Overlapping CPIK intervals (no statistically significant difference)\n",
      "- GOOGLE and TIKTOK: Overlapping CPIK intervals (no statistically significant difference)\n",
      "- META and TIKTOK: Overlapping CPIK intervals (no statistically significant difference)\n",
      "\n",
      "Combined ROI and CPIK Insights:\n",
      "\n",
      "High Confidence Performers (Low uncertainty in both ROI and CPIK):\n",
      "- TIKTOK: 5.00x ROI (±3%), $0.200 CPIK (±3%)\n",
      "\n",
      "Uncertainty Patterns:\n",
      "- Average ROI uncertainty: ±89%\n",
      "- Average CPIK uncertainty: ±121%\n",
      "- CPIK estimates show higher uncertainty than ROI estimates\n",
      "\n",
      "Model Reliability Assessment:\n",
      "- High ROI Confidence: TIKTOK (narrow intervals)\n",
      "- Low ROI Confidence: DV_360_X1, GOOGLE (wide intervals)\n",
      "- High CPIK Confidence: TIKTOK (narrow intervals)\n",
      "- Low CPIK Confidence: DV_360_X1, GOOGLE (wide intervals)\n",
      "\n",
      "Methodology Note: Note: CPIK (cost per incremental KPI) point\n",
      "estimate is determined by the posterior median, whereas ROI point estimate is\n",
      "determined by the posterior mean.Marketing Response Curves Performance Analysis:\n",
      "\n",
      "Current Channel Performance:\n",
      "1. META:\n",
      "   - Current Spend: $243,623 (10.4% of total)\n",
      "   - Current Revenue: $1,245,430 (11.3% of total)\n",
      "   - Current ROI: 5.11x\n",
      "2. TIKTOK:\n",
      "   - Current Spend: $12,306 (0.5% of total)\n",
      "   - Current Revenue: $61,527 (0.6% of total)\n",
      "   - Current ROI: 5.00x\n",
      "3. GOOGLE:\n",
      "   - Current Spend: $526,693 (22.5% of total)\n",
      "   - Current Revenue: $2,444,298 (22.3% of total)\n",
      "   - Current ROI: 4.64x\n",
      "4. DV_360_X1:\n",
      "   - Current Spend: $1,558,208 (66.6% of total)\n",
      "   - Current Revenue: $7,227,618 (65.8% of total)\n",
      "   - Current ROI: 4.64x\n",
      "\n",
      "Portfolio Summary:\n",
      "- Total Spend: $2,340,830\n",
      "- Total Revenue: $10,978,873\n",
      "- Overall ROI: 4.69x\n",
      "- Active Channels: 4\n",
      "\n",
      "Spend Increase Scenario Analysis:\n",
      "\n",
      "25% Spend Increase Scenarios:\n",
      "- META: +$60,906 → +$113,594 (Marginal ROI: 1.87x)\n",
      "- TIKTOK: +$3,077 → +$5,627 (Marginal ROI: 1.83x)\n",
      "- GOOGLE: +$131,673 → +$228,438 (Marginal ROI: 1.73x)\n",
      "- DV_360_X1: +$389,552 → +$614,472 (Marginal ROI: 1.58x)\n",
      "Portfolio 25% Increase: +$585,208 → +$962,132 (Portfolio Marginal ROI: 1.64x)\n",
      "\n",
      "50% Spend Increase Scenarios:\n",
      "- META: +$121,811 → +$206,189 (Marginal ROI: 1.69x)\n",
      "- TIKTOK: +$6,153 → +$10,243 (Marginal ROI: 1.66x)\n",
      "- GOOGLE: +$263,346 → +$414,808 (Marginal ROI: 1.58x)\n",
      "- DV_360_X1: +$779,104 → +$1,105,102 (Marginal ROI: 1.42x)\n",
      "Portfolio 50% Increase: +$1,170,415 → +$1,736,342 (Portfolio Marginal ROI: 1.48x)\n",
      "\n",
      "100% Spend Increase Scenarios:\n",
      "- META: +$243,623 → +$350,315 (Marginal ROI: 1.44x)\n",
      "- TIKTOK: +$12,306 → +$17,486 (Marginal ROI: 1.42x)\n",
      "- GOOGLE: +$526,693 → +$704,568 (Marginal ROI: 1.34x)\n",
      "- DV_360_X1: +$1,558,208 → +$1,847,447 (Marginal ROI: 1.19x)\n",
      "Portfolio 100% Increase: +$2,340,830 → +$2,919,815 (Portfolio Marginal ROI: 1.25x)\n",
      "\n",
      "Marginal Returns Analysis:\n",
      "\n",
      "Efficiency Threshold Analysis:\n",
      "- META: Efficiency maintained until 1.2x current spend\n",
      "- TIKTOK: Efficiency maintained until 1.2x current spend\n",
      "- GOOGLE: Efficiency maintained until 1.2x current spend\n",
      "- DV_360_X1: Efficiency maintained until 1.2x current spend\n",
      "\n",
      "Scale and Growth Opportunity Analysis:\n",
      "- Most Efficient Channel: META (5.11x ROI)\n",
      "- Largest Spend Channel: DV_360_X1 ($1,558,208)\n",
      "- Highest Revenue Channel: DV_360_X1 ($7,227,618)\n",
      "\n",
      "Spend Scale Differences:\n",
      "- DV_360_X1 spends 126.6x more than TIKTOK\n",
      "- DV_360_X1 spends 6.4x more than META\n",
      "- DV_360_X1 spends 3.0x more than GOOGLE\n",
      "\n",
      "Portfolio Concentration:\n",
      "- Spend Concentration: Top channel (DV_360_X1) represents 66.6% of total spend\n",
      "- Revenue Concentration: Top channel (DV_360_X1) represents 65.8% of total revenue\n",
      "- Channels with >10% spend share: 3 of 4\n",
      "\n",
      "Growth Potential Summary:\n",
      "- META: Up to 32% revenue growth potential (max 2.2x current spend)\n",
      "- TIKTOK: Up to 32% revenue growth potential (max 2.2x current spend)\n",
      "- GOOGLE: Up to 32% revenue growth potential (max 2.2x current spend)\n",
      "- DV_360_X1: Up to 29% revenue growth potential (max 2.2x current spend)\n",
      "\n",
      "Methodology Note: Response curves show cumulative incremental revenue from total media spend over the selected time period, constructed based on historical flighting patterns.\n"
     ]
    }
   ],
   "source": [
    "result = \"\"\n",
    "\n",
    "result = get_channel_contribution(soup) + \"\\n\" + get_spend_outcome_insights(soup) + \"\\n\" + get_channel_time_insights_with_anomalies(soup)['formatted_output'] + \"\\n\" + get_roi_insights(soup) + \"\\n\"+ get_roi_effectiveness_insights(soup) + \"\\n\" + get_roi_marginal_insights(soup) + \"\\n\" + get_roi_cpik_confidence_insights(soup) + extract_response_curves_data_for_rag(soup)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result saved to output.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the result to a text file\n",
    "with open(\"summary_extract_output.txt\", \"w\") as f:\n",
    "    f.write(result)\n",
    "\n",
    "print(\"Result saved to output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Read your HTML file\n",
    "with open('output/summary_output.html', 'r', encoding='utf-8') as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Example: Extract the Vega-Lite JSON for the response curves chart\n",
    "# (You can change the chart id to any other, e.g., 'channel-drivers-chart', etc.)\n",
    "chart_id = 'response-curves-chart'\n",
    "\n",
    "# Find the script block for the chart\n",
    "pattern = re.compile(\n",
    "    r'chart-embed id=\"' + re.escape(chart_id) + r'\".*?JSON\\.parse\\(\"({.*?})\"\\);',\n",
    "    re.DOTALL\n",
    ")\n",
    "match = pattern.search(html)\n",
    "if not match:\n",
    "    raise ValueError(f\"Could not find Vega-Lite spec for chart id '{chart_id}'\")\n",
    "\n",
    "# Unescape and load the JSON\n",
    "vega_json_str = match.group(1)\n",
    "vega_json_str = vega_json_str.encode('utf-8').decode('unicode_escape')\n",
    "vega_spec = json.loads(vega_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(vega_spec['datasets'].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 15:19:29.015 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-03 15:19:29.075 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/plansuraamornkul/miniconda3/envs/idac_new/lib/python3.13/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-07-03 15:19:29.076 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-03 15:19:29.076 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import streamlit as st\n",
    "\n",
    "# Step 2 continued: Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 3: Plot (example for response curves)\n",
    "chart = alt.Chart(df).mark_line().encode(\n",
    "    x=alt.X('spend', title='Spend'),\n",
    "    y=alt.Y('mean', title='Incremental outcome'),\n",
    "    color='channel:N',\n",
    "    tooltip=['channel', 'spend', 'mean']\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=400,\n",
    "    title=\"Response curves by marketing channel (top 4)\"\n",
    ")\n",
    "\n",
    "# Add current spend points if present\n",
    "if 'current_spend' in df.columns:\n",
    "    points = alt.Chart(df[df['current_spend'].notnull()]).mark_point(filled=True, size=80).encode(\n",
    "        x='spend',\n",
    "        y='mean',\n",
    "        color='channel:N',\n",
    "        shape='current_spend:N',\n",
    "        tooltip=['channel', 'spend', 'mean']\n",
    "    )\n",
    "    chart = chart + points\n",
    "\n",
    "st.altair_chart(chart, use_container_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1067841559.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mstreamlit run /Users/plansuraamornkul/miniconda3/envs/idac_new/lib/python3.13/site-packages/ipykernel_launcher.py\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idac_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78be259ae8f52cb4f09525a9f670116c3dc26611fae831ffbbd168bd6bc01594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
