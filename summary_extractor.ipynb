{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/summary_output.html', 'r', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Contribution Analysis:\n",
      "\n",
      "Business Context: Your channel contributions help you\n",
      "understand what drove your revenue. Dv_360_X1 and Google drove the most overall\n",
      "revenue.\n",
      "\n",
      "Revenue Attribution:\n",
      "- Baseline revenue accounts for 90.9% of total revenue\n",
      "- Marketing channels drive 9.1% of total revenue\n",
      "- Total revenue split: 90.9% organic/baseline vs 9.1% paid marketing\n",
      "\n",
      "Marketing Channel Performance:\n",
      "- DV_360_X1 contributes 6.0% of total revenue ($7,227,618)\n",
      "- GOOGLE contributes 2.0% of total revenue ($2,444,298)\n",
      "- META contributes 1.0% of total revenue ($1,245,430)\n",
      "- TIKTOK contributes 0.1% of total revenue ($61,527)\n",
      "\n",
      "Key Insights:\n",
      "- DV_360_X1 is the top performing marketing channel at 6.0%\n",
      "- Baseline/organic traffic dominates revenue generation at 90.9%\n",
      "- Marketing channels collectively contribute 9.1% to total revenue\n",
      "\n",
      "Methodology: Note: This graphic encompasses all of\n",
      "your revenue drivers, but breaks down your marketing revenue by the baseline\n",
      "and all channels.\n"
     ]
    }
   ],
   "source": [
    "def get_channel_contribution(soup):\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    chart_card = soup.find(\"card\", {\"id\": \"channel-contrib\"})\n",
    "    \n",
    "    if not chart_card:\n",
    "        return \"No channel contribution data found.\"\n",
    "    \n",
    "    # Extract data (same extraction logic as before)\n",
    "    insight_text = chart_card.find(\"p\", {\"class\": \"insights-text\"})\n",
    "    insight_text_content = insight_text.get_text(strip=True) if insight_text else None\n",
    "    \n",
    "    card_title = chart_card.find(\"card-title\")\n",
    "    card_title_content = card_title.get_text(strip=True) if card_title else None\n",
    "    \n",
    "    chart_description = chart_card.find(\"chart-description\")\n",
    "    chart_description_content = chart_description.get_text(strip=True) if chart_description else None\n",
    "    \n",
    "    # [Same JSON extraction logic as before...]\n",
    "    script_tag = chart_card.find(\"script\", {\"type\": \"text/javascript\"})\n",
    "    channel_data = []\n",
    "    \n",
    "    if script_tag:\n",
    "        script_content = script_tag.get_text()\n",
    "        json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                escaped_json = json_match.group(1)\n",
    "                unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "                chart_spec = json.loads(unescaped_json)\n",
    "                datasets = chart_spec.get('datasets', {})\n",
    "                for dataset_key, dataset_value in datasets.items():\n",
    "                    if isinstance(dataset_value, list):\n",
    "                        channel_data = dataset_value\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Process data\n",
    "    baseline_data = None\n",
    "    marketing_channels = []\n",
    "    \n",
    "    for channel in channel_data:\n",
    "        if channel.get('channel') == 'BASELINE':\n",
    "            baseline_data = channel\n",
    "        else:\n",
    "            marketing_channels.append(channel)\n",
    "    \n",
    "    marketing_channels.sort(key=lambda x: x.get('incremental_outcome', 0), reverse=True)\n",
    "    \n",
    "    # Format for RAG/Vector DB\n",
    "    baseline_pct = round(baseline_data.get('pct_of_contribution', 0) * 100, 1) if baseline_data else 0\n",
    "    total_marketing_pct = round(sum(ch.get('pct_of_contribution', 0) for ch in marketing_channels) * 100, 1)\n",
    "    \n",
    "    # Create structured, searchable content\n",
    "    rag_content = f\"\"\"\n",
    "Channel Contribution Analysis:\n",
    "\n",
    "Business Context: {insight_text_content}\n",
    "\n",
    "Revenue Attribution:\n",
    "- Baseline revenue accounts for {baseline_pct}% of total revenue\n",
    "- Marketing channels drive {total_marketing_pct}% of total revenue\n",
    "- Total revenue split: {baseline_pct}% organic/baseline vs {total_marketing_pct}% paid marketing\n",
    "\n",
    "Marketing Channel Performance:\n",
    "\"\"\".strip()\n",
    "        \n",
    "    # Add individual channel performance\n",
    "    for i, channel in enumerate(marketing_channels, 1):\n",
    "        ch_pct = round(channel.get('pct_of_contribution', 0) * 100, 1)\n",
    "        revenue = channel.get('incremental_outcome', 0)\n",
    "        rag_content += f\"\\n- {channel.get('channel')} contributes {ch_pct}% of total revenue (${revenue:,.0f})\"\n",
    "    \n",
    "    # Add key insights for better retrieval\n",
    "    if marketing_channels:\n",
    "        top_channel = marketing_channels[0]\n",
    "        top_ch_pct = round(top_channel.get('pct_of_contribution', 0) * 100, 1)\n",
    "        rag_content += f\"\\n\\nKey Insights:\\n- {top_channel.get('channel')} is the top performing marketing channel at {top_ch_pct}%\"\n",
    "        rag_content += f\"\\n- Baseline/organic traffic dominates revenue generation at {baseline_pct}%\"\n",
    "        rag_content += f\"\\n- Marketing channels collectively contribute {total_marketing_pct}% to total revenue\"\n",
    "    \n",
    "    if chart_description_content:\n",
    "        rag_content += f\"\\n\\nMethodology: {chart_description_content}\"\n",
    "    \n",
    "    return rag_content\n",
    "\n",
    "\n",
    "print(get_channel_contribution(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marketing Channel Spend and ROI Analysis:\n",
      "\n",
      "Performance Overview:\n",
      "- Marketing channels account for 100.0% of attributed revenue\n",
      "- Total marketing spend allocation: 100.0%\n",
      "- Average ROI across all channels: 4.8x\n",
      "\n",
      "Channel Performance by ROI:\n",
      "- META: 5.1x ROI, 11.3% revenue share, 10.4% spend share\n",
      "- TIKTOK: 5.0x ROI, 0.6% revenue share, 0.5% spend share\n",
      "- GOOGLE: 4.6x ROI, 22.3% revenue share, 22.5% spend share\n",
      "- DV_360_X1: 4.6x ROI, 65.8% revenue share, 66.6% spend share\n",
      "\n",
      "Channel Efficiency Analysis:\n",
      "- Most efficient spend allocation: META (revenue/spend ratio: 1.09)\n",
      "- Least efficient spend allocation: DV_360_X1 (revenue/spend ratio: 0.99)\n",
      "\n",
      "ROI Performance:\n",
      "- Highest ROI: META at 5.1x return\n",
      "- Lowest ROI: DV_360_X1 at 4.6x return\n",
      "- ROI range: 4.6x to 5.1x across all channels\n",
      "\n",
      "Budget Allocation Insights:\n",
      "- META: Over-performing (generates 11.3% revenue with 10.4% spend)\n",
      "- TIKTOK: Over-performing (generates 0.6% revenue with 0.5% spend)\n",
      "- GOOGLE: Under-performing (generates 22.3% revenue with 22.5% spend)\n",
      "- DV_360_X1: Under-performing (generates 65.8% revenue with 66.6% spend)\n",
      "\n",
      "Methodology: Note: Return on investment is calculated by\n",
      "dividing the revenue attributed to a channel by marketing costs.\n"
     ]
    }
   ],
   "source": [
    "def get_spend_outcome_insights(soup):\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    # Find the spend-outcome chart\n",
    "    chart_embed = soup.find(\"chart-embed\", {\"id\": \"spend-outcome-chart\"})\n",
    "    if not chart_embed:\n",
    "        return \"No spend-outcome data found.\"\n",
    "    \n",
    "    # Find the parent chart element to get description and script\n",
    "    chart_element = chart_embed.find_parent(\"chart\")\n",
    "    if not chart_element:\n",
    "        return \"No spend-outcome chart element found.\"\n",
    "    \n",
    "    # Extract chart description\n",
    "    chart_description = chart_element.find(\"chart-description\")\n",
    "    chart_description_content = chart_description.get_text(strip=True) if chart_description else None\n",
    "    \n",
    "    # Extract chart data from script\n",
    "    script_tag = chart_element.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "    channel_data = []\n",
    "    \n",
    "    if script_tag:\n",
    "        script_content = script_tag.get_text()\n",
    "        \n",
    "        # Find the JSON.parse() content\n",
    "        json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                # Get the escaped JSON string and unescape it\n",
    "                escaped_json = json_match.group(1)\n",
    "                unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "                \n",
    "                # Parse the JSON\n",
    "                chart_spec = json.loads(unescaped_json)\n",
    "                \n",
    "                # Extract the dataset\n",
    "                datasets = chart_spec.get('datasets', {})\n",
    "                for dataset_key, dataset_value in datasets.items():\n",
    "                    if isinstance(dataset_value, list):\n",
    "                        channel_data = dataset_value\n",
    "                        break\n",
    "                        \n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                pass\n",
    "    \n",
    "    if not channel_data:\n",
    "        return \"No spend-outcome data could be extracted.\"\n",
    "    \n",
    "    # Process the data - separate revenue and spend data\n",
    "    revenue_data = [ch for ch in channel_data if ch.get('label') == '% Revenue']\n",
    "    spend_data = [ch for ch in channel_data if ch.get('label') == '% Spend']\n",
    "    \n",
    "    # Create channel analysis by combining revenue and spend data\n",
    "    channel_analysis = {}\n",
    "    \n",
    "    for rev_ch in revenue_data:\n",
    "        channel = rev_ch['channel']\n",
    "        spend_ch = next((s for s in spend_data if s['channel'] == channel), None)\n",
    "        \n",
    "        if spend_ch:\n",
    "            channel_analysis[channel] = {\n",
    "                'revenue_pct': rev_ch['pct'] * 100,\n",
    "                'spend_pct': spend_ch['pct'] * 100,\n",
    "                'roi': rev_ch['roi'],\n",
    "                'efficiency': rev_ch['pct'] / spend_ch['pct'] if spend_ch['pct'] > 0 else 0\n",
    "            }\n",
    "    \n",
    "    # Sort channels by ROI\n",
    "    sorted_channels = sorted(channel_analysis.items(), key=lambda x: x[1]['roi'], reverse=True)\n",
    "    \n",
    "    # Calculate totals\n",
    "    total_revenue_pct = sum(ch['revenue_pct'] for ch in channel_analysis.values())\n",
    "    total_spend_pct = sum(ch['spend_pct'] for ch in channel_analysis.values())\n",
    "    avg_roi = sum(ch['roi'] for ch in channel_analysis.values()) / len(channel_analysis)\n",
    "    \n",
    "    # Format results for RAG\n",
    "    rag_content = f\"\"\"\n",
    "Marketing Channel Spend and ROI Analysis:\n",
    "\n",
    "Performance Overview:\n",
    "- Marketing channels account for {total_revenue_pct:.1f}% of attributed revenue\n",
    "- Total marketing spend allocation: {total_spend_pct:.1f}%\n",
    "- Average ROI across all channels: {avg_roi:.1f}x\n",
    "\n",
    "Channel Performance by ROI:\n",
    "\"\"\".strip()\n",
    "    \n",
    "    # Add individual channel performance\n",
    "    for channel, data in sorted_channels:\n",
    "        rag_content += f\"\\n- {channel.upper()}: {data['roi']:.1f}x ROI, {data['revenue_pct']:.1f}% revenue share, {data['spend_pct']:.1f}% spend share\"\n",
    "    \n",
    "    # Add efficiency insights\n",
    "    rag_content += f\"\\n\\nChannel Efficiency Analysis:\"\n",
    "    most_efficient = max(sorted_channels, key=lambda x: x[1]['efficiency'])\n",
    "    least_efficient = min(sorted_channels, key=lambda x: x[1]['efficiency'])\n",
    "    \n",
    "    rag_content += f\"\\n- Most efficient spend allocation: {most_efficient[0].upper()} (revenue/spend ratio: {most_efficient[1]['efficiency']:.2f})\"\n",
    "    rag_content += f\"\\n- Least efficient spend allocation: {least_efficient[0].upper()} (revenue/spend ratio: {least_efficient[1]['efficiency']:.2f})\"\n",
    "    \n",
    "    # ROI insights\n",
    "    best_roi_channel = sorted_channels[0]\n",
    "    worst_roi_channel = sorted_channels[-1]\n",
    "    \n",
    "    rag_content += f\"\\n\\nROI Performance:\"\n",
    "    rag_content += f\"\\n- Highest ROI: {best_roi_channel[0].upper()} at {best_roi_channel[1]['roi']:.1f}x return\"\n",
    "    rag_content += f\"\\n- Lowest ROI: {worst_roi_channel[0].upper()} at {worst_roi_channel[1]['roi']:.1f}x return\"\n",
    "    rag_content += f\"\\n- ROI range: {worst_roi_channel[1]['roi']:.1f}x to {best_roi_channel[1]['roi']:.1f}x across all channels\"\n",
    "    \n",
    "    # Budget allocation insights\n",
    "    rag_content += f\"\\n\\nBudget Allocation Insights:\"\n",
    "    for channel, data in sorted_channels:\n",
    "        if data['revenue_pct'] > data['spend_pct']:\n",
    "            rag_content += f\"\\n- {channel.upper()}: Over-performing (generates {data['revenue_pct']:.1f}% revenue with {data['spend_pct']:.1f}% spend)\"\n",
    "        elif data['revenue_pct'] < data['spend_pct']:\n",
    "            rag_content += f\"\\n- {channel.upper()}: Under-performing (generates {data['revenue_pct']:.1f}% revenue with {data['spend_pct']:.1f}% spend)\"\n",
    "        else:\n",
    "            rag_content += f\"\\n- {channel.upper()}: Balanced performance (revenue and spend percentages aligned)\"\n",
    "    \n",
    "    if chart_description_content:\n",
    "        rag_content += f\"\\n\\nMethodology: {chart_description_content}\"\n",
    "\n",
    "    return rag_content\n",
    "\n",
    "print(get_spend_outcome_insights(soup))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "import calendar\n",
    "\n",
    "def get_channel_time_insights_with_anomalies(soup):\n",
    "    \n",
    "    # [Same extraction logic as before...]\n",
    "    chart_embed = soup.find(\"chart-embed\", {\"id\": \"channel-contrib-by-time-chart\"})\n",
    "    if not chart_embed:\n",
    "        return \"No time-series channel contribution data found.\"\n",
    "    \n",
    "    chart_element = chart_embed.find_parent(\"chart\")\n",
    "    if not chart_element:\n",
    "        return \"No time-series chart element found.\"\n",
    "    \n",
    "    chart_description = chart_element.find(\"chart-description\")\n",
    "    chart_description_content = chart_description.get_text(strip=True) if chart_description else None\n",
    "    \n",
    "    script_tag = chart_element.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "    time_data = []\n",
    "    \n",
    "    if script_tag:\n",
    "        script_content = script_tag.get_text()\n",
    "        json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                escaped_json = json_match.group(1)\n",
    "                unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "                chart_spec = json.loads(unescaped_json)\n",
    "                datasets = chart_spec.get('datasets', {})\n",
    "                for dataset_key, dataset_value in datasets.items():\n",
    "                    if isinstance(dataset_value, list):\n",
    "                        time_data = dataset_value\n",
    "                        break\n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                pass\n",
    "    \n",
    "    if not time_data:\n",
    "        return \"No time-series data could be extracted.\"\n",
    "    \n",
    "    # Organize data by channel and time\n",
    "    channel_monthly_data = defaultdict(lambda: defaultdict(dict))\n",
    "    channel_quarterly_data = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    for record in time_data:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(record['time'], '%Y-%m-%d')\n",
    "            revenue = record.get('incremental_outcome', 0) or 0\n",
    "            contribution_pct = record.get('pct_of_contribution', 0) * 100\n",
    "            channel = record['channel']\n",
    "            \n",
    "            # Monthly data by channel\n",
    "            month_key = date_obj.strftime('%Y-%m')\n",
    "            channel_monthly_data[channel][month_key] = {\n",
    "                'revenue': revenue,\n",
    "                'contribution_pct': contribution_pct,\n",
    "                'date': date_obj\n",
    "            }\n",
    "            \n",
    "            # Quarterly data by channel\n",
    "            quarter = f\"Q{(date_obj.month - 1) // 3 + 1}\"\n",
    "            quarter_key = f\"{date_obj.year}-{quarter}\"\n",
    "            \n",
    "            # Keep the latest data point for each quarter\n",
    "            if (quarter_key not in channel_quarterly_data[channel] or \n",
    "                date_obj > channel_quarterly_data[channel][quarter_key].get('date', datetime.min)):\n",
    "                channel_quarterly_data[channel][quarter_key] = {\n",
    "                    'revenue': revenue,\n",
    "                    'contribution_pct': contribution_pct,\n",
    "                    'date': date_obj\n",
    "                }\n",
    "                \n",
    "        except (ValueError, KeyError):\n",
    "            continue\n",
    "    \n",
    "    # Generate comprehensive summaries\n",
    "    chunks = []\n",
    "    \n",
    "    # Chunk 1: Monthly Performance by Channel with Anomalies\n",
    "    chunk1 = create_monthly_channel_summaries_with_anomalies(channel_monthly_data)\n",
    "    chunks.append(chunk1)\n",
    "    \n",
    "    # Chunk 2: Quarterly Performance by Channel with Trends\n",
    "    chunk2 = create_quarterly_channel_summaries_with_trends(channel_quarterly_data)\n",
    "    chunks.append(chunk2)\n",
    "    \n",
    "    # Chunk 3: Spike and Dip Analysis Across All Channels\n",
    "    chunk3 = create_anomaly_analysis(channel_monthly_data)\n",
    "    chunks.append(chunk3)\n",
    "    \n",
    "    # Chunk 4: Channel Performance Comparison and Rankings\n",
    "    chunk4 = create_channel_comparison_analysis(channel_monthly_data, channel_quarterly_data)\n",
    "    chunks.append(chunk4)\n",
    "    \n",
    "    # Chunk 5: Growth Momentum and Trend Analysis\n",
    "    chunk5 = create_momentum_analysis(channel_monthly_data, channel_quarterly_data)\n",
    "    chunks.append(chunk5)\n",
    "    \n",
    "    # Create formatted output\n",
    "    formatted_output = []\n",
    "    \n",
    "    formatted_output.append(\"=== MONTHLY CHANNEL PERFORMANCE WITH ANOMALIES ===\")\n",
    "    formatted_output.append(chunks[0])\n",
    "    formatted_output.append(\"\\n=== QUARTERLY CHANNEL TRENDS ===\")\n",
    "    formatted_output.append(chunks[1])\n",
    "    formatted_output.append(\"\\n=== SPIKE AND DIP ANALYSIS ===\")\n",
    "    formatted_output.append(chunks[2])\n",
    "    formatted_output.append(\"\\n=== CHANNEL COMPARISON ===\")\n",
    "    formatted_output.append(chunks[3])\n",
    "    formatted_output.append(\"\\n=== MOMENTUM ANALYSIS ===\")\n",
    "    formatted_output.append(chunks[4])\n",
    "    \n",
    "    # Return both individual chunks and formatted output\n",
    "    return {\n",
    "        'chunks': chunks,\n",
    "        'formatted_output': '\\n'.join(formatted_output),\n",
    "        'summary_sections': {\n",
    "            'monthly_anomalies': chunks[0],\n",
    "            'quarterly_trends': chunks[1],\n",
    "            'spike_dip_analysis': chunks[2],\n",
    "            'channel_comparison': chunks[3],\n",
    "            'momentum_analysis': chunks[4]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def detect_spikes_and_dips(revenue_series, threshold_multiplier=1.5):\n",
    "    \"\"\"Detect spikes and dips in revenue series\"\"\"\n",
    "    if len(revenue_series) < 3:\n",
    "        return [], []\n",
    "    \n",
    "    # Calculate moving average and standard deviation\n",
    "    values = [v for v in revenue_series.values() if v > 0]\n",
    "    if len(values) < 2:\n",
    "        return [], []\n",
    "    \n",
    "    mean_revenue = statistics.mean(values)\n",
    "    std_revenue = statistics.stdev(values) if len(values) > 1 else 0\n",
    "    \n",
    "    spikes = []\n",
    "    dips = []\n",
    "    \n",
    "    for period, revenue in revenue_series.items():\n",
    "        if revenue > 0:\n",
    "            # Spike detection: revenue > mean + (threshold * std)\n",
    "            if revenue > mean_revenue + (threshold_multiplier * std_revenue):\n",
    "                spike_magnitude = ((revenue - mean_revenue) / mean_revenue) * 100\n",
    "                spikes.append({\n",
    "                    'period': period,\n",
    "                    'revenue': revenue,\n",
    "                    'magnitude': spike_magnitude\n",
    "                })\n",
    "            \n",
    "            # Dip detection: revenue < mean - (threshold * std) and significantly below mean\n",
    "            elif revenue < mean_revenue - (threshold_multiplier * std_revenue) and revenue < mean_revenue * 0.5:\n",
    "                dip_magnitude = ((mean_revenue - revenue) / mean_revenue) * 100\n",
    "                dips.append({\n",
    "                    'period': period,\n",
    "                    'revenue': revenue,\n",
    "                    'magnitude': dip_magnitude\n",
    "                })\n",
    "    \n",
    "    return spikes, dips\n",
    "\n",
    "def create_monthly_channel_summaries_with_anomalies(channel_monthly_data):\n",
    "    \"\"\"Create monthly summaries by channel with spike/dip detection\"\"\"\n",
    "    \n",
    "    summary = \"Monthly Channel Performance Analysis with Anomaly Detection:\\n\"\n",
    "    \n",
    "    for channel in sorted(channel_monthly_data.keys()):\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        monthly_data = channel_monthly_data[channel]\n",
    "        if not monthly_data:\n",
    "            continue\n",
    "        \n",
    "        # Get revenue series for spike/dip detection\n",
    "        revenue_series = {month: data['revenue'] for month, data in monthly_data.items()}\n",
    "        spikes, dips = detect_spikes_and_dips(revenue_series)\n",
    "        \n",
    "        # Calculate basic stats\n",
    "        revenues = [data['revenue'] for data in monthly_data.values() if data['revenue'] > 0]\n",
    "        if not revenues:\n",
    "            continue\n",
    "            \n",
    "        avg_revenue = statistics.mean(revenues)\n",
    "        max_revenue = max(revenues)\n",
    "        min_revenue = min(revenues)\n",
    "        \n",
    "        # Find peak and trough months\n",
    "        peak_month = max(monthly_data.items(), key=lambda x: x[1]['revenue'])\n",
    "        trough_month = min(monthly_data.items(), key=lambda x: x[1]['revenue'])\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "{channel.upper()} - Monthly Performance:\n",
    "- Average Monthly Revenue: ${avg_revenue:,.0f}\n",
    "- Peak Month: {peak_month[0]} (${peak_month[1]['revenue']:,.0f})\n",
    "- Lowest Month: {trough_month[0]} (${trough_month[1]['revenue']:,.0f})\n",
    "- Active Months: {len([r for r in revenues if r > 0])} out of {len(monthly_data)}\n",
    "\"\"\"\n",
    "        \n",
    "        # Add spike analysis\n",
    "        if spikes:\n",
    "            summary += f\"- Revenue Spikes Detected: {len(spikes)}\\n\"\n",
    "            for spike in sorted(spikes, key=lambda x: x['magnitude'], reverse=True)[:3]:\n",
    "                summary += f\"  • {spike['period']}: ${spike['revenue']:,.0f} (+{spike['magnitude']:.0f}% above average)\\n\"\n",
    "        \n",
    "        # Add dip analysis\n",
    "        if dips:\n",
    "            summary += f\"- Revenue Dips Detected: {len(dips)}\\n\"\n",
    "            for dip in sorted(dips, key=lambda x: x['magnitude'], reverse=True)[:3]:\n",
    "                summary += f\"  • {dip['period']}: ${dip['revenue']:,.0f} (-{dip['magnitude']:.0f}% below average)\\n\"\n",
    "        \n",
    "        # Month-over-month growth analysis\n",
    "        sorted_months = sorted(monthly_data.keys())\n",
    "        growth_periods = []\n",
    "        decline_periods = []\n",
    "        \n",
    "        for i in range(1, len(sorted_months)):\n",
    "            current_month = sorted_months[i]\n",
    "            prev_month = sorted_months[i-1]\n",
    "            \n",
    "            current_rev = monthly_data[current_month]['revenue']\n",
    "            prev_rev = monthly_data[prev_month]['revenue']\n",
    "            \n",
    "            if prev_rev > 0 and current_rev > 0:\n",
    "                growth_rate = ((current_rev - prev_rev) / prev_rev) * 100\n",
    "                if growth_rate > 50:  # Significant growth\n",
    "                    growth_periods.append(f\"{current_month} (+{growth_rate:.0f}%)\")\n",
    "                elif growth_rate < -50:  # Significant decline\n",
    "                    decline_periods.append(f\"{current_month} ({growth_rate:.0f}%)\")\n",
    "        \n",
    "        if growth_periods:\n",
    "            summary += f\"- High Growth Periods: {', '.join(growth_periods[:3])}\\n\"\n",
    "        if decline_periods:\n",
    "            summary += f\"- Decline Periods: {', '.join(decline_periods[:3])}\\n\"\n",
    "        \n",
    "        summary += \"\\n\"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "def create_quarterly_channel_summaries_with_trends(channel_quarterly_data):\n",
    "    \"\"\"Create quarterly summaries by channel with trend analysis\"\"\"\n",
    "    \n",
    "    summary = \"Quarterly Channel Performance Analysis with Trend Detection:\\n\"\n",
    "    \n",
    "    for channel in sorted(channel_quarterly_data.keys()):\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        quarterly_data = channel_quarterly_data[channel]\n",
    "        if not quarterly_data:\n",
    "            continue\n",
    "        \n",
    "        sorted_quarters = sorted(quarterly_data.keys())\n",
    "        if len(sorted_quarters) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Calculate quarterly trends\n",
    "        revenues = [quarterly_data[q]['revenue'] for q in sorted_quarters]\n",
    "        \n",
    "        # Trend analysis\n",
    "        trend_direction = \"stable\"\n",
    "        if len(revenues) >= 3:\n",
    "            recent_trend = revenues[-3:]\n",
    "            if all(recent_trend[i] < recent_trend[i+1] for i in range(len(recent_trend)-1)):\n",
    "                trend_direction = \"consistently growing\"\n",
    "            elif all(recent_trend[i] > recent_trend[i+1] for i in range(len(recent_trend)-1)):\n",
    "                trend_direction = \"consistently declining\"\n",
    "            elif revenues[-1] > revenues[0]:\n",
    "                trend_direction = \"overall growth\"\n",
    "            elif revenues[-1] < revenues[0]:\n",
    "                trend_direction = \"overall decline\"\n",
    "        \n",
    "        # Quarter-over-quarter growth rates\n",
    "        qoq_growth = []\n",
    "        for i in range(1, len(sorted_quarters)):\n",
    "            current_rev = quarterly_data[sorted_quarters[i]]['revenue']\n",
    "            prev_rev = quarterly_data[sorted_quarters[i-1]]['revenue']\n",
    "            \n",
    "            if prev_rev > 0:\n",
    "                growth_rate = ((current_rev - prev_rev) / prev_rev) * 100\n",
    "                qoq_growth.append((sorted_quarters[i], growth_rate))\n",
    "        \n",
    "        # Find best and worst quarters\n",
    "        best_quarter = max(quarterly_data.items(), key=lambda x: x[1]['revenue'])\n",
    "        worst_quarter = min(quarterly_data.items(), key=lambda x: x[1]['revenue'])\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "{channel.upper()} - Quarterly Analysis:\n",
    "- Trend Direction: {trend_direction.title()}\n",
    "- Best Quarter: {best_quarter[0]} (${best_quarter[1]['revenue']:,.0f})\n",
    "- Worst Quarter: {worst_quarter[0]} (${worst_quarter[1]['revenue']:,.0f})\n",
    "- Total Quarters Active: {len([r for r in revenues if r > 0])}\n",
    "\"\"\"\n",
    "        \n",
    "        # Add significant QoQ changes\n",
    "        significant_growth = [q for q, g in qoq_growth if g > 100]\n",
    "        significant_decline = [q for q, g in qoq_growth if g < -50]\n",
    "        \n",
    "        if significant_growth:\n",
    "            summary += f\"- Major Growth Quarters: {', '.join(significant_growth)}\\n\"\n",
    "        if significant_decline:\n",
    "            summary += f\"- Major Decline Quarters: {', '.join(significant_decline)}\\n\"\n",
    "        \n",
    "        # Recent performance (last 2 quarters)\n",
    "        if len(sorted_quarters) >= 2:\n",
    "            recent_quarters = sorted_quarters[-2:]\n",
    "            recent_avg = statistics.mean([quarterly_data[q]['revenue'] for q in recent_quarters])\n",
    "            summary += f\"- Recent Performance (Last 2Q): ${recent_avg:,.0f} average\\n\"\n",
    "        \n",
    "        summary += \"\\n\"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "def create_anomaly_analysis(channel_monthly_data):\n",
    "    \"\"\"Create comprehensive spike and dip analysis across all channels\"\"\"\n",
    "    \n",
    "    summary = \"Revenue Anomaly Analysis - Spikes and Dips Across All Channels:\\n\"\n",
    "    \n",
    "    all_spikes = []\n",
    "    all_dips = []\n",
    "    \n",
    "    # Collect all spikes and dips across channels\n",
    "    for channel, monthly_data in channel_monthly_data.items():\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        revenue_series = {month: data['revenue'] for month, data in monthly_data.items()}\n",
    "        spikes, dips = detect_spikes_and_dips(revenue_series)\n",
    "        \n",
    "        for spike in spikes:\n",
    "            spike['channel'] = channel\n",
    "            all_spikes.append(spike)\n",
    "            \n",
    "        for dip in dips:\n",
    "            dip['channel'] = channel\n",
    "            all_dips.append(dip)\n",
    "    \n",
    "    # Analyze biggest spikes\n",
    "    if all_spikes:\n",
    "        summary += f\"\\nTop Revenue Spikes (Highest Magnitude):\\n\"\n",
    "        top_spikes = sorted(all_spikes, key=lambda x: x['magnitude'], reverse=True)[:5]\n",
    "        for i, spike in enumerate(top_spikes, 1):\n",
    "            summary += f\"{i}. {spike['channel'].upper()} in {spike['period']}: ${spike['revenue']:,.0f} (+{spike['magnitude']:.0f}% above average)\\n\"\n",
    "    \n",
    "    # Analyze biggest dips\n",
    "    if all_dips:\n",
    "        summary += f\"\\nSignificant Revenue Dips:\\n\"\n",
    "        top_dips = sorted(all_dips, key=lambda x: x['magnitude'], reverse=True)[:5]\n",
    "        for i, dip in enumerate(top_dips, 1):\n",
    "            summary += f\"{i}. {dip['channel'].upper()} in {dip['period']}: ${dip['revenue']:,.0f} (-{dip['magnitude']:.0f}% below average)\\n\"\n",
    "    \n",
    "    # Seasonal spike analysis\n",
    "    spike_months = defaultdict(int)\n",
    "    dip_months = defaultdict(int)\n",
    "    \n",
    "    for spike in all_spikes:\n",
    "        try:\n",
    "            month_num = int(spike['period'].split('-')[1])\n",
    "            spike_months[month_num] += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for dip in all_dips:\n",
    "        try:\n",
    "            month_num = int(dip['period'].split('-')[1])\n",
    "            dip_months[month_num] += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if spike_months:\n",
    "        peak_spike_month = max(spike_months.items(), key=lambda x: x[1])\n",
    "        month_name = calendar.month_name[peak_spike_month[0]]\n",
    "        summary += f\"\\nSeasonal Patterns:\\n\"\n",
    "        summary += f\"- Most Common Spike Month: {month_name} ({peak_spike_month[1]} occurrences)\\n\"\n",
    "    \n",
    "    if dip_months:\n",
    "        peak_dip_month = max(dip_months.items(), key=lambda x: x[1])\n",
    "        month_name = calendar.month_name[peak_dip_month[0]]\n",
    "        summary += f\"- Most Common Dip Month: {month_name} ({peak_dip_month[1]} occurrences)\\n\"\n",
    "    \n",
    "    # Channel volatility analysis\n",
    "    channel_volatility = {}\n",
    "    for channel, monthly_data in channel_monthly_data.items():\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        revenues = [data['revenue'] for data in monthly_data.values() if data['revenue'] > 0]\n",
    "        if len(revenues) > 1:\n",
    "            avg_rev = statistics.mean(revenues)\n",
    "            std_rev = statistics.stdev(revenues)\n",
    "            volatility = (std_rev / avg_rev) * 100 if avg_rev > 0 else 0\n",
    "            channel_volatility[channel] = volatility\n",
    "    \n",
    "    if channel_volatility:\n",
    "        summary += f\"\\nChannel Volatility Analysis (Revenue Consistency):\\n\"\n",
    "        sorted_volatility = sorted(channel_volatility.items(), key=lambda x: x[1])\n",
    "        \n",
    "        most_stable = sorted_volatility[0]\n",
    "        most_volatile = sorted_volatility[-1]\n",
    "        \n",
    "        summary += f\"- Most Stable Channel: {most_stable[0].upper()} ({most_stable[1]:.0f}% volatility)\\n\"\n",
    "        summary += f\"- Most Volatile Channel: {most_volatile[0].upper()} ({most_volatile[1]:.0f}% volatility)\\n\"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "def create_channel_comparison_analysis(channel_monthly_data, channel_quarterly_data):\n",
    "    \"\"\"Create channel performance comparison and rankings\"\"\"\n",
    "    \n",
    "    summary = \"Channel Performance Comparison and Rankings:\\n\"\n",
    "    \n",
    "    # Calculate total revenue by channel\n",
    "    channel_totals = {}\n",
    "    channel_consistency = {}\n",
    "    channel_peak_performance = {}\n",
    "    \n",
    "    for channel, monthly_data in channel_monthly_data.items():\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        revenues = [data['revenue'] for data in monthly_data.values()]\n",
    "        active_revenues = [r for r in revenues if r > 0]\n",
    "        \n",
    "        if active_revenues:\n",
    "            channel_totals[channel] = sum(active_revenues)\n",
    "            channel_consistency[channel] = len(active_revenues) / len(monthly_data) * 100\n",
    "            channel_peak_performance[channel] = max(active_revenues)\n",
    "    \n",
    "    # Rankings\n",
    "    if channel_totals:\n",
    "        summary += f\"\\nTotal Revenue Rankings:\\n\"\n",
    "        revenue_ranking = sorted(channel_totals.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (channel, total_rev) in enumerate(revenue_ranking, 1):\n",
    "            summary += f\"{i}. {channel.upper()}: ${total_rev:,.0f} cumulative revenue\\n\"\n",
    "    \n",
    "    if channel_consistency:\n",
    "        summary += f\"\\nConsistency Rankings (% of months active):\\n\"\n",
    "        consistency_ranking = sorted(channel_consistency.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (channel, consistency) in enumerate(consistency_ranking, 1):\n",
    "            summary += f\"{i}. {channel.upper()}: {consistency:.0f}% of months with revenue\\n\"\n",
    "    \n",
    "    if channel_peak_performance:\n",
    "        summary += f\"\\nPeak Performance Rankings (Highest single month):\\n\"\n",
    "        peak_ranking = sorted(channel_peak_performance.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (channel, peak_rev) in enumerate(peak_ranking, 1):\n",
    "            summary += f\"{i}. {channel.upper()}: ${peak_rev:,.0f} peak monthly revenue\\n\"\n",
    "    \n",
    "    # Channel lifecycle analysis\n",
    "    summary += f\"\\nChannel Lifecycle Analysis:\\n\"\n",
    "    \n",
    "    for channel, monthly_data in channel_monthly_data.items():\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        sorted_months = sorted(monthly_data.keys())\n",
    "        active_months = [month for month in sorted_months if monthly_data[month]['revenue'] > 0]\n",
    "        \n",
    "        if active_months:\n",
    "            launch_month = active_months[0]\n",
    "            latest_month = active_months[-1]\n",
    "            \n",
    "            # Determine lifecycle stage\n",
    "            recent_months = sorted_months[-3:] if len(sorted_months) >= 3 else sorted_months\n",
    "            recent_activity = sum(1 for month in recent_months if monthly_data[month]['revenue'] > 0)\n",
    "            \n",
    "            if recent_activity == 0:\n",
    "                stage = \"Dormant\"\n",
    "            elif len(active_months) <= 3:\n",
    "                stage = \"Launch Phase\"\n",
    "            elif recent_activity == len(recent_months):\n",
    "                stage = \"Active/Mature\"\n",
    "            else:\n",
    "                stage = \"Intermittent\"\n",
    "            \n",
    "            summary += f\"- {channel.upper()}: {stage} (Active: {launch_month} to {latest_month})\\n\"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "def create_momentum_analysis(channel_monthly_data, channel_quarterly_data):\n",
    "    \"\"\"Create growth momentum and trend analysis\"\"\"\n",
    "    \n",
    "    summary = \"Channel Growth Momentum and Trend Analysis:\\n\"\n",
    "    \n",
    "    for channel, monthly_data in channel_monthly_data.items():\n",
    "        if channel == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        sorted_months = sorted(monthly_data.keys())\n",
    "        if len(sorted_months) < 6:  # Need at least 6 months for momentum analysis\n",
    "            continue\n",
    "        \n",
    "        # Get last 6 months of data\n",
    "        recent_months = sorted_months[-6:]\n",
    "        recent_revenues = [monthly_data[month]['revenue'] for month in recent_months]\n",
    "        \n",
    "        # Calculate momentum indicators\n",
    "        first_half_avg = statistics.mean(recent_revenues[:3])\n",
    "        second_half_avg = statistics.mean(recent_revenues[3:])\n",
    "        \n",
    "        momentum = \"neutral\"\n",
    "        momentum_pct = 0\n",
    "        \n",
    "        if first_half_avg > 0:\n",
    "            momentum_pct = ((second_half_avg - first_half_avg) / first_half_avg) * 100\n",
    "            \n",
    "            if momentum_pct > 20:\n",
    "                momentum = \"strong positive\"\n",
    "            elif momentum_pct > 5:\n",
    "                momentum = \"positive\"\n",
    "            elif momentum_pct < -20:\n",
    "                momentum = \"strong negative\"\n",
    "            elif momentum_pct < -5:\n",
    "                momentum = \"negative\"\n",
    "        \n",
    "        # Trend consistency\n",
    "        positive_months = sum(1 for i in range(1, len(recent_revenues)) \n",
    "                             if recent_revenues[i] > recent_revenues[i-1])\n",
    "        trend_consistency = (positive_months / (len(recent_revenues) - 1)) * 100\n",
    "        \n",
    "        summary += f\"\"\"\n",
    "{channel.upper()} - 6-Month Momentum Analysis:\n",
    "- Growth Momentum: {momentum.title()} ({momentum_pct:+.0f}%)\n",
    "- Trend Consistency: {trend_consistency:.0f}% of months showed growth\n",
    "- Recent Average (Last 3 months): ${second_half_avg:,.0f}\n",
    "- Previous Average (3 months prior): ${first_half_avg:,.0f}\n",
    "\"\"\"\n",
    "        \n",
    "    \n",
    "        \n",
    "        summary += \"\\n\"\n",
    "    \n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MONTHLY CHANNEL PERFORMANCE WITH ANOMALIES ===\n",
      "Monthly Channel Performance Analysis with Anomaly Detection:\n",
      "\n",
      "DV_360_X1 - Monthly Performance:\n",
      "- Average Monthly Revenue: $161,021\n",
      "- Peak Month: 2024-08 ($286,080)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 10 out of 30\n",
      "- Decline Periods: 2025-01 (-55%), 2025-02 (-80%)\n",
      "\n",
      "\n",
      "GOOGLE - Monthly Performance:\n",
      "- Average Monthly Revenue: $35,335\n",
      "- Peak Month: 2023-10 ($71,109)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 16 out of 30\n",
      "- High Growth Periods: 2024-05 (+1598%), 2024-10 (+220%)\n",
      "- Decline Periods: 2024-01 (-74%), 2024-02 (-82%), 2024-09 (-77%)\n",
      "\n",
      "\n",
      "META - Monthly Performance:\n",
      "- Average Monthly Revenue: $20,639\n",
      "- Peak Month: 2024-05 ($35,488)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 14 out of 30\n",
      "- High Growth Periods: 2024-05 (+79%)\n",
      "- Decline Periods: 2025-01 (-58%), 2025-02 (-63%)\n",
      "\n",
      "\n",
      "TIKTOK - Monthly Performance:\n",
      "- Average Monthly Revenue: $3,254\n",
      "- Peak Month: 2024-12 ($7,722)\n",
      "- Lowest Month: 2022-12 ($0)\n",
      "- Active Months: 4 out of 30\n",
      "- High Growth Periods: 2024-12 (+208%)\n",
      "- Decline Periods: 2025-01 (-70%), 2025-02 (-80%)\n",
      "\n",
      "=== QUARTERLY CHANNEL TRENDS ===\n",
      "Quarterly Channel Performance Analysis with Trend Detection:\n",
      "\n",
      "DV_360_X1 - Quarterly Analysis:\n",
      "- Trend Direction: Stable\n",
      "- Best Quarter: 2024-Q3 ($276,999)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 2\n",
      "- Major Decline Quarters: 2025-Q1\n",
      "- Recent Performance (Last 2Q): $0 average\n",
      "\n",
      "\n",
      "GOOGLE - Quarterly Analysis:\n",
      "- Trend Direction: Stable\n",
      "- Best Quarter: 2023-Q4 ($66,506)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 4\n",
      "- Major Growth Quarters: 2024-Q4\n",
      "- Major Decline Quarters: 2024-Q1, 2024-Q3, 2025-Q1\n",
      "- Recent Performance (Last 2Q): $0 average\n",
      "\n",
      "\n",
      "META - Quarterly Analysis:\n",
      "- Trend Direction: Consistently Declining\n",
      "- Best Quarter: 2024-Q2 ($34,550)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 5\n",
      "- Major Decline Quarters: 2025-Q1\n",
      "- Recent Performance (Last 2Q): $5,039 average\n",
      "\n",
      "\n",
      "TIKTOK - Quarterly Analysis:\n",
      "- Trend Direction: Stable\n",
      "- Best Quarter: 2024-Q4 ($7,722)\n",
      "- Worst Quarter: 2022-Q4 ($0)\n",
      "- Total Quarters Active: 1\n",
      "- Major Decline Quarters: 2025-Q1\n",
      "- Recent Performance (Last 2Q): $0 average\n",
      "\n",
      "=== SPIKE AND DIP ANALYSIS ===\n",
      "Revenue Anomaly Analysis - Spikes and Dips Across All Channels:\n",
      "\n",
      "Channel Volatility Analysis (Revenue Consistency):\n",
      "- Most Stable Channel: META (58% volatility)\n",
      "- Most Volatile Channel: TIKTOK (96% volatility)\n",
      "\n",
      "=== CHANNEL COMPARISON ===\n",
      "Channel Performance Comparison and Rankings:\n",
      "\n",
      "Total Revenue Rankings:\n",
      "1. DV_360_X1: $1,610,209 cumulative revenue\n",
      "2. GOOGLE: $565,360 cumulative revenue\n",
      "3. META: $288,948 cumulative revenue\n",
      "4. TIKTOK: $13,017 cumulative revenue\n",
      "\n",
      "Consistency Rankings (% of months active):\n",
      "1. GOOGLE: 53% of months with revenue\n",
      "2. META: 47% of months with revenue\n",
      "3. DV_360_X1: 33% of months with revenue\n",
      "4. TIKTOK: 13% of months with revenue\n",
      "\n",
      "Peak Performance Rankings (Highest single month):\n",
      "1. DV_360_X1: $286,080 peak monthly revenue\n",
      "2. GOOGLE: $71,109 peak monthly revenue\n",
      "3. META: $35,488 peak monthly revenue\n",
      "4. TIKTOK: $7,722 peak monthly revenue\n",
      "\n",
      "Channel Lifecycle Analysis:\n",
      "- DV_360_X1: Dormant (Active: 2024-01 to 2025-02)\n",
      "- GOOGLE: Dormant (Active: 2023-10 to 2025-02)\n",
      "- META: Active/Mature (Active: 2024-04 to 2025-05)\n",
      "- TIKTOK: Dormant (Active: 2024-11 to 2025-02)\n",
      "\n",
      "=== MOMENTUM ANALYSIS ===\n",
      "Channel Growth Momentum and Trend Analysis:\n",
      "\n",
      "DV_360_X1 - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-100%)\n",
      "- Trend Consistency: 0% of months showed growth\n",
      "- Recent Average (Last 3 months): $0\n",
      "- Previous Average (3 months prior): $108,797\n",
      "\n",
      "\n",
      "GOOGLE - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-100%)\n",
      "- Trend Consistency: 0% of months showed growth\n",
      "- Recent Average (Last 3 months): $0\n",
      "- Previous Average (3 months prior): $18,947\n",
      "\n",
      "\n",
      "META - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-67%)\n",
      "- Trend Consistency: 20% of months showed growth\n",
      "- Recent Average (Last 3 months): $4,841\n",
      "- Previous Average (3 months prior): $14,516\n",
      "\n",
      "\n",
      "TIKTOK - 6-Month Momentum Analysis:\n",
      "- Growth Momentum: Strong Negative (-100%)\n",
      "- Trend Consistency: 0% of months showed growth\n",
      "- Recent Average (Last 3 months): $0\n",
      "- Previous Average (3 months prior): $3,504\n"
     ]
    }
   ],
   "source": [
    "result = get_channel_time_insights_with_anomalies(soup)\n",
    "\n",
    "print(result['formatted_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Return on investment\n",
      "\n",
      "Insights: Your return on investment (ROI) helps\n",
      "you understand how your marketing activities impacted your business objectives.\n",
      "Meta drove the highest ROI at 5.1. For every $1\n",
      "you spent on Meta, you saw $5.11 in revenue.\n",
      "Dv_360_X1 had the highest effectiveness, which is your\n",
      "incremental outcome per media unit. Meta had the highest marginal\n",
      "ROI at 2.08. Meta drove the lowest CPIK\n",
      "at $0.20. For every KPI unit, you spent $0.20.\n"
     ]
    }
   ],
   "source": [
    "def get_roi_insights(soup):\n",
    "    \"\"\"Extract ROI title and insights text\"\"\"\n",
    "    \n",
    "    # Find the ROI card\n",
    "    roi_card = soup.find(\"card\", {\"id\": \"performance-breakdown\"})\n",
    "    if not roi_card:\n",
    "        return \"No ROI performance data found.\"\n",
    "    \n",
    "    # Extract card title\n",
    "    card_title = roi_card.find(\"card-title\")\n",
    "    card_title_content = card_title.get_text(strip=True) if card_title else None\n",
    "    \n",
    "    # Extract insights text\n",
    "    insights_text = roi_card.find(\"p\", {\"class\": \"insights-text\"})\n",
    "    insights_text_content = insights_text.get_text(strip=True) if insights_text else None\n",
    "    \n",
    "    if not card_title_content and not insights_text_content:\n",
    "        return \"No ROI data found.\"\n",
    "    \n",
    "    # Format output\n",
    "    output = \"\"\n",
    "    if card_title_content:\n",
    "        output += f\"Title: {card_title_content}\\n\\n\"\n",
    "    \n",
    "    if insights_text_content:\n",
    "        output += f\"Insights: {insights_text_content}\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(get_roi_insights(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI vs Effectiveness Analysis:\n",
      "\n",
      "Performance Overview:\n",
      "- Average ROI across channels: 4.8x\n",
      "- Average effectiveness: 0.0169 incremental outcome per impression\n",
      "- Total media spend analyzed: $2,340,830\n",
      "\n",
      "ROI Rankings:\n",
      "1. META: 5.1x ROI\n",
      "2. TIKTOK: 5.0x ROI\n",
      "3. GOOGLE: 4.6x ROI\n",
      "4. DV_360_X1: 4.6x ROI\n",
      "\n",
      "Effectiveness Rankings:\n",
      "1. DV_360_X1: 0.0491 outcome per impression\n",
      "2. GOOGLE: 0.0065 outcome per impression\n",
      "3. META: 0.0062 outcome per impression\n",
      "4. TIKTOK: 0.0057 outcome per impression\n",
      "\n",
      "Spend Allocation:\n",
      "1. DV_360_X1: $1,558,208 (66.6% of total spend)\n",
      "2. GOOGLE: $526,693 (22.5% of total spend)\n",
      "3. META: $243,623 (10.4% of total spend)\n",
      "4. TIKTOK: $12,306 (0.5% of total spend)\n",
      "\n",
      "Channel Performance Categories:\n",
      "\n",
      "High Potential:\n",
      "- DV_360_X1 (4.6x ROI, 0.0491 effectiveness)\n",
      "\n",
      "Optimization Needed:\n",
      "- GOOGLE (4.6x ROI, 0.0065 effectiveness)\n",
      "\n",
      "Cost Efficient:\n",
      "- META (5.1x ROI, 0.0062 effectiveness)\n",
      "- TIKTOK (5.0x ROI, 0.0057 effectiveness)\n",
      "\n",
      "Strategic Insights:\n",
      "- META delivers highest ROI (5.1x) - prioritize for budget allocation\n",
      "- DV_360_X1 shows highest effectiveness (0.0491) - strong media performance per impression\n",
      "- DV_360_X1 receives largest budget ($1,558,208) - monitor efficiency closely\n",
      "- DV_360_X1: High effectiveness but expensive - optimize costs to improve ROI\n",
      "- GOOGLE: Both ROI and effectiveness below average - requires optimization or budget reallocation\n",
      "- META: Cost efficient but low reach - consider scaling if effectiveness can be maintained\n",
      "- TIKTOK: Cost efficient but low reach - consider scaling if effectiveness can be maintained\n",
      "\n",
      "Methodology: Note: Effectiveness measures the\n",
      "incremental outcome generated per impression. A low ROI does not necessarily\n",
      "imply low media effectiveness; it may result from high media cost, as positioned\n",
      "in the upper-left corner of the chart. Conversely, a high ROI can coexist with\n",
      "low media effectiveness and low media costs, as indicated in the bottom-right\n",
      "corner of the chart. The diagonal section of the chart suggests that the ROI is\n",
      "primarily influenced by media effectiveness. The size of the bubbles represents\n",
      "the scale of the media spend.\n"
     ]
    }
   ],
   "source": [
    "def get_roi_effectiveness_insights(soup):\n",
    "    \"\"\"Extract ROI vs Effectiveness chart insights\"\"\"\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    # Find the ROI effectiveness chart\n",
    "    chart_embed = soup.find(\"chart-embed\", {\"id\": \"roi-effectiveness-chart\"})\n",
    "    if not chart_embed:\n",
    "        return \"No ROI effectiveness chart found.\"\n",
    "    \n",
    "    # Find the parent chart element\n",
    "    chart_element = chart_embed.find_parent(\"chart\")\n",
    "    if not chart_element:\n",
    "        return \"No ROI effectiveness chart element found.\"\n",
    "    \n",
    "    # Extract chart description\n",
    "    chart_description = chart_element.find(\"chart-description\")\n",
    "    chart_description_content = chart_description.get_text(strip=True) if chart_description else None\n",
    "    \n",
    "    # Extract chart data from script\n",
    "    script_tag = chart_element.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "    chart_data = []\n",
    "    \n",
    "    if script_tag:\n",
    "        script_content = script_tag.get_text()\n",
    "        json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                escaped_json = json_match.group(1)\n",
    "                unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "                chart_spec = json.loads(unescaped_json)\n",
    "                \n",
    "                datasets = chart_spec.get('datasets', {})\n",
    "                for dataset_key, dataset_value in datasets.items():\n",
    "                    if isinstance(dataset_value, list):\n",
    "                        chart_data = dataset_value\n",
    "                        break\n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                pass\n",
    "    \n",
    "    if not chart_data:\n",
    "        return \"No ROI effectiveness data could be extracted.\"\n",
    "    \n",
    "    # Analyze the data\n",
    "    analysis = analyze_roi_effectiveness(chart_data, chart_description_content)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def analyze_roi_effectiveness(chart_data, chart_description):\n",
    "    \"\"\"Analyze ROI vs Effectiveness data\"\"\"\n",
    "    \n",
    "    # Sort channels by different metrics\n",
    "    by_roi = sorted(chart_data, key=lambda x: x['roi'], reverse=True)\n",
    "    by_effectiveness = sorted(chart_data, key=lambda x: x['effectiveness'], reverse=True)\n",
    "    by_spend = sorted(chart_data, key=lambda x: x['spend'], reverse=True)\n",
    "    \n",
    "    # Calculate totals and averages\n",
    "    total_spend = sum(ch['spend'] for ch in chart_data)\n",
    "    avg_roi = sum(ch['roi'] for ch in chart_data) / len(chart_data)\n",
    "    avg_effectiveness = sum(ch['effectiveness'] for ch in chart_data) / len(chart_data)\n",
    "    \n",
    "    # Categorize channels based on ROI and effectiveness\n",
    "    high_roi_threshold = avg_roi\n",
    "    high_effectiveness_threshold = avg_effectiveness\n",
    "    \n",
    "    channel_categories = {}\n",
    "    for channel in chart_data:\n",
    "        roi_level = \"High\" if channel['roi'] >= high_roi_threshold else \"Low\"\n",
    "        eff_level = \"High\" if channel['effectiveness'] >= high_effectiveness_threshold else \"Low\"\n",
    "        \n",
    "        if roi_level == \"High\" and eff_level == \"High\":\n",
    "            category = \"Star Performers\"\n",
    "        elif roi_level == \"High\" and eff_level == \"Low\":\n",
    "            category = \"Cost Efficient\"\n",
    "        elif roi_level == \"Low\" and eff_level == \"High\":\n",
    "            category = \"High Potential\"\n",
    "        else:\n",
    "            category = \"Optimization Needed\"\n",
    "        \n",
    "        channel_categories[channel['channel']] = {\n",
    "            'category': category,\n",
    "            'roi': channel['roi'],\n",
    "            'effectiveness': channel['effectiveness'],\n",
    "            'spend': channel['spend'],\n",
    "            'spend_share': (channel['spend'] / total_spend) * 100\n",
    "        }\n",
    "    \n",
    "    # Build analysis\n",
    "    analysis = f\"\"\"\n",
    "ROI vs Effectiveness Analysis:\n",
    "\n",
    "Performance Overview:\n",
    "- Average ROI across channels: {avg_roi:.1f}x\n",
    "- Average effectiveness: {avg_effectiveness:.4f} incremental outcome per impression\n",
    "- Total media spend analyzed: ${total_spend:,.0f}\n",
    "\n",
    "ROI Rankings:\n",
    "\"\"\".strip()\n",
    "    \n",
    "    for i, channel in enumerate(by_roi, 1):\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: {channel['roi']:.1f}x ROI\"\n",
    "    \n",
    "    analysis += f\"\\n\\nEffectiveness Rankings:\"\n",
    "    for i, channel in enumerate(by_effectiveness, 1):\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: {channel['effectiveness']:.4f} outcome per impression\"\n",
    "    \n",
    "    analysis += f\"\\n\\nSpend Allocation:\"\n",
    "    for i, channel in enumerate(by_spend, 1):\n",
    "        spend_pct = (channel['spend'] / total_spend) * 100\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: ${channel['spend']:,.0f} ({spend_pct:.1f}% of total spend)\"\n",
    "    \n",
    "    # Channel categorization\n",
    "    analysis += f\"\\n\\nChannel Performance Categories:\"\n",
    "    \n",
    "    categories = {}\n",
    "    for channel, data in channel_categories.items():\n",
    "        category = data['category']\n",
    "        if category not in categories:\n",
    "            categories[category] = []\n",
    "        categories[category].append(f\"{channel.upper()} ({data['roi']:.1f}x ROI, {data['effectiveness']:.4f} effectiveness)\")\n",
    "    \n",
    "    for category, channels in categories.items():\n",
    "        analysis += f\"\\n\\n{category}:\"\n",
    "        for channel_info in channels:\n",
    "            analysis += f\"\\n- {channel_info}\"\n",
    "    \n",
    "    # Strategic insights\n",
    "    analysis += f\"\\n\\nStrategic Insights:\"\n",
    "    \n",
    "    # Find best performers\n",
    "    highest_roi = by_roi[0]\n",
    "    highest_effectiveness = by_effectiveness[0]\n",
    "    largest_spend = by_spend[0]\n",
    "    \n",
    "    analysis += f\"\\n- {highest_roi['channel'].upper()} delivers highest ROI ({highest_roi['roi']:.1f}x) - prioritize for budget allocation\"\n",
    "    analysis += f\"\\n- {highest_effectiveness['channel'].upper()} shows highest effectiveness ({highest_effectiveness['effectiveness']:.4f}) - strong media performance per impression\"\n",
    "    analysis += f\"\\n- {largest_spend['channel'].upper()} receives largest budget (${largest_spend['spend']:,.0f}) - monitor efficiency closely\"\n",
    "    \n",
    "    # Efficiency vs spend analysis\n",
    "    for channel, data in channel_categories.items():\n",
    "        if data['category'] == \"Star Performers\":\n",
    "            analysis += f\"\\n- {channel.upper()}: Ideal performance - high ROI and effectiveness, maintain investment\"\n",
    "        elif data['category'] == \"Cost Efficient\":\n",
    "            analysis += f\"\\n- {channel.upper()}: Cost efficient but low reach - consider scaling if effectiveness can be maintained\"\n",
    "        elif data['category'] == \"High Potential\":\n",
    "            analysis += f\"\\n- {channel.upper()}: High effectiveness but expensive - optimize costs to improve ROI\"\n",
    "        elif data['category'] == \"Optimization Needed\":\n",
    "            analysis += f\"\\n- {channel.upper()}: Both ROI and effectiveness below average - requires optimization or budget reallocation\"\n",
    "    \n",
    "    if chart_description:\n",
    "        analysis += f\"\\n\\nMethodology: {chart_description}\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "print(get_roi_effectiveness_insights(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI vs Marginal ROI Performance Analysis:\n",
      "\n",
      "Performance Metrics:\n",
      "- Average ROI across channels: 4.8x\n",
      "- Average Marginal ROI: 2.0x (additional return per additional dollar)\n",
      "- Total media spend analyzed: $2,340,830\n",
      "\n",
      "ROI Rankings:\n",
      "1. META: 5.1x ROI\n",
      "2. TIKTOK: 5.0x ROI\n",
      "3. GOOGLE: 4.6x ROI\n",
      "4. DV_360_X1: 4.6x ROI\n",
      "\n",
      "Marginal ROI Rankings (Incremental Efficiency):\n",
      "1. META: 2.1x marginal ROI\n",
      "2. TIKTOK: 2.0x marginal ROI\n",
      "3. GOOGLE: 1.9x marginal ROI\n",
      "4. DV_360_X1: 1.8x marginal ROI\n",
      "\n",
      "Spend Distribution:\n",
      "1. DV_360_X1: $1,558,208 (66.6% of total spend)\n",
      "2. GOOGLE: $526,693 (22.5% of total spend)\n",
      "3. META: $243,623 (10.4% of total spend)\n",
      "4. TIKTOK: $12,306 (0.5% of total spend)\n",
      "\n",
      "Saturation Indicators:\n",
      "- DV_360_X1: High saturation signal (ROI/Marginal ROI ratio: 2.6x)\n",
      "- GOOGLE: Moderate saturation signal (ROI/Marginal ROI ratio: 2.4x)\n",
      "- META: Moderate saturation signal (ROI/Marginal ROI ratio: 2.5x)\n",
      "- TIKTOK: Moderate saturation signal (ROI/Marginal ROI ratio: 2.5x)\n",
      "\n",
      "Efficiency Gap Analysis:\n",
      "- DV_360_X1: 2.9x gap between current and marginal returns\n",
      "- GOOGLE: 2.7x gap between current and marginal returns\n",
      "- META: 3.0x gap between current and marginal returns\n",
      "- TIKTOK: 3.0x gap between current and marginal returns\n",
      "\n",
      "Performance Consistency:\n",
      "- ROI variation across channels: 0.21 standard deviation\n",
      "- Marginal ROI variation: 0.12 standard deviation\n",
      "- ROI and marginal ROI show similar variation patterns\n",
      "\n",
      "Channel Performance Patterns:\n",
      "- META leads in both ROI and marginal ROI - consistent high performer\n",
      "- DV_360_X1: High spend concentration (66.6%) with below-average marginal efficiency\n",
      "- META: Low spend allocation (10.4%) but above-average marginal efficiency\n",
      "- TIKTOK: Low spend allocation (0.5%) but above-average marginal efficiency\n",
      "\n",
      "Diminishing Returns Assessment:\n",
      "- DV_360_X1: Moderate diminishing returns (current ROI 2.6x higher than marginal)\n",
      "- GOOGLE: Moderate diminishing returns (current ROI 2.4x higher than marginal)\n",
      "- META: Moderate diminishing returns (current ROI 2.5x higher than marginal)\n",
      "- TIKTOK: Moderate diminishing returns (current ROI 2.5x higher than marginal)\n",
      "\n",
      "Methodology: Note: Marginal ROI measures the additional\n",
      "return generated for every additional dollar spent. It's an indicator of\n",
      "efficiency of additional spend. Channels with a high ROI but a low marginal ROI\n",
      "are likely in the saturation phase, where the initial investments have paid off,\n",
      "but additional investment does not bring in as much return. Conversely, channels\n",
      "that have a high ROI and a high marginal ROI perform well and continue to yield\n",
      "high returns with additional spending. The size of the bubbles represents the\n",
      "scale of the media spend.\n"
     ]
    }
   ],
   "source": [
    "def get_roi_marginal_insights(soup):\n",
    "    \"\"\"Extract ROI vs Marginal ROI chart insights - focused on performance analysis\"\"\"\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    # Find the ROI marginal chart\n",
    "    chart_embed = soup.find(\"chart-embed\", {\"id\": \"roi-marginal-chart\"})\n",
    "    if not chart_embed:\n",
    "        return \"No ROI marginal chart found.\"\n",
    "    \n",
    "    # Find the parent chart element\n",
    "    chart_element = chart_embed.find_parent(\"chart\")\n",
    "    if not chart_element:\n",
    "        return \"No ROI marginal chart element found.\"\n",
    "    \n",
    "    # Extract chart description\n",
    "    chart_description = chart_element.find(\"chart-description\")\n",
    "    chart_description_content = chart_description.get_text(strip=True) if chart_description else None\n",
    "    \n",
    "    # Extract chart data from script\n",
    "    script_tag = chart_element.find_next(\"script\", {\"type\": \"text/javascript\"})\n",
    "    chart_data = []\n",
    "    \n",
    "    if script_tag:\n",
    "        script_content = script_tag.get_text()\n",
    "        json_match = re.search(r'JSON\\.parse\\(\"(.+?)\"\\)', script_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                escaped_json = json_match.group(1)\n",
    "                unescaped_json = escaped_json.replace('\\\\\"', '\"').replace('\\\\n', '').replace('\\\\\\\\', '\\\\')\n",
    "                chart_spec = json.loads(unescaped_json)\n",
    "                \n",
    "                datasets = chart_spec.get('datasets', {})\n",
    "                for dataset_key, dataset_value in datasets.items():\n",
    "                    if isinstance(dataset_value, list):\n",
    "                        chart_data = dataset_value\n",
    "                        break\n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                pass\n",
    "    \n",
    "    if not chart_data:\n",
    "        return \"No ROI marginal data could be extracted.\"\n",
    "    \n",
    "    # Analyze the data\n",
    "    analysis = analyze_roi_marginal_performance(chart_data, chart_description_content)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def analyze_roi_marginal_performance(chart_data, chart_description):\n",
    "    \"\"\"Analyze ROI vs Marginal ROI performance insights only\"\"\"\n",
    "    \n",
    "    # Sort channels by different metrics\n",
    "    by_roi = sorted(chart_data, key=lambda x: x['roi'], reverse=True)\n",
    "    by_marginal_roi = sorted(chart_data, key=lambda x: x['mroi'], reverse=True)\n",
    "    by_spend = sorted(chart_data, key=lambda x: x['spend'], reverse=True)\n",
    "    \n",
    "    # Calculate totals and averages\n",
    "    total_spend = sum(ch['spend'] for ch in chart_data)\n",
    "    avg_roi = sum(ch['roi'] for ch in chart_data) / len(chart_data)\n",
    "    avg_marginal_roi = sum(ch['mroi'] for ch in chart_data) / len(chart_data)\n",
    "    \n",
    "    # Build analysis\n",
    "    analysis = f\"\"\"\n",
    "ROI vs Marginal ROI Performance Analysis:\n",
    "\n",
    "Performance Metrics:\n",
    "- Average ROI across channels: {avg_roi:.1f}x\n",
    "- Average Marginal ROI: {avg_marginal_roi:.1f}x (additional return per additional dollar)\n",
    "- Total media spend analyzed: ${total_spend:,.0f}\n",
    "\n",
    "ROI Rankings:\n",
    "\"\"\".strip()\n",
    "    \n",
    "    for i, channel in enumerate(by_roi, 1):\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: {channel['roi']:.1f}x ROI\"\n",
    "    \n",
    "    analysis += f\"\\n\\nMarginal ROI Rankings (Incremental Efficiency):\"\n",
    "    for i, channel in enumerate(by_marginal_roi, 1):\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: {channel['mroi']:.1f}x marginal ROI\"\n",
    "    \n",
    "    analysis += f\"\\n\\nSpend Distribution:\"\n",
    "    for i, channel in enumerate(by_spend, 1):\n",
    "        spend_pct = (channel['spend'] / total_spend) * 100\n",
    "        analysis += f\"\\n{i}. {channel['channel'].upper()}: ${channel['spend']:,.0f} ({spend_pct:.1f}% of total spend)\"\n",
    "    \n",
    "    # Saturation analysis (key insight for MMM)\n",
    "    analysis += f\"\\n\\nSaturation Indicators:\"\n",
    "    for channel in chart_data:\n",
    "        roi_marginal_ratio = channel['roi'] / channel['mroi']\n",
    "        saturation_level = \"High\" if roi_marginal_ratio > 2.5 else \"Moderate\" if roi_marginal_ratio > 2.0 else \"Low\"\n",
    "        analysis += f\"\\n- {channel['channel'].upper()}: {saturation_level} saturation signal (ROI/Marginal ROI ratio: {roi_marginal_ratio:.1f}x)\"\n",
    "    \n",
    "    # Efficiency gap analysis\n",
    "    analysis += f\"\\n\\nEfficiency Gap Analysis:\"\n",
    "    for channel in chart_data:\n",
    "        efficiency_gap = channel['roi'] - channel['mroi']\n",
    "        analysis += f\"\\n- {channel['channel'].upper()}: {efficiency_gap:.1f}x gap between current and marginal returns\"\n",
    "    \n",
    "    # Performance consistency analysis\n",
    "    analysis += f\"\\n\\nPerformance Consistency:\"\n",
    "    roi_std = (sum((ch['roi'] - avg_roi) ** 2 for ch in chart_data) / len(chart_data)) ** 0.5\n",
    "    marginal_roi_std = (sum((ch['mroi'] - avg_marginal_roi) ** 2 for ch in chart_data) / len(chart_data)) ** 0.5\n",
    "    \n",
    "    analysis += f\"\\n- ROI variation across channels: {roi_std:.2f} standard deviation\"\n",
    "    analysis += f\"\\n- Marginal ROI variation: {marginal_roi_std:.2f} standard deviation\"\n",
    "    \n",
    "    if marginal_roi_std > roi_std:\n",
    "        analysis += f\"\\n- Marginal ROI shows higher variation than base ROI, indicating different scaling potentials\"\n",
    "    else:\n",
    "        analysis += f\"\\n- ROI and marginal ROI show similar variation patterns\"\n",
    "    \n",
    "    # Channel performance patterns\n",
    "    analysis += f\"\\n\\nChannel Performance Patterns:\"\n",
    "    \n",
    "    best_roi = by_roi[0]\n",
    "    best_marginal = by_marginal_roi[0]\n",
    "    largest_spend = by_spend[0]\n",
    "    \n",
    "    if best_roi['channel'] == best_marginal['channel']:\n",
    "        analysis += f\"\\n- {best_roi['channel'].upper()} leads in both ROI and marginal ROI - consistent high performer\"\n",
    "    else:\n",
    "        analysis += f\"\\n- {best_roi['channel'].upper()} has highest ROI while {best_marginal['channel'].upper()} has highest marginal ROI - different optimization opportunities\"\n",
    "    \n",
    "    # Spend efficiency vs performance\n",
    "    for channel in chart_data:\n",
    "        spend_share = (channel['spend'] / total_spend) * 100\n",
    "        if spend_share > 50 and channel['mroi'] < avg_marginal_roi:\n",
    "            analysis += f\"\\n- {channel['channel'].upper()}: High spend concentration ({spend_share:.1f}%) with below-average marginal efficiency\"\n",
    "        elif spend_share < 15 and channel['mroi'] > avg_marginal_roi:\n",
    "            analysis += f\"\\n- {channel['channel'].upper()}: Low spend allocation ({spend_share:.1f}%) but above-average marginal efficiency\"\n",
    "    \n",
    "    # Diminishing returns analysis\n",
    "    analysis += f\"\\n\\nDiminishing Returns Assessment:\"\n",
    "    for channel in chart_data:\n",
    "        returns_ratio = channel['roi'] / channel['mroi']\n",
    "        if returns_ratio > 3.0:\n",
    "            analysis += f\"\\n- {channel['channel'].upper()}: Strong diminishing returns pattern (current ROI {returns_ratio:.1f}x higher than marginal)\"\n",
    "        elif returns_ratio > 2.0:\n",
    "            analysis += f\"\\n- {channel['channel'].upper()}: Moderate diminishing returns (current ROI {returns_ratio:.1f}x higher than marginal)\"\n",
    "        else:\n",
    "            analysis += f\"\\n- {channel['channel'].upper()}: Minimal diminishing returns (current ROI {returns_ratio:.1f}x higher than marginal)\"\n",
    "    \n",
    "    if chart_description:\n",
    "        analysis += f\"\\n\\nMethodology: {chart_description}\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "print(get_roi_marginal_insights(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idac_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78be259ae8f52cb4f09525a9f670116c3dc26611fae831ffbbd168bd6bc01594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
